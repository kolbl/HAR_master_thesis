Using TensorFlow backend.
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Running ML algorithms with the following parameters:
Lags (time steps): 40
Features: 3403
Training samples: 5322
Testing samples: 2898
Logistic Regression training time: 122.77147173881531s
Logistic Regression : training accuracy is:  0.66
Missing classes in prediction:  {19.0, 14.0}
Logistic Regression : testing accuracy is:  0.6563 , recall (weighted) is:  0.6702 precision (weighted) is:  0.7144 F1 (weighted) is:  0.6311 MCC is: 0.6448 RMSE is: 3.987
-------------------------------------------------------------------------------------------------------
Accuracies per class for Logistic Regression
[0.89 0.97 0.81 0.98 0.77 0.83 0.94 0.88 0.77 0.   0.25 0.31 0.   0.76
 0.8  0.81 0.06 0.    nan 0.94 0.76 0.43 0.87]
Naive Bayes training time: 0.5412507057189941s
Gaussian Naive Bayes : training accuracy is:  0.34
Missing classes in prediction:  {19.0, 12.0, 14.0}
Gaussian Naive Bayes : testing accuracy is:  0.3354 , recall (weighted) is:  0.4333 precision (weighted) is:  0.5575 F1 (weighted) is:  0.3964 MCC is: 0.3213 RMSE is: 6.4822
-------------------------------------------------------------------------------------------------------
Accuracies per class for Naive Bayes
[0.32 0.87 0.46 0.58 0.4  0.54 0.1   nan 0.   0.46 0.   0.   0.12 0.
 0.69 0.27 0.79 0.36 0.    nan 0.28 0.83 0.29 0.8 ]
Decision tree training time: 2.7169251441955566s
Decision Tree : training accuracy is:  0.43
Missing classes in prediction:  set()
Decision Tree : testing accuracy is:  0.4293 , recall (weighted) is:  0.4293 precision (weighted) is:  0.4839 F1 (weighted) is:  0.4003 MCC is: 0.4027 RMSE is: 7.0095
-------------------------------------------------------------------------------------------------------
Accuracies per class for Decision Tree
[0.19 0.4  0.81 0.77 0.54 0.7  0.88  nan 0.   0.72 0.   0.09 0.22 0.
 0.31 0.7  0.33 0.03 0.    nan 0.63 0.42 0.55 0.29]
Feature importances Decision Tree:
             importance
var83(t)       0.104142
var38(t-9)     0.057857
var44(t-13)    0.038900
var44(t-3)     0.036181
var83(t-40)    0.033478
...                 ...
var79(t-26)    0.000000
var80(t-26)    0.000000
var81(t-26)    0.000000
var82(t-26)    0.000000
var1(t-40)     0.000000

[3403 rows x 1 columns]
Support Vector Machine training time: 1162.4286632537842s
Support Vector Machine : training accuracy is:  0.53
Missing classes in prediction:  {1.0, 11.0, 13.0, 14.0, 16.0, 18.0, 19.0, 23.0}
Support Vector Machine : testing accuracy is:  0.5273 , recall (weighted) is:  0.6139 precision (weighted) is:  0.4793 F1 (weighted) is:  0.5023 MCC is: 0.5198 RMSE is: 5.7286
-------------------------------------------------------------------------------------------------------
Accuracies per class for Support Vector Machine
[0.   0.82 0.92 0.95 0.87 1.   0.97 0.95 0.54 0.   0.   0.   0.   0.43
 0.   0.79 0.   0.   0.41 0.75 0.   0.82]
K-Nearest Neighbors training time: 1.7895152568817139s
K-Nearest Neighbors : training accuracy is:  0.35
Missing classes in prediction:  {1.0, 2.0, 10.0, 11.0, 13.0, 14.0, 15.0, 19.0, 23.0}
K-Nearest Neighbors : testing accuracy is:  0.3492 , recall (weighted) is:  0.4299 precision (weighted) is:  0.4716 F1 (weighted) is:  0.3154 MCC is: 0.2846 RMSE is: 7.6944
-------------------------------------------------------------------------------------------------------
Accuracies per class for K-Nearest Neighbors
[0.   0.   0.19 0.17 0.   0.   1.   0.89 0.   0.   0.53 0.   0.   0.
 0.27 0.12 0.   0.   1.   0.02 0.   0.56]
Random Forest training time: 51.82493305206299s
Random Forest : training accuracy is:  0.46
Missing classes in prediction:  {19.0, 12.0, 13.0, 14.0}
Random Forest : testing accuracy is:  0.461 , recall (weighted) is:  0.6117 precision (weighted) is:  0.5 F1 (weighted) is:  0.509 MCC is: 0.4451 RMSE is: 6.6735
-------------------------------------------------------------------------------------------------------
Accuracies per class for Random Forest
[0.   0.96 0.98 0.98 0.52 0.91 0.96 0.   0.62 0.   0.   0.   0.   0.17
 0.37 0.42 0.   0.   0.61 0.54 0.15 0.79]
Feature importances Random Forest:
             importance
var83(t)       0.012170
var83(t-1)     0.011087
var83(t-2)     0.009701
var83(t-6)     0.009613
var83(t-4)     0.008937
...                 ...
var19(t-32)    0.000000
var45(t-6)     0.000000
var45(t-15)    0.000000
var5(t-27)     0.000000
var5(t-14)     0.000000

[3403 rows x 1 columns]
Bagging training time: 18.421640157699585s
Bagging : training accuracy is:  0.48
Missing classes in prediction:  {19.0, 14.0}
Bagging : testing accuracy is:  0.4769 , recall (weighted) is:  0.487 precision (weighted) is:  0.5531 F1 (weighted) is:  0.4336 MCC is: 0.4581 RMSE is: 6.4834
-------------------------------------------------------------------------------------------------------
Accuracies per class for Bagging
[0.08 0.6  0.98 0.88 0.6  0.8  0.93  nan 0.   0.74 0.   0.05 0.47 0.
 0.36 0.77 0.57 0.   0.    nan 0.61 0.45 0.37 0.52]
Extra Tree training time: 0.5037047863006592s
Extra Tree : training accuracy is:  0.56
Missing classes in prediction:  {14.0}
Extra Tree : testing accuracy is:  0.5638 , recall (weighted) is:  0.565 precision (weighted) is:  0.6215 F1 (weighted) is:  0.513 MCC is: 0.5518 RMSE is: 4.5705
-------------------------------------------------------------------------------------------------------
Accuracies per class for Extra Tree
[0.31 0.96 0.97 0.89 0.84 0.94 0.89  nan 0.   0.69 0.   0.06 0.32 0.
 0.42 0.6  0.68 0.   0.02  nan 0.91 0.64 0.58 0.88]
Feature importances Extra Forest:
             importance
var83(t-39)    0.012742
var83(t-4)     0.012254
var38(t-32)    0.011471
var83(t-23)    0.010785
var83(t-1)     0.010563
...                 ...
var46(t-16)    0.000000
var45(t-16)    0.000000
var25(t-35)    0.000000
var29(t-39)    0.000000
var1(t-40)     0.000000

[3403 rows x 1 columns]
Gradient Boosting training time: 1011.3834402561188s
Gradient Boosting : training accuracy is:  0.56
Missing classes in prediction:  {19.0, 14.0}
Gradient Boosting : testing accuracy is:  0.5618 , recall (weighted) is:  0.5736 precision (weighted) is:  0.6441 F1 (weighted) is:  0.5162 MCC is: 0.5531 RMSE is: 4.8253
-------------------------------------------------------------------------------------------------------
Accuracies per class for Gradient Boosting
[0.85 0.88 0.92 0.95 0.85 0.67 0.95 0.05 0.72 0.   0.01 0.36 0.   0.76
 0.8  0.84 0.61 0.    nan 0.61 0.83 0.43 0.55]
Feature importances Gradient Boosting:
             importance
var83(t)       0.075970
var38(t-8)     0.023298
var83(t-10)    0.020585
var38(t-9)     0.017743
var83(t-40)    0.015418
...                 ...
var58(t-14)    0.000000
var57(t-14)    0.000000
var56(t-14)    0.000000
var55(t-14)    0.000000
var45(t-17)    0.000000

[3403 rows x 1 columns]
Multilayer Perceptron training time: 217.51997184753418s
Multilayer Perceptron : training accuracy is:  0.5
Missing classes in prediction:  {19.0, 14.0}
Multilayer Perceptron : testing accuracy is:  0.5031 , recall (weighted) is:  0.5137 precision (weighted) is:  0.5871 F1 (weighted) is:  0.4866 MCC is: 0.4767 RMSE is: 5.3834
-------------------------------------------------------------------------------------------------------
Accuracies per class for Multilayer Perceptron
[0.57 0.47 0.82 0.99 0.6  0.94 0.86  nan 0.   0.36 0.   0.3  0.   0.
 0.31 0.13 0.48 0.   0.    nan 0.79 0.65 0.   0.04]
WARNING:tensorflow:From /home/linda/Uni/Master_Thesis/play/HAR_master_thesis/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/linda/Uni/Master_Thesis/play/HAR_master_thesis/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2020-01-05 23:04:37.132607: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-05 23:04:37.206748: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2020-01-05 23:04:37.210874: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0xe4e2330 executing computations on platform Host. Devices:
2020-01-05 23:04:37.210973: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 3s - loss: 2.9129 - acc: 0.1819 - val_loss: 2.4962 - val_acc: 0.2442
Epoch 2/50
 - 3s - loss: 2.3693 - acc: 0.3340 - val_loss: 1.9390 - val_acc: 0.4966
Epoch 3/50
 - 3s - loss: 1.9057 - acc: 0.4733 - val_loss: 1.4788 - val_acc: 0.6018
Epoch 4/50
 - 4s - loss: 1.5641 - acc: 0.5600 - val_loss: 1.2171 - val_acc: 0.6597
Epoch 5/50
 - 4s - loss: 1.3026 - acc: 0.6470 - val_loss: 1.0426 - val_acc: 0.7145
Epoch 6/50
 - 4s - loss: 1.1054 - acc: 0.6860 - val_loss: 0.9087 - val_acc: 0.7498
Epoch 7/50
 - 4s - loss: 0.9545 - acc: 0.7344 - val_loss: 0.8238 - val_acc: 0.7648
Epoch 8/50
 - 4s - loss: 0.8131 - acc: 0.7670 - val_loss: 0.7337 - val_acc: 0.7934
Epoch 9/50
 - 4s - loss: 0.7179 - acc: 0.7990 - val_loss: 0.6951 - val_acc: 0.8062
Epoch 10/50
 - 4s - loss: 0.6376 - acc: 0.8281 - val_loss: 0.6458 - val_acc: 0.8084
Epoch 11/50
 - 4s - loss: 0.5693 - acc: 0.8381 - val_loss: 0.6170 - val_acc: 0.8234
Epoch 12/50
 - 4s - loss: 0.5008 - acc: 0.8634 - val_loss: 0.5907 - val_acc: 0.8264
Epoch 13/50
 - 4s - loss: 0.4576 - acc: 0.8722 - val_loss: 0.5856 - val_acc: 0.8234
Epoch 14/50
 - 4s - loss: 0.4110 - acc: 0.8865 - val_loss: 0.5900 - val_acc: 0.8204
Epoch 15/50
 - 4s - loss: 0.3783 - acc: 0.8887 - val_loss: 0.5487 - val_acc: 0.8422
Epoch 16/50
 - 4s - loss: 0.3439 - acc: 0.9013 - val_loss: 0.5396 - val_acc: 0.8445
Epoch 17/50
 - 4s - loss: 0.3201 - acc: 0.9121 - val_loss: 0.5615 - val_acc: 0.8347
Epoch 18/50
 - 4s - loss: 0.2944 - acc: 0.9163 - val_loss: 0.5421 - val_acc: 0.8460
Epoch 19/50
 - 4s - loss: 0.2738 - acc: 0.9243 - val_loss: 0.5498 - val_acc: 0.8415
Epoch 20/50
 - 4s - loss: 0.2540 - acc: 0.9271 - val_loss: 0.5273 - val_acc: 0.8588
Epoch 21/50
 - 4s - loss: 0.2304 - acc: 0.9346 - val_loss: 0.5337 - val_acc: 0.8520
Epoch 22/50
 - 4s - loss: 0.2130 - acc: 0.9401 - val_loss: 0.5310 - val_acc: 0.8557
Epoch 23/50
 - 4s - loss: 0.2013 - acc: 0.9456 - val_loss: 0.5366 - val_acc: 0.8565
Epoch 24/50
 - 4s - loss: 0.1904 - acc: 0.9411 - val_loss: 0.5391 - val_acc: 0.8603
Epoch 25/50
 - 4s - loss: 0.1724 - acc: 0.9534 - val_loss: 0.5456 - val_acc: 0.8588
Epoch 00025: early stopping
CNN training time: 90.33762717247009s
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_1 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_1 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                81984
_________________________________________________________________
dropout_1 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.912918722142674, 2.3693291150748474, 1.9057224304892497, 1.564128768517615, 1.3026045771574193, 1.105363006558175, 0.9544769350270244, 0.813087151180724, 0.7178601881136674, 0.637606193258074, 0.5692751903217114, 0.5008259319584323, 0.45759973942430193, 0.4110365879772811, 0.3783336173714511, 0.3439145803513808, 0.3200758593447352, 0.29441497230113967, 0.2737548188518344, 0.2540163324122401, 0.23042223924133767, 0.21304388717654066, 0.20128937915379871, 0.19041197300361346, 0.17236897590881842]
[0.1819093, 0.3340015, 0.47331497, 0.56001, 0.64695567, 0.6860436, 0.7344024, 0.7669757, 0.7990479, 0.82811326, 0.8381358, 0.8634427, 0.87221247, 0.88649464, 0.88874966, 0.9012779, 0.9120521, 0.9163117, 0.92432976, 0.92708594, 0.93460286, 0.9401153, 0.9456277, 0.9411175, 0.9533951]
[2.496182212271608, 1.9390388433475139, 1.4788309890443807, 1.2171413702519427, 1.0426411638575153, 0.90869389279311, 0.823785670729988, 0.7336506091228285, 0.6950573336884412, 0.6457852384707541, 0.6169820090010928, 0.5907104788202451, 0.5855865822499285, 0.5900374805048503, 0.5487050850779289, 0.5395854831012071, 0.561453872017018, 0.5420660769285827, 0.5497818237831448, 0.5272643594326775, 0.5337050938109421, 0.5309574434099883, 0.5365640646535992, 0.5390624689970086, 0.5455955743688803]
[0.24417731165885925, 0.4966190755367279, 0.601803183555603, 0.6596543788909912, 0.714500367641449, 0.7498121857643127, 0.764838457107544, 0.7933884263038635, 0.8061608076095581, 0.8084146976470947, 0.8234410285949707, 0.8264462947845459, 0.8234410285949707, 0.8204357624053955, 0.8422238826751709, 0.8444778323173523, 0.8347107172012329, 0.8459804654121399, 0.8414725661277771, 0.8587528467178345, 0.8519909977912903, 0.8557475805282593, 0.8564988970756531, 0.8602554202079773, 0.8587528467178345]

  32/2898 [..............................] - ETA: 4s
 544/2898 [====>.........................] - ETA: 0s
1056/2898 [=========>....................] - ETA: 0s
1568/2898 [===============>..............] - ETA: 0s
2080/2898 [====================>.........] - ETA: 0s
2400/2898 [=======================>......] - ETA: 0s
2720/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 0s 138us/step
Accuracies per class for CNN
[0.79 0.99 0.98 0.96 0.81 0.39 0.95 0.89 0.67 0.   0.41 0.27 0.   0.72
 0.63 0.76 0.   0.    nan 0.86 0.88 0.37 0.73]
[2.912918722142674, 2.3693291150748474, 1.9057224304892497, 1.564128768517615, 1.3026045771574193, 1.105363006558175, 0.9544769350270244, 0.813087151180724, 0.7178601881136674, 0.637606193258074, 0.5692751903217114, 0.5008259319584323, 0.45759973942430193, 0.4110365879772811, 0.3783336173714511, 0.3439145803513808, 0.3200758593447352, 0.29441497230113967, 0.2737548188518344, 0.2540163324122401, 0.23042223924133767, 0.21304388717654066, 0.20128937915379871, 0.19041197300361346, 0.17236897590881842]
[0.1819093, 0.3340015, 0.47331497, 0.56001, 0.64695567, 0.6860436, 0.7344024, 0.7669757, 0.7990479, 0.82811326, 0.8381358, 0.8634427, 0.87221247, 0.88649464, 0.88874966, 0.9012779, 0.9120521, 0.9163117, 0.92432976, 0.92708594, 0.93460286, 0.9401153, 0.9456277, 0.9411175, 0.9533951]
[2.496182212271608, 1.9390388433475139, 1.4788309890443807, 1.2171413702519427, 1.0426411638575153, 0.90869389279311, 0.823785670729988, 0.7336506091228285, 0.6950573336884412, 0.6457852384707541, 0.6169820090010928, 0.5907104788202451, 0.5855865822499285, 0.5900374805048503, 0.5487050850779289, 0.5395854831012071, 0.561453872017018, 0.5420660769285827, 0.5497818237831448, 0.5272643594326775, 0.5337050938109421, 0.5309574434099883, 0.5365640646535992, 0.5390624689970086, 0.5455955743688803]
[0.24417731165885925, 0.4966190755367279, 0.601803183555603, 0.6596543788909912, 0.714500367641449, 0.7498121857643127, 0.764838457107544, 0.7933884263038635, 0.8061608076095581, 0.8084146976470947, 0.8234410285949707, 0.8264462947845459, 0.8234410285949707, 0.8204357624053955, 0.8422238826751709, 0.8444778323173523, 0.8347107172012329, 0.8459804654121399, 0.8414725661277771, 0.8587528467178345, 0.8519909977912903, 0.8557475805282593, 0.8564988970756531, 0.8602554202079773, 0.8587528467178345]
recall 0.6832
precision 0.7312
f1 0.6604
mcc 0.6534
RMSE: 4.204
classification report:
              precision    recall  f1-score   support

           1       0.62      0.79      0.70        72
           2       0.48      0.99      0.64        68
           3       0.94      0.98      0.96       252
           4       0.91      0.96      0.93       132
           5       0.76      0.81      0.79       191
           6       0.87      0.39      0.53       150
           7       0.48      0.95      0.64       240
           9       0.46      0.89      0.60       100
          10       0.61      0.67      0.64        61
          11       0.00      0.00      0.00        87
          12       0.97      0.41      0.57       595
          13       0.57      0.27      0.37        59
          14       0.00      0.00      0.00         6
          15       0.68      0.72      0.70        72
          16       0.83      0.63      0.72        30
          17       0.67      0.76      0.71       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.82      0.86      0.84       222
          22       0.53      0.88      0.66       168
          23       0.89      0.37      0.52        65
          24       0.63      0.73      0.68        82

   micro avg       0.67      0.67      0.67      2898
   macro avg       0.55      0.57      0.53      2898
weighted avg       0.72      0.67      0.65      2898

>#1: 66.908
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 4s - loss: 2.9422 - acc: 0.1599 - val_loss: 2.5338 - val_acc: 0.2397
Epoch 2/50
 - 4s - loss: 2.3894 - acc: 0.3155 - val_loss: 1.9994 - val_acc: 0.4515
Epoch 3/50
 - 4s - loss: 1.9561 - acc: 0.4513 - val_loss: 1.5247 - val_acc: 0.5733
Epoch 4/50
 - 4s - loss: 1.5986 - acc: 0.5445 - val_loss: 1.2098 - val_acc: 0.6536
Epoch 5/50
 - 4s - loss: 1.3173 - acc: 0.6314 - val_loss: 0.9888 - val_acc: 0.7032
Epoch 6/50
 - 4s - loss: 1.1208 - acc: 0.6743 - val_loss: 0.8740 - val_acc: 0.7235
Epoch 7/50
 - 5s - loss: 0.9665 - acc: 0.7236 - val_loss: 0.7795 - val_acc: 0.7476
Epoch 8/50
 - 5s - loss: 0.8445 - acc: 0.7705 - val_loss: 0.7151 - val_acc: 0.7513
Epoch 9/50
 - 5s - loss: 0.7322 - acc: 0.7933 - val_loss: 0.6649 - val_acc: 0.7784
Epoch 10/50
 - 4s - loss: 0.6570 - acc: 0.8259 - val_loss: 0.6435 - val_acc: 0.7986
Epoch 11/50
 - 5s - loss: 0.5996 - acc: 0.8306 - val_loss: 0.6185 - val_acc: 0.8002
Epoch 12/50
 - 5s - loss: 0.5381 - acc: 0.8467 - val_loss: 0.6054 - val_acc: 0.8032
Epoch 13/50
 - 5s - loss: 0.4706 - acc: 0.8692 - val_loss: 0.5830 - val_acc: 0.8227
Epoch 14/50
 - 5s - loss: 0.4284 - acc: 0.8825 - val_loss: 0.5743 - val_acc: 0.8227
Epoch 15/50
 - 5s - loss: 0.3991 - acc: 0.8900 - val_loss: 0.5972 - val_acc: 0.8107
Epoch 16/50
 - 4s - loss: 0.3666 - acc: 0.9008 - val_loss: 0.5617 - val_acc: 0.8325
Epoch 17/50
 - 4s - loss: 0.3343 - acc: 0.9043 - val_loss: 0.5600 - val_acc: 0.8392
Epoch 18/50
 - 4s - loss: 0.3190 - acc: 0.9118 - val_loss: 0.5840 - val_acc: 0.8152
Epoch 19/50
 - 4s - loss: 0.2926 - acc: 0.9218 - val_loss: 0.5807 - val_acc: 0.8197
Epoch 20/50
 - 5s - loss: 0.2628 - acc: 0.9251 - val_loss: 0.5814 - val_acc: 0.8257
Epoch 21/50
 - 5s - loss: 0.2280 - acc: 0.9384 - val_loss: 0.5653 - val_acc: 0.8445
Epoch 22/50
 - 5s - loss: 0.2086 - acc: 0.9431 - val_loss: 0.5692 - val_acc: 0.8400
Epoch 00022: early stopping
CNN training time: 99.12935996055603s
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_2 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_2 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_3 (Dense)              (None, 64)                81984
_________________________________________________________________
dropout_2 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_4 (Dense)              (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9421788455847957, 2.38944864392176, 1.9561487686507866, 1.5986495117209543, 1.3172890672215642, 1.1207573023519475, 0.9664623284737169, 0.8445305852707176, 0.7321651500543107, 0.6569504092246307, 0.5995810772441372, 0.5381293379236947, 0.4706137093477573, 0.4284033679715157, 0.3991316401732962, 0.3666117509804373, 0.3342566343297516, 0.3190386771730223, 0.29260843928869185, 0.26277987080270704, 0.2280370610352796, 0.208571121669306]
[0.15985969, 0.3154598, 0.45126534, 0.5444751, 0.6314207, 0.6742671, 0.72362816, 0.7704836, 0.7932849, 0.8258582, 0.8306189, 0.84665495, 0.8692057, 0.88248557, 0.8900025, 0.90077674, 0.90428466, 0.9118016, 0.9218241, 0.92508143, 0.9383613, 0.943122]
[2.5337716205329666, 1.9993715002841739, 1.5246880644474774, 1.2098438996208691, 0.9888293871769415, 0.8739910224504394, 0.7794833527633026, 0.7150835303161475, 0.6649228437040295, 0.6434658050133373, 0.618522928039502, 0.6054012004232172, 0.5830478185269482, 0.5742763670811752, 0.5971817056724352, 0.561742479736302, 0.5599671493307232, 0.5840221492961174, 0.5806560988991603, 0.5814289015600143, 0.5652571215157879, 0.5691662245945721]
[0.23966942727565765, 0.4515402019023895, 0.5732532143592834, 0.6536439061164856, 0.7032306790351868, 0.7235161662101746, 0.7475582361221313, 0.7513148188591003, 0.7783621549606323, 0.7986476421356201, 0.8001502752304077, 0.8031555414199829, 0.8226897120475769, 0.8226897120475769, 0.8106686472892761, 0.8324568271636963, 0.8392186164855957, 0.8151765465736389, 0.8196844458580017, 0.8256949782371521, 0.8444778323173523, 0.8399699330329895]

  32/2898 [..............................] - ETA: 11s
 288/2898 [=>............................] - ETA: 1s
 704/2898 [======>.......................] - ETA: 0s
1088/2898 [==========>...................] - ETA: 0s
1536/2898 [==============>...............] - ETA: 0s
1728/2898 [================>.............] - ETA: 0s
1952/2898 [===================>..........] - ETA: 0s
2144/2898 [=====================>........] - ETA: 0s
2336/2898 [=======================>......] - ETA: 0s
2592/2898 [=========================>....] - ETA: 0s
2848/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 246us/step
Accuracies per class for CNN
[0.33 0.94 0.99 0.76 0.87 0.25 0.78 0.9  0.57 0.   0.65 0.22 0.   0.76
 0.7  0.79 0.03 0.   0.9  0.83 0.29 0.73]
[2.9421788455847957, 2.38944864392176, 1.9561487686507866, 1.5986495117209543, 1.3172890672215642, 1.1207573023519475, 0.9664623284737169, 0.8445305852707176, 0.7321651500543107, 0.6569504092246307, 0.5995810772441372, 0.5381293379236947, 0.4706137093477573, 0.4284033679715157, 0.3991316401732962, 0.3666117509804373, 0.3342566343297516, 0.3190386771730223, 0.29260843928869185, 0.26277987080270704, 0.2280370610352796, 0.208571121669306]
[0.15985969, 0.3154598, 0.45126534, 0.5444751, 0.6314207, 0.6742671, 0.72362816, 0.7704836, 0.7932849, 0.8258582, 0.8306189, 0.84665495, 0.8692057, 0.88248557, 0.8900025, 0.90077674, 0.90428466, 0.9118016, 0.9218241, 0.92508143, 0.9383613, 0.943122]
[2.5337716205329666, 1.9993715002841739, 1.5246880644474774, 1.2098438996208691, 0.9888293871769415, 0.8739910224504394, 0.7794833527633026, 0.7150835303161475, 0.6649228437040295, 0.6434658050133373, 0.618522928039502, 0.6054012004232172, 0.5830478185269482, 0.5742763670811752, 0.5971817056724352, 0.561742479736302, 0.5599671493307232, 0.5840221492961174, 0.5806560988991603, 0.5814289015600143, 0.5652571215157879, 0.5691662245945721]
[0.23966942727565765, 0.4515402019023895, 0.5732532143592834, 0.6536439061164856, 0.7032306790351868, 0.7235161662101746, 0.7475582361221313, 0.7513148188591003, 0.7783621549606323, 0.7986476421356201, 0.8001502752304077, 0.8031555414199829, 0.8226897120475769, 0.8226897120475769, 0.8106686472892761, 0.8324568271636963, 0.8392186164855957, 0.8151765465736389, 0.8196844458580017, 0.8256949782371521, 0.8444778323173523, 0.8399699330329895]
recall 0.6956
precision 0.695
f1 0.6698
mcc 0.6584
RMSE: 3.702
classification report:
              precision    recall  f1-score   support

           1       0.62      0.33      0.43        72
           2       0.36      0.94      0.52        68
           3       0.82      0.99      0.90       252
           4       0.69      0.76      0.72       132
           5       0.67      0.87      0.76       191
           6       0.71      0.25      0.37       150
           7       0.56      0.78      0.65       240
           9       0.48      0.90      0.62       100
          10       0.55      0.57      0.56        61
          11       0.00      0.00      0.00        87
          12       0.94      0.65      0.77       595
          13       0.52      0.22      0.31        59
          14       0.00      0.00      0.00         6
          15       0.71      0.76      0.74        72
          16       0.84      0.70      0.76        30
          17       0.68      0.79      0.73       156
          18       0.06      0.03      0.04        36
          19       0.00      0.00      0.00        54
          21       0.87      0.90      0.88       222
          22       0.61      0.83      0.70       168
          23       0.59      0.29      0.39        65
          24       0.52      0.73      0.61        82

   micro avg       0.68      0.68      0.68      2898
   macro avg       0.54      0.56      0.52      2898
weighted avg       0.68      0.68      0.66      2898

>#2: 68.116
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 5s - loss: 2.9412 - acc: 0.1689 - val_loss: 2.4790 - val_acc: 0.3118
Epoch 2/50
 - 5s - loss: 2.3470 - acc: 0.3483 - val_loss: 1.7870 - val_acc: 0.5177
Epoch 3/50
 - 4s - loss: 1.8634 - acc: 0.4836 - val_loss: 1.3973 - val_acc: 0.6138
Epoch 4/50
 - 5s - loss: 1.4689 - acc: 0.5938 - val_loss: 1.1495 - val_acc: 0.6784
Epoch 5/50
 - 5s - loss: 1.2306 - acc: 0.6607 - val_loss: 0.9969 - val_acc: 0.7198
Epoch 6/50
 - 5s - loss: 1.0323 - acc: 0.7048 - val_loss: 0.8706 - val_acc: 0.7588
Epoch 7/50
 - 4s - loss: 0.8950 - acc: 0.7462 - val_loss: 0.8199 - val_acc: 0.7536
Epoch 8/50
 - 5s - loss: 0.7932 - acc: 0.7725 - val_loss: 0.7554 - val_acc: 0.7829
Epoch 9/50
 - 5s - loss: 0.6787 - acc: 0.8108 - val_loss: 0.7153 - val_acc: 0.7844
Epoch 10/50
 - 5s - loss: 0.6183 - acc: 0.8226 - val_loss: 0.6865 - val_acc: 0.7964
Epoch 11/50
 - 5s - loss: 0.5478 - acc: 0.8454 - val_loss: 0.6692 - val_acc: 0.8002
Epoch 12/50
 - 5s - loss: 0.4915 - acc: 0.8597 - val_loss: 0.6569 - val_acc: 0.8002
Epoch 13/50
 - 5s - loss: 0.4459 - acc: 0.8807 - val_loss: 0.6445 - val_acc: 0.7994
Epoch 14/50
 - 5s - loss: 0.4159 - acc: 0.8875 - val_loss: 0.6246 - val_acc: 0.8107
Epoch 15/50
 - 5s - loss: 0.3632 - acc: 0.9018 - val_loss: 0.6197 - val_acc: 0.8159
Epoch 16/50
 - 4s - loss: 0.3504 - acc: 0.8998 - val_loss: 0.6280 - val_acc: 0.8144
Epoch 17/50
 - 4s - loss: 0.3085 - acc: 0.9146 - val_loss: 0.6474 - val_acc: 0.8009
Epoch 18/50
 - 5s - loss: 0.2907 - acc: 0.9126 - val_loss: 0.6195 - val_acc: 0.8144
Epoch 19/50
 - 4s - loss: 0.2631 - acc: 0.9233 - val_loss: 0.6042 - val_acc: 0.8197
Epoch 20/50
 - 4s - loss: 0.2465 - acc: 0.9268 - val_loss: 0.6285 - val_acc: 0.8129
Epoch 21/50
 - 4s - loss: 0.2234 - acc: 0.9359 - val_loss: 0.6399 - val_acc: 0.8167
Epoch 22/50
 - 5s - loss: 0.2019 - acc: 0.9456 - val_loss: 0.6580 - val_acc: 0.8099
Epoch 23/50
 - 4s - loss: 0.1988 - acc: 0.9476 - val_loss: 0.6587 - val_acc: 0.8009
Epoch 24/50
 - 4s - loss: 0.1891 - acc: 0.9461 - val_loss: 0.6435 - val_acc: 0.8039
Epoch 00024: early stopping
CNN training time: 110.13598322868347s
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_3 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_3 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_5 (Dense)              (None, 64)                81984
_________________________________________________________________
dropout_3 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_6 (Dense)              (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.941236832556465, 2.3470080270635187, 1.863362796420749, 1.4688787467953213, 1.2306257017172426, 1.0322717168076472, 0.8949777147062011, 0.7932259238148176, 0.6786836769128001, 0.6183209579486509, 0.5478341964492819, 0.4915231800766645, 0.44590225941823003, 0.4159059802164397, 0.36323640069822083, 0.3503676588303801, 0.3085478680188776, 0.2906995936431585, 0.2631411401205348, 0.24648116026680272, 0.22336541292455464, 0.20186284576450048, 0.1988277917555411, 0.1891011734244212]
[0.16887999, 0.34828365, 0.48358807, 0.5938361, 0.6607367, 0.7048359, 0.7461789, 0.7724881, 0.81082433, 0.82260084, 0.8454022, 0.8596843, 0.88073164, 0.8874969, 0.901779, 0.8997745, 0.91455775, 0.91255325, 0.9233275, 0.92683536, 0.9358557, 0.9456277, 0.9476322, 0.9461288]
[2.479022669711568, 1.7870346973660654, 1.3973310680697504, 1.1494942732032174, 0.9969437785172355, 0.8705981381546781, 0.819905552098557, 0.7553725010224924, 0.7152660275633838, 0.6864701615443389, 0.6692018646109782, 0.656891943076189, 0.6445202777401038, 0.6245946264314859, 0.6196619002395487, 0.6280228711961805, 0.64742967061476, 0.6195249165555188, 0.604154152175639, 0.6284928880611232, 0.6398659345185357, 0.6580123207445293, 0.6586726994654855, 0.6435388616088514]
[0.3117956519126892, 0.5176559090614319, 0.613824188709259, 0.6784372925758362, 0.7197595834732056, 0.7588279247283936, 0.7535687685012817, 0.7828699946403503, 0.7843726277351379, 0.7963936924934387, 0.8001502752304077, 0.8001502752304077, 0.7993989586830139, 0.8106686472892761, 0.8159278631210327, 0.8144252300262451, 0.8009015917778015, 0.8144252300262451, 0.8196844458580017, 0.8129225969314575, 0.8166791796684265, 0.8099173307418823, 0.8009015917778015, 0.8039068579673767]

  32/2898 [..............................] - ETA: 11s
 480/2898 [===>..........................] - ETA: 0s
 864/2898 [=======>......................] - ETA: 0s
1280/2898 [============>.................] - ETA: 0s
1760/2898 [=================>............] - ETA: 0s
2208/2898 [=====================>........] - ETA: 0s
2400/2898 [=======================>......] - ETA: 0s
2624/2898 [==========================>...] - ETA: 0s
2898/2898 [==============================] - 1s 190us/step
Accuracies per class for CNN
[0.5  0.97 0.98 0.87 0.81 0.8  0.84 0.73 0.61 0.   0.69 0.22 0.   0.82
 0.57 0.82 0.08 0.02  nan 0.89 0.84 0.23 0.74]
[2.941236832556465, 2.3470080270635187, 1.863362796420749, 1.4688787467953213, 1.2306257017172426, 1.0322717168076472, 0.8949777147062011, 0.7932259238148176, 0.6786836769128001, 0.6183209579486509, 0.5478341964492819, 0.4915231800766645, 0.44590225941823003, 0.4159059802164397, 0.36323640069822083, 0.3503676588303801, 0.3085478680188776, 0.2906995936431585, 0.2631411401205348, 0.24648116026680272, 0.22336541292455464, 0.20186284576450048, 0.1988277917555411, 0.1891011734244212]
[0.16887999, 0.34828365, 0.48358807, 0.5938361, 0.6607367, 0.7048359, 0.7461789, 0.7724881, 0.81082433, 0.82260084, 0.8454022, 0.8596843, 0.88073164, 0.8874969, 0.901779, 0.8997745, 0.91455775, 0.91255325, 0.9233275, 0.92683536, 0.9358557, 0.9456277, 0.9476322, 0.9461288]
[2.479022669711568, 1.7870346973660654, 1.3973310680697504, 1.1494942732032174, 0.9969437785172355, 0.8705981381546781, 0.819905552098557, 0.7553725010224924, 0.7152660275633838, 0.6864701615443389, 0.6692018646109782, 0.656891943076189, 0.6445202777401038, 0.6245946264314859, 0.6196619002395487, 0.6280228711961805, 0.64742967061476, 0.6195249165555188, 0.604154152175639, 0.6284928880611232, 0.6398659345185357, 0.6580123207445293, 0.6586726994654855, 0.6435388616088514]
[0.3117956519126892, 0.5176559090614319, 0.613824188709259, 0.6784372925758362, 0.7197595834732056, 0.7588279247283936, 0.7535687685012817, 0.7828699946403503, 0.7843726277351379, 0.7963936924934387, 0.8001502752304077, 0.8001502752304077, 0.7993989586830139, 0.8106686472892761, 0.8159278631210327, 0.8144252300262451, 0.8009015917778015, 0.8144252300262451, 0.8196844458580017, 0.8129225969314575, 0.8166791796684265, 0.8099173307418823, 0.8009015917778015, 0.8039068579673767]
recall 0.7244
precision 0.7584
f1 0.7143
mcc 0.7027
RMSE: 3.485
classification report:
              precision    recall  f1-score   support

           1       0.56      0.50      0.53        72
           2       0.44      0.97      0.61        68
           3       0.89      0.98      0.93       252
           4       0.75      0.87      0.81       132
           5       0.92      0.81      0.86       191
           6       1.00      0.80      0.89       150
           7       0.75      0.84      0.79       240
           9       0.39      0.73      0.51       100
          10       0.61      0.61      0.61        61
          11       0.00      0.00      0.00        87
          12       0.96      0.69      0.80       595
          13       0.48      0.22      0.30        59
          14       0.00      0.00      0.00         6
          15       0.60      0.82      0.69        72
          16       0.81      0.57      0.67        30
          17       0.69      0.82      0.75       156
          18       0.20      0.08      0.12        36
          19       1.00      0.02      0.04        54
          20       0.00      0.00      0.00         0
          21       0.87      0.89      0.88       222
          22       0.54      0.84      0.66       168
          23       0.83      0.23      0.36        65
          24       0.57      0.74      0.65        82

   micro avg       0.72      0.72      0.72      2898
   macro avg       0.60      0.57      0.54      2898
weighted avg       0.76      0.72      0.71      2898

>#3: 72.291
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 5s - loss: 2.9171 - acc: 0.1764 - val_loss: 2.5261 - val_acc: 0.2096
Epoch 2/50
 - 4s - loss: 2.2635 - acc: 0.3596 - val_loss: 1.8203 - val_acc: 0.5116
Epoch 3/50
 - 4s - loss: 1.7653 - acc: 0.5031 - val_loss: 1.3783 - val_acc: 0.6311
Epoch 4/50
 - 4s - loss: 1.4152 - acc: 0.6006 - val_loss: 1.0928 - val_acc: 0.7408
Epoch 5/50
 - 5s - loss: 1.1887 - acc: 0.6552 - val_loss: 0.8935 - val_acc: 0.7881
Epoch 6/50
 - 4s - loss: 1.0134 - acc: 0.7154 - val_loss: 0.7844 - val_acc: 0.8062
Epoch 7/50
 - 4s - loss: 0.8710 - acc: 0.7527 - val_loss: 0.6824 - val_acc: 0.8264
Epoch 8/50
 - 4s - loss: 0.7649 - acc: 0.7755 - val_loss: 0.6164 - val_acc: 0.8392
Epoch 9/50
 - 5s - loss: 0.6724 - acc: 0.8106 - val_loss: 0.5747 - val_acc: 0.8377
Epoch 10/50
 - 4s - loss: 0.5903 - acc: 0.8339 - val_loss: 0.5313 - val_acc: 0.8527
Epoch 11/50
 - 4s - loss: 0.5155 - acc: 0.8537 - val_loss: 0.5267 - val_acc: 0.8415
Epoch 12/50
 - 4s - loss: 0.4760 - acc: 0.8685 - val_loss: 0.5076 - val_acc: 0.8490
Epoch 13/50
 - 4s - loss: 0.4166 - acc: 0.8867 - val_loss: 0.5025 - val_acc: 0.8460
Epoch 14/50
 - 5s - loss: 0.3973 - acc: 0.8862 - val_loss: 0.4747 - val_acc: 0.8588
Epoch 15/50
 - 4s - loss: 0.3600 - acc: 0.8985 - val_loss: 0.4914 - val_acc: 0.8535
Epoch 16/50
 - 4s - loss: 0.3204 - acc: 0.9065 - val_loss: 0.4967 - val_acc: 0.8618
Epoch 17/50
 - 4s - loss: 0.3020 - acc: 0.9133 - val_loss: 0.4910 - val_acc: 0.8588
Epoch 18/50
 - 4s - loss: 0.2736 - acc: 0.9253 - val_loss: 0.4731 - val_acc: 0.8648
Epoch 19/50
 - 4s - loss: 0.2543 - acc: 0.9308 - val_loss: 0.4695 - val_acc: 0.8640
Epoch 20/50
 - 4s - loss: 0.2293 - acc: 0.9341 - val_loss: 0.4856 - val_acc: 0.8655
Epoch 21/50
 - 4s - loss: 0.2165 - acc: 0.9369 - val_loss: 0.4866 - val_acc: 0.8730
Epoch 22/50
 - 4s - loss: 0.1974 - acc: 0.9436 - val_loss: 0.5141 - val_acc: 0.8670
Epoch 23/50
 - 4s - loss: 0.1881 - acc: 0.9464 - val_loss: 0.5211 - val_acc: 0.8670
Epoch 24/50
 - 4s - loss: 0.1739 - acc: 0.9521 - val_loss: 0.5007 - val_acc: 0.8625
Epoch 00024: early stopping
CNN training time: 107.73394060134888s
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_4 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_4 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_4 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_7 (Dense)              (None, 64)                81984
_________________________________________________________________
dropout_4 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_8 (Dense)              (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9170622496609697, 2.2634926084209672, 1.7653179976653257, 1.4152238051493613, 1.1887098285555093, 1.0133644950260396, 0.8710379805574551, 0.7649202920359718, 0.6723863385417981, 0.5903315258733237, 0.5154524148320273, 0.47602357211203294, 0.41664854073524216, 0.3972704399214517, 0.3599605910921998, 0.32039316934587675, 0.30202763742215444, 0.2735624334736218, 0.2543445630728278, 0.2293393231156131, 0.21648586868504197, 0.19737402960049547, 0.1880527786503939, 0.17393145210633199]
[0.17639689, 0.359559, 0.50313205, 0.6006014, 0.65522426, 0.71535957, 0.75269353, 0.7754949, 0.8105738, 0.8338762, 0.8536708, 0.86845404, 0.88674515, 0.88624406, 0.89852166, 0.90653974, 0.9133049, 0.925332, 0.9308444, 0.9341017, 0.93685794, 0.9436231, 0.94637936, 0.9521423]
[2.526141444155366, 1.8203060723152669, 1.3783029353737293, 1.0928457063405161, 0.8934545751008927, 0.7844315460091702, 0.6823754806825815, 0.6164301965084098, 0.5746931922006274, 0.5312875371660224, 0.5266907536828346, 0.5076116944726332, 0.502491435663665, 0.47473379857371095, 0.4913990852467794, 0.49666848267157065, 0.4909807963805793, 0.47312454652371877, 0.46945677466846686, 0.48563811995934203, 0.48659782038050703, 0.5141070657631792, 0.5210577256460485, 0.5006810032384257]
[0.20961682498455048, 0.5116453766822815, 0.6311044096946716, 0.7407963871955872, 0.7881292104721069, 0.8061608076095581, 0.8264462947845459, 0.8392186164855957, 0.8377159833908081, 0.8527423143386841, 0.8414725661277771, 0.8489857316017151, 0.8459804654121399, 0.8587528467178345, 0.8534936308860779, 0.8617580533027649, 0.8587528467178345, 0.8647633194923401, 0.8640120029449463, 0.8655146360397339, 0.8730278015136719, 0.8670172691345215, 0.8670172691345215, 0.8625093698501587]

  32/2898 [..............................] - ETA: 13s
 448/2898 [===>..........................] - ETA: 1s
 768/2898 [======>.......................] - ETA: 0s
1056/2898 [=========>....................] - ETA: 0s
1344/2898 [============>.................] - ETA: 0s
1728/2898 [================>.............] - ETA: 0s
2016/2898 [===================>..........] - ETA: 0s
2272/2898 [======================>.......] - ETA: 0s
2528/2898 [=========================>....] - ETA: 0s
2720/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 1s 234us/step
Accuracies per class for CNN
[0.78 0.97 0.98 0.95 0.85 0.91 0.94 0.95 0.66 0.   0.49 0.2  0.   0.79
 0.67 0.84 0.   0.    nan 0.88 0.82 0.32 0.8 ]
[2.9170622496609697, 2.2634926084209672, 1.7653179976653257, 1.4152238051493613, 1.1887098285555093, 1.0133644950260396, 0.8710379805574551, 0.7649202920359718, 0.6723863385417981, 0.5903315258733237, 0.5154524148320273, 0.47602357211203294, 0.41664854073524216, 0.3972704399214517, 0.3599605910921998, 0.32039316934587675, 0.30202763742215444, 0.2735624334736218, 0.2543445630728278, 0.2293393231156131, 0.21648586868504197, 0.19737402960049547, 0.1880527786503939, 0.17393145210633199]
[0.17639689, 0.359559, 0.50313205, 0.6006014, 0.65522426, 0.71535957, 0.75269353, 0.7754949, 0.8105738, 0.8338762, 0.8536708, 0.86845404, 0.88674515, 0.88624406, 0.89852166, 0.90653974, 0.9133049, 0.925332, 0.9308444, 0.9341017, 0.93685794, 0.9436231, 0.94637936, 0.9521423]
[2.526141444155366, 1.8203060723152669, 1.3783029353737293, 1.0928457063405161, 0.8934545751008927, 0.7844315460091702, 0.6823754806825815, 0.6164301965084098, 0.5746931922006274, 0.5312875371660224, 0.5266907536828346, 0.5076116944726332, 0.502491435663665, 0.47473379857371095, 0.4913990852467794, 0.49666848267157065, 0.4909807963805793, 0.47312454652371877, 0.46945677466846686, 0.48563811995934203, 0.48659782038050703, 0.5141070657631792, 0.5210577256460485, 0.5006810032384257]
[0.20961682498455048, 0.5116453766822815, 0.6311044096946716, 0.7407963871955872, 0.7881292104721069, 0.8061608076095581, 0.8264462947845459, 0.8392186164855957, 0.8377159833908081, 0.8527423143386841, 0.8414725661277771, 0.8489857316017151, 0.8459804654121399, 0.8587528467178345, 0.8534936308860779, 0.8617580533027649, 0.8587528467178345, 0.8647633194923401, 0.8640120029449463, 0.8655146360397339, 0.8730278015136719, 0.8670172691345215, 0.8670172691345215, 0.8625093698501587]
recall 0.7583
precision 0.7701
f1 0.7317
mcc 0.7052
RMSE: 3.902
classification report:
              precision    recall  f1-score   support

           1       0.70      0.78      0.74        72
           2       0.48      0.97      0.64        68
           3       0.96      0.98      0.97       252
           4       0.93      0.95      0.94       132
           5       0.95      0.85      0.90       191
           6       0.74      0.91      0.82       150
           7       0.55      0.94      0.70       240
           9       0.54      0.95      0.69       100
          10       0.52      0.66      0.58        61
          11       0.00      0.00      0.00        87
          12       0.98      0.49      0.65       595
          13       0.48      0.20      0.29        59
          14       0.00      0.00      0.00         6
          15       0.66      0.79      0.72        72
          16       0.87      0.67      0.75        30
          17       0.68      0.84      0.75       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.88      0.88      0.88       222
          22       0.55      0.82      0.66       168
          23       0.62      0.32      0.42        65
          24       0.54      0.80      0.64        82

   micro avg       0.72      0.72      0.72      2898
   macro avg       0.55      0.60      0.55      2898
weighted avg       0.73      0.72      0.69      2898

>#4: 71.981
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 5s - loss: 2.9313 - acc: 0.1551 - val_loss: 2.4603 - val_acc: 0.3336
Epoch 2/50
 - 5s - loss: 2.2894 - acc: 0.3666 - val_loss: 1.7987 - val_acc: 0.4966
Epoch 3/50
 - 5s - loss: 1.8037 - acc: 0.5039 - val_loss: 1.3946 - val_acc: 0.5455
Epoch 4/50
 - 4s - loss: 1.4728 - acc: 0.5801 - val_loss: 1.1695 - val_acc: 0.6228
Epoch 5/50
 - 4s - loss: 1.2657 - acc: 0.6347 - val_loss: 1.0149 - val_acc: 0.6717
Epoch 6/50
 - 5s - loss: 1.0609 - acc: 0.6998 - val_loss: 0.9061 - val_acc: 0.7055
Epoch 7/50
 - 4s - loss: 0.9118 - acc: 0.7484 - val_loss: 0.8064 - val_acc: 0.7558
Epoch 8/50
 - 5s - loss: 0.8166 - acc: 0.7697 - val_loss: 0.7489 - val_acc: 0.7881
Epoch 9/50
 - 5s - loss: 0.7089 - acc: 0.8026 - val_loss: 0.7029 - val_acc: 0.8092
Epoch 10/50
 - 5s - loss: 0.6053 - acc: 0.8346 - val_loss: 0.6612 - val_acc: 0.8242
Epoch 11/50
 - 5s - loss: 0.5613 - acc: 0.8419 - val_loss: 0.6372 - val_acc: 0.8310
Epoch 12/50
 - 6s - loss: 0.5033 - acc: 0.8662 - val_loss: 0.6219 - val_acc: 0.8242
Epoch 13/50
 - 4s - loss: 0.4650 - acc: 0.8687 - val_loss: 0.6036 - val_acc: 0.8370
Epoch 14/50
 - 4s - loss: 0.4107 - acc: 0.8895 - val_loss: 0.5970 - val_acc: 0.8287
Epoch 15/50
 - 4s - loss: 0.3774 - acc: 0.8948 - val_loss: 0.5911 - val_acc: 0.8407
Epoch 16/50
 - 5s - loss: 0.3395 - acc: 0.9055 - val_loss: 0.5928 - val_acc: 0.8385
Epoch 17/50
 - 7s - loss: 0.3170 - acc: 0.9126 - val_loss: 0.5955 - val_acc: 0.8340
Epoch 18/50
 - 4s - loss: 0.2746 - acc: 0.9208 - val_loss: 0.5938 - val_acc: 0.8392
Epoch 19/50
 - 5s - loss: 0.2632 - acc: 0.9276 - val_loss: 0.5886 - val_acc: 0.8370
Epoch 20/50
 - 4s - loss: 0.2398 - acc: 0.9336 - val_loss: 0.5905 - val_acc: 0.8340
Epoch 21/50
 - 5s - loss: 0.2162 - acc: 0.9351 - val_loss: 0.6196 - val_acc: 0.8467
Epoch 22/50
 - 7s - loss: 0.2135 - acc: 0.9424 - val_loss: 0.6138 - val_acc: 0.8445
Epoch 23/50
 - 5s - loss: 0.1947 - acc: 0.9454 - val_loss: 0.5994 - val_acc: 0.8407
Epoch 24/50
 - 7s - loss: 0.1810 - acc: 0.9506 - val_loss: 0.6259 - val_acc: 0.8392
Epoch 00024: early stopping
CNN training time: 119.65465879440308s
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_5 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_5 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_5 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_9 (Dense)              (None, 64)                81984
_________________________________________________________________
dropout_5 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_10 (Dense)             (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.931286204622163, 2.289418533266121, 1.8036560527933656, 1.4727563099523544, 1.265654153009936, 1.060859102160028, 0.9117990083438495, 0.8165671655500322, 0.7088545269681226, 0.6053373910867301, 0.5613120907354765, 0.5032932057392634, 0.4649978718971584, 0.4106539235458145, 0.37739783189525145, 0.3395356068540213, 0.317029738758027, 0.2745954696567934, 0.26317455478868224, 0.23983539698133594, 0.216179474985125, 0.21353341678535984, 0.19473246028849772, 0.18097328516633543]
[0.15509897, 0.3665748, 0.5038837, 0.5800551, 0.634678, 0.69982463, 0.74843395, 0.7697319, 0.80255574, 0.8346279, 0.84189427, 0.86619896, 0.86870456, 0.8895014, 0.89476323, 0.9055375, 0.91255325, 0.92082185, 0.9275871, 0.9336006, 0.935104, 0.94237036, 0.9453771, 0.95063895]
[2.460287505079265, 1.7986822525734834, 1.3946471872979496, 1.1695036456033665, 1.014882441794823, 0.9060964043427799, 0.8063708757834718, 0.7488607902147694, 0.7029176900671636, 0.6612167270018129, 0.6372310152585401, 0.6219421216202928, 0.6036303510955141, 0.5970110738795199, 0.5911154281203455, 0.592812657761562, 0.5954607254980508, 0.5937502748537595, 0.5886356741224276, 0.5905100412601806, 0.6196318161805997, 0.6138428867850526, 0.5993844767142271, 0.6258538577570398]
[0.3335837721824646, 0.4966190755367279, 0.5454545617103577, 0.6228399872779846, 0.671675443649292, 0.7054845690727234, 0.7558227181434631, 0.7881292104721069, 0.8091660141944885, 0.8241923451423645, 0.8309541940689087, 0.8241923451423645, 0.8369646668434143, 0.8287002444267273, 0.8407212495803833, 0.8384672999382019, 0.8339594006538391, 0.8392186164855957, 0.8369646668434143, 0.8339594006538391, 0.8467317819595337, 0.8444778323173523, 0.8407212495803833, 0.8392186164855957]

  32/2898 [..............................] - ETA: 26s
 288/2898 [=>............................] - ETA: 3s
 480/2898 [===>..........................] - ETA: 2s
 672/2898 [=====>........................] - ETA: 1s
 832/2898 [=======>......................] - ETA: 1s
1024/2898 [=========>....................] - ETA: 1s
1280/2898 [============>.................] - ETA: 0s
1504/2898 [==============>...............] - ETA: 0s
1760/2898 [=================>............] - ETA: 0s
1920/2898 [==================>...........] - ETA: 0s
2144/2898 [=====================>........] - ETA: 0s
2432/2898 [========================>.....] - ETA: 0s
2720/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 1s 354us/step
Accuracies per class for CNN
[0.68 0.88 0.96 0.85 0.83 0.47 0.78 0.59 0.77 0.   0.76 0.22 0.   0.79
 0.73 0.82 0.   0.    nan 0.91 0.85 0.26 0.85]
[2.931286204622163, 2.289418533266121, 1.8036560527933656, 1.4727563099523544, 1.265654153009936, 1.060859102160028, 0.9117990083438495, 0.8165671655500322, 0.7088545269681226, 0.6053373910867301, 0.5613120907354765, 0.5032932057392634, 0.4649978718971584, 0.4106539235458145, 0.37739783189525145, 0.3395356068540213, 0.317029738758027, 0.2745954696567934, 0.26317455478868224, 0.23983539698133594, 0.216179474985125, 0.21353341678535984, 0.19473246028849772, 0.18097328516633543]
[0.15509897, 0.3665748, 0.5038837, 0.5800551, 0.634678, 0.69982463, 0.74843395, 0.7697319, 0.80255574, 0.8346279, 0.84189427, 0.86619896, 0.86870456, 0.8895014, 0.89476323, 0.9055375, 0.91255325, 0.92082185, 0.9275871, 0.9336006, 0.935104, 0.94237036, 0.9453771, 0.95063895]
[2.460287505079265, 1.7986822525734834, 1.3946471872979496, 1.1695036456033665, 1.014882441794823, 0.9060964043427799, 0.8063708757834718, 0.7488607902147694, 0.7029176900671636, 0.6612167270018129, 0.6372310152585401, 0.6219421216202928, 0.6036303510955141, 0.5970110738795199, 0.5911154281203455, 0.592812657761562, 0.5954607254980508, 0.5937502748537595, 0.5886356741224276, 0.5905100412601806, 0.6196318161805997, 0.6138428867850526, 0.5993844767142271, 0.6258538577570398]
[0.3335837721824646, 0.4966190755367279, 0.5454545617103577, 0.6228399872779846, 0.671675443649292, 0.7054845690727234, 0.7558227181434631, 0.7881292104721069, 0.8091660141944885, 0.8241923451423645, 0.8309541940689087, 0.8241923451423645, 0.8369646668434143, 0.8287002444267273, 0.8407212495803833, 0.8384672999382019, 0.8339594006538391, 0.8392186164855957, 0.8369646668434143, 0.8339594006538391, 0.8467317819595337, 0.8444778323173523, 0.8407212495803833, 0.8392186164855957]
recall 0.7343
precision 0.7348
f1 0.7196
mcc 0.6963
RMSE: 3.322
classification report:
              precision    recall  f1-score   support

           1       0.62      0.68      0.65        72
           2       0.48      0.88      0.62        68
           3       0.90      0.96      0.93       252
           4       0.67      0.85      0.75       132
           5       0.90      0.83      0.86       191
           6       0.86      0.47      0.61       150
           7       0.79      0.78      0.78       240
           9       0.36      0.59      0.45       100
          10       0.61      0.77      0.68        61
          11       0.00      0.00      0.00        87
          12       0.88      0.76      0.81       595
          13       0.41      0.22      0.29        59
          14       0.00      0.00      0.00         6
          15       0.64      0.79      0.71        72
          16       0.76      0.73      0.75        30
          17       0.68      0.82      0.75       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.92      0.91      0.91       222
          22       0.59      0.85      0.69       168
          23       0.89      0.26      0.40        65
          24       0.54      0.85      0.66        82

   micro avg       0.72      0.72      0.72      2898
   macro avg       0.54      0.57      0.53      2898
weighted avg       0.72      0.72      0.70      2898

>#5: 71.912
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 5s - loss: 2.9954 - acc: 0.1448 - val_loss: 2.6452 - val_acc: 0.2427
Epoch 2/50
 - 4s - loss: 2.4308 - acc: 0.3155 - val_loss: 2.0213 - val_acc: 0.5124
Epoch 3/50
 - 4s - loss: 1.9545 - acc: 0.4640 - val_loss: 1.5307 - val_acc: 0.5838
Epoch 4/50
 - 4s - loss: 1.6027 - acc: 0.5420 - val_loss: 1.2529 - val_acc: 0.6597
Epoch 5/50
 - 4s - loss: 1.3442 - acc: 0.6184 - val_loss: 1.0569 - val_acc: 0.7085
Epoch 6/50
 - 4s - loss: 1.1399 - acc: 0.6800 - val_loss: 0.9332 - val_acc: 0.7325
Epoch 7/50
 - 5s - loss: 0.9788 - acc: 0.7256 - val_loss: 0.8199 - val_acc: 0.7528
Epoch 8/50
 - 5s - loss: 0.8489 - acc: 0.7677 - val_loss: 0.7603 - val_acc: 0.7678
Epoch 9/50
 - 4s - loss: 0.7466 - acc: 0.7905 - val_loss: 0.7124 - val_acc: 0.7791
Epoch 10/50
 - 5s - loss: 0.6602 - acc: 0.8178 - val_loss: 0.6665 - val_acc: 0.7896
Epoch 11/50
 - 7s - loss: 0.5767 - acc: 0.8369 - val_loss: 0.6438 - val_acc: 0.8047
Epoch 12/50
 - 4s - loss: 0.5407 - acc: 0.8532 - val_loss: 0.6285 - val_acc: 0.8107
Epoch 13/50
 - 7s - loss: 0.4842 - acc: 0.8637 - val_loss: 0.6192 - val_acc: 0.8122
Epoch 14/50
 - 4s - loss: 0.4464 - acc: 0.8725 - val_loss: 0.6459 - val_acc: 0.8062
Epoch 15/50
 - 4s - loss: 0.3939 - acc: 0.8900 - val_loss: 0.6529 - val_acc: 0.8039
Epoch 16/50
 - 5s - loss: 0.3678 - acc: 0.9000 - val_loss: 0.6145 - val_acc: 0.8182
Epoch 17/50
 - 4s - loss: 0.3303 - acc: 0.9080 - val_loss: 0.6123 - val_acc: 0.8189
Epoch 18/50
 - 5s - loss: 0.2911 - acc: 0.9186 - val_loss: 0.6450 - val_acc: 0.8122
Epoch 19/50
 - 4s - loss: 0.2788 - acc: 0.9211 - val_loss: 0.6346 - val_acc: 0.8197
Epoch 20/50
 - 4s - loss: 0.2601 - acc: 0.9251 - val_loss: 0.6061 - val_acc: 0.8204
Epoch 21/50
 - 6s - loss: 0.2500 - acc: 0.9281 - val_loss: 0.6416 - val_acc: 0.8122
Epoch 22/50
 - 6s - loss: 0.2442 - acc: 0.9298 - val_loss: 0.6245 - val_acc: 0.8152
Epoch 23/50
 - 6s - loss: 0.2045 - acc: 0.9419 - val_loss: 0.6120 - val_acc: 0.8257
Epoch 24/50
 - 7s - loss: 0.1995 - acc: 0.9419 - val_loss: 0.6594 - val_acc: 0.8152
Epoch 25/50
 - 5s - loss: 0.1883 - acc: 0.9491 - val_loss: 0.6283 - val_acc: 0.8325
Epoch 00025: early stopping
CNN training time: 125.67162585258484s
Model: "sequential_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_6 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_6 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_6 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_11 (Dense)             (None, 64)                81984
_________________________________________________________________
dropout_6 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_12 (Dense)             (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9954206841358175, 2.430807540217936, 1.9544885449972078, 1.6027239215611828, 1.344160213698737, 1.1399471118810056, 0.9787890466494358, 0.8489282112259421, 0.7466013982262676, 0.6601697939659945, 0.5767374554820746, 0.5407197942997799, 0.4842409391444577, 0.4463638534359702, 0.3938904866610673, 0.3678462433123698, 0.33029268775403636, 0.29108790158235215, 0.27884329887004683, 0.2601443611018719, 0.2500373939579914, 0.244189652329839, 0.2045323197189151, 0.19951975181480663, 0.1883186307098465]
[0.14482586, 0.3154598, 0.4640441, 0.5419694, 0.6183914, 0.68003005, 0.72563267, 0.7677274, 0.7905287, 0.81784016, 0.836883, 0.8531696, 0.8636933, 0.87246305, 0.8900025, 0.90002507, 0.9080431, 0.91856676, 0.9210724, 0.92508143, 0.9280882, 0.9298422, 0.9418692, 0.9418692, 0.94913554]
[2.645165298954156, 2.0213383856480682, 1.5306932936085964, 1.2528738949355507, 1.0569223646819033, 0.9332152620198626, 0.8198985242145591, 0.760307018666799, 0.7124477625006892, 0.6665293722312721, 0.6437824390733973, 0.6284939966984597, 0.6191903236820653, 0.6458893866580596, 0.6528517958211265, 0.6144623934419833, 0.6123463025698903, 0.6450287640165778, 0.6345573869753602, 0.6061024093428589, 0.6416027113097557, 0.6244896425473465, 0.6119556286284537, 0.659379127154541, 0.6283066351295084]
[0.24267467856407166, 0.5123966932296753, 0.5837715864181519, 0.6596543788909912, 0.7084898352622986, 0.7325319051742554, 0.7528174519538879, 0.7678437232971191, 0.7791134715080261, 0.7896318435668945, 0.8046581745147705, 0.8106686472892761, 0.8121712803840637, 0.8061608076095581, 0.8039068579673767, 0.8181818127632141, 0.8189331293106079, 0.8121712803840637, 0.8196844458580017, 0.8204357624053955, 0.8121712803840637, 0.8151765465736389, 0.8256949782371521, 0.8151765465736389, 0.8324568271636963]

  32/2898 [..............................] - ETA: 25s
 320/2898 [==>...........................] - ETA: 2s
 736/2898 [======>.......................] - ETA: 1s
1184/2898 [===========>..................] - ETA: 0s
1568/2898 [===============>..............] - ETA: 0s
1856/2898 [==================>...........] - ETA: 0s
2080/2898 [====================>.........] - ETA: 0s
2336/2898 [=======================>......] - ETA: 0s
2656/2898 [==========================>...] - ETA: 0s
2898/2898 [==============================] - 1s 262us/step
Accuracies per class for CNN
[0.74 0.97 0.98 0.81 0.81 0.31 0.92 0.96 0.52 0.   0.69 0.44 0.   0.81
 0.67 0.78 0.   0.   0.86 0.79 0.28 0.66]
[2.9954206841358175, 2.430807540217936, 1.9544885449972078, 1.6027239215611828, 1.344160213698737, 1.1399471118810056, 0.9787890466494358, 0.8489282112259421, 0.7466013982262676, 0.6601697939659945, 0.5767374554820746, 0.5407197942997799, 0.4842409391444577, 0.4463638534359702, 0.3938904866610673, 0.3678462433123698, 0.33029268775403636, 0.29108790158235215, 0.27884329887004683, 0.2601443611018719, 0.2500373939579914, 0.244189652329839, 0.2045323197189151, 0.19951975181480663, 0.1883186307098465]
[0.14482586, 0.3154598, 0.4640441, 0.5419694, 0.6183914, 0.68003005, 0.72563267, 0.7677274, 0.7905287, 0.81784016, 0.836883, 0.8531696, 0.8636933, 0.87246305, 0.8900025, 0.90002507, 0.9080431, 0.91856676, 0.9210724, 0.92508143, 0.9280882, 0.9298422, 0.9418692, 0.9418692, 0.94913554]
[2.645165298954156, 2.0213383856480682, 1.5306932936085964, 1.2528738949355507, 1.0569223646819033, 0.9332152620198626, 0.8198985242145591, 0.760307018666799, 0.7124477625006892, 0.6665293722312721, 0.6437824390733973, 0.6284939966984597, 0.6191903236820653, 0.6458893866580596, 0.6528517958211265, 0.6144623934419833, 0.6123463025698903, 0.6450287640165778, 0.6345573869753602, 0.6061024093428589, 0.6416027113097557, 0.6244896425473465, 0.6119556286284537, 0.659379127154541, 0.6283066351295084]
[0.24267467856407166, 0.5123966932296753, 0.5837715864181519, 0.6596543788909912, 0.7084898352622986, 0.7325319051742554, 0.7528174519538879, 0.7678437232971191, 0.7791134715080261, 0.7896318435668945, 0.8046581745147705, 0.8106686472892761, 0.8121712803840637, 0.8061608076095581, 0.8039068579673767, 0.8181818127632141, 0.8189331293106079, 0.8121712803840637, 0.8196844458580017, 0.8204357624053955, 0.8121712803840637, 0.8151765465736389, 0.8256949782371521, 0.8151765465736389, 0.8324568271636963]
recall 0.7237
precision 0.7371
f1 0.7053
mcc 0.6883
RMSE: 3.738
classification report:
              precision    recall  f1-score   support

           1       0.62      0.74      0.68        72
           2       0.54      0.97      0.69        68
           3       0.88      0.98      0.92       252
           4       0.91      0.81      0.86       132
           5       0.99      0.81      0.89       191
           6       0.90      0.31      0.46       150
           7       0.60      0.92      0.73       240
           9       0.56      0.96      0.71       100
          10       0.58      0.52      0.55        61
          11       0.00      0.00      0.00        87
          12       0.94      0.69      0.80       595
          13       0.43      0.44      0.44        59
          14       0.00      0.00      0.00         6
          15       0.46      0.81      0.59        72
          16       0.80      0.67      0.73        30
          17       0.68      0.78      0.72       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.95      0.86      0.90       222
          22       0.44      0.79      0.56       168
          23       0.60      0.28      0.38        65
          24       0.48      0.66      0.55        82

   micro avg       0.71      0.71      0.71      2898
   macro avg       0.56      0.59      0.55      2898
weighted avg       0.72      0.71      0.69      2898

>#6: 70.876
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 7s - loss: 2.9846 - acc: 0.1511 - val_loss: 2.6480 - val_acc: 0.2637
Epoch 2/50
 - 11s - loss: 2.4510 - acc: 0.3165 - val_loss: 1.9995 - val_acc: 0.5289
Epoch 3/50
 - 7s - loss: 1.9620 - acc: 0.4638 - val_loss: 1.5805 - val_acc: 0.6078
Epoch 4/50
 - 9s - loss: 1.6245 - acc: 0.5440 - val_loss: 1.3007 - val_acc: 0.6687
Epoch 5/50
 - 7s - loss: 1.3673 - acc: 0.6247 - val_loss: 1.0946 - val_acc: 0.7175
Epoch 6/50
 - 7s - loss: 1.1588 - acc: 0.6720 - val_loss: 0.9620 - val_acc: 0.7333
Epoch 7/50
 - 8s - loss: 1.0128 - acc: 0.7204 - val_loss: 0.8661 - val_acc: 0.7596
Epoch 8/50
 - 8s - loss: 0.8888 - acc: 0.7469 - val_loss: 0.7823 - val_acc: 0.7784
Epoch 9/50
 - 7s - loss: 0.7872 - acc: 0.7757 - val_loss: 0.7356 - val_acc: 0.7881
Epoch 10/50
 - 7s - loss: 0.6965 - acc: 0.8011 - val_loss: 0.7018 - val_acc: 0.7994
Epoch 11/50
 - 8s - loss: 0.6413 - acc: 0.8086 - val_loss: 0.6506 - val_acc: 0.8122
Epoch 12/50
 - 8s - loss: 0.5578 - acc: 0.8424 - val_loss: 0.6488 - val_acc: 0.8069
Epoch 13/50
 - 6s - loss: 0.4932 - acc: 0.8617 - val_loss: 0.6352 - val_acc: 0.8092
Epoch 14/50
 - 5s - loss: 0.4715 - acc: 0.8659 - val_loss: 0.6090 - val_acc: 0.8152
Epoch 15/50
 - 6s - loss: 0.4290 - acc: 0.8800 - val_loss: 0.6282 - val_acc: 0.7964
Epoch 16/50
 - 6s - loss: 0.3954 - acc: 0.8903 - val_loss: 0.6179 - val_acc: 0.8129
Epoch 17/50
 - 5s - loss: 0.3780 - acc: 0.8903 - val_loss: 0.6152 - val_acc: 0.8084
Epoch 18/50
 - 5s - loss: 0.3359 - acc: 0.9025 - val_loss: 0.6401 - val_acc: 0.7934
Epoch 19/50
 - 7s - loss: 0.2964 - acc: 0.9211 - val_loss: 0.6131 - val_acc: 0.8219
Epoch 00019: early stopping
CNN training time: 136.39088225364685s
Model: "sequential_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_7 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_7 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_7 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_13 (Dense)             (None, 64)                81984
_________________________________________________________________
dropout_7 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_14 (Dense)             (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9845624339069765, 2.451015055963671, 1.962033726868624, 1.6245494753503167, 1.3673434274836318, 1.1588432745652404, 1.0128120326267174, 0.8888242906876574, 0.787173978361763, 0.696517067434186, 0.6413356498828245, 0.5577528808265692, 0.49324699016633, 0.47145442520348185, 0.42904467913346067, 0.3953551312879536, 0.377959694152965, 0.3359469751131705, 0.2964286145488308]
[0.15108995, 0.31646204, 0.46379355, 0.5439739, 0.6246555, 0.67201203, 0.7203708, 0.7469306, 0.77574545, 0.8010524, 0.80856925, 0.84239537, 0.8616888, 0.8659484, 0.87997997, 0.89025307, 0.89025307, 0.90253067, 0.9210724]
[2.648033861331166, 1.9995349641584257, 1.5804646047746034, 1.300671893179663, 1.0946281048994733, 0.9619713183492422, 0.8661487356799492, 0.7823265106084873, 0.7356286271087613, 0.7017750458513364, 0.6505965087433352, 0.6487872510555268, 0.6351607355568523, 0.6090344883439255, 0.628161599691707, 0.6178824263288983, 0.6152230879547682, 0.6400520187781686, 0.6130942814554261]
[0.26371148228645325, 0.5289255976676941, 0.6078136563301086, 0.6686701774597168, 0.7175056338310242, 0.7332832217216492, 0.7595792412757874, 0.7783621549606323, 0.7881292104721069, 0.7993989586830139, 0.8121712803840637, 0.8069121241569519, 0.8091660141944885, 0.8151765465736389, 0.7963936924934387, 0.8129225969314575, 0.8084146976470947, 0.7933884263038635, 0.8219383955001831]

  32/2898 [..............................] - ETA: 49s
 160/2898 [>.............................] - ETA: 10s
 384/2898 [==>...........................] - ETA: 4s
 640/2898 [=====>........................] - ETA: 2s
 864/2898 [=======>......................] - ETA: 1s
1056/2898 [=========>....................] - ETA: 1s
1216/2898 [===========>..................] - ETA: 1s
1408/2898 [=============>................] - ETA: 1s
1568/2898 [===============>..............] - ETA: 0s
1728/2898 [================>.............] - ETA: 0s
2016/2898 [===================>..........] - ETA: 0s
2368/2898 [=======================>......] - ETA: 0s
2592/2898 [=========================>....] - ETA: 0s
2784/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 1s 453us/step
Accuracies per class for CNN
[0.51 0.87 0.98 0.96 0.85 0.68 0.82 0.87 0.51 0.   0.38 0.2  0.   0.74
 0.8  0.74 0.19 0.    nan 0.9  0.9  0.35 0.72]
[2.9845624339069765, 2.451015055963671, 1.962033726868624, 1.6245494753503167, 1.3673434274836318, 1.1588432745652404, 1.0128120326267174, 0.8888242906876574, 0.787173978361763, 0.696517067434186, 0.6413356498828245, 0.5577528808265692, 0.49324699016633, 0.47145442520348185, 0.42904467913346067, 0.3953551312879536, 0.377959694152965, 0.3359469751131705, 0.2964286145488308]
[0.15108995, 0.31646204, 0.46379355, 0.5439739, 0.6246555, 0.67201203, 0.7203708, 0.7469306, 0.77574545, 0.8010524, 0.80856925, 0.84239537, 0.8616888, 0.8659484, 0.87997997, 0.89025307, 0.89025307, 0.90253067, 0.9210724]
[2.648033861331166, 1.9995349641584257, 1.5804646047746034, 1.300671893179663, 1.0946281048994733, 0.9619713183492422, 0.8661487356799492, 0.7823265106084873, 0.7356286271087613, 0.7017750458513364, 0.6505965087433352, 0.6487872510555268, 0.6351607355568523, 0.6090344883439255, 0.628161599691707, 0.6178824263288983, 0.6152230879547682, 0.6400520187781686, 0.6130942814554261]
[0.26371148228645325, 0.5289255976676941, 0.6078136563301086, 0.6686701774597168, 0.7175056338310242, 0.7332832217216492, 0.7595792412757874, 0.7783621549606323, 0.7881292104721069, 0.7993989586830139, 0.8121712803840637, 0.8069121241569519, 0.8091660141944885, 0.8151765465736389, 0.7963936924934387, 0.8129225969314575, 0.8084146976470947, 0.7933884263038635, 0.8219383955001831]
recall 0.6758
precision 0.7179
f1 0.6523
mcc 0.6456
RMSE: 4.332
classification report:
              precision    recall  f1-score   support

           1       0.59      0.51      0.55        72
           2       0.42      0.87      0.57        68
           3       0.90      0.98      0.94       252
           4       0.77      0.96      0.86       132
           5       0.87      0.85      0.86       191
           6       0.76      0.68      0.72       150
           7       0.55      0.82      0.66       240
           9       0.54      0.87      0.67       100
          10       0.62      0.51      0.56        61
          11       0.00      0.00      0.00        87
          12       0.95      0.38      0.54       595
          13       0.75      0.20      0.32        59
          14       0.00      0.00      0.00         6
          15       0.63      0.74      0.68        72
          16       0.80      0.80      0.80        30
          17       0.71      0.74      0.73       156
          18       0.26      0.19      0.22        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.72      0.90      0.80       222
          22       0.42      0.90      0.58       168
          23       0.92      0.35      0.51        65
          24       0.53      0.72      0.61        82

   micro avg       0.66      0.66      0.66      2898
   macro avg       0.55      0.56      0.53      2898
weighted avg       0.70      0.66      0.64      2898

>#7: 66.184
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 9s - loss: 2.9561 - acc: 0.1566 - val_loss: 2.3471 - val_acc: 0.4117
Epoch 2/50
 - 5s - loss: 2.2825 - acc: 0.3656 - val_loss: 1.6470 - val_acc: 0.5485
Epoch 3/50
 - 5s - loss: 1.7495 - acc: 0.5219 - val_loss: 1.2770 - val_acc: 0.6289
Epoch 4/50
 - 5s - loss: 1.3837 - acc: 0.6111 - val_loss: 1.0346 - val_acc: 0.6942
Epoch 5/50
 - 6s - loss: 1.1469 - acc: 0.6880 - val_loss: 0.8761 - val_acc: 0.7423
Epoch 6/50
 - 7s - loss: 0.9842 - acc: 0.7306 - val_loss: 0.7843 - val_acc: 0.7573
Epoch 7/50
 - 6s - loss: 0.8660 - acc: 0.7612 - val_loss: 0.7260 - val_acc: 0.7731
Epoch 8/50
 - 6s - loss: 0.7405 - acc: 0.7945 - val_loss: 0.6474 - val_acc: 0.8144
Epoch 9/50
 - 6s - loss: 0.6793 - acc: 0.8123 - val_loss: 0.6273 - val_acc: 0.8069
Epoch 10/50
 - 9s - loss: 0.5881 - acc: 0.8364 - val_loss: 0.5740 - val_acc: 0.8257
Epoch 11/50
 - 9s - loss: 0.5361 - acc: 0.8467 - val_loss: 0.5555 - val_acc: 0.8317
Epoch 12/50
 - 6s - loss: 0.4856 - acc: 0.8634 - val_loss: 0.5510 - val_acc: 0.8392
Epoch 13/50
 - 6s - loss: 0.4407 - acc: 0.8720 - val_loss: 0.5531 - val_acc: 0.8347
Epoch 14/50
 - 6s - loss: 0.3966 - acc: 0.8895 - val_loss: 0.5527 - val_acc: 0.8295
Epoch 15/50
 - 6s - loss: 0.3670 - acc: 0.8950 - val_loss: 0.5705 - val_acc: 0.8174
Epoch 16/50
 - 6s - loss: 0.3236 - acc: 0.9136 - val_loss: 0.5692 - val_acc: 0.8167
Epoch 17/50
 - 6s - loss: 0.3037 - acc: 0.9088 - val_loss: 0.5560 - val_acc: 0.8242
Epoch 00017: early stopping
CNN training time: 112.7891161441803s
Model: "sequential_8"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_8 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_8 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_8 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_15 (Dense)             (None, 64)                81984
_________________________________________________________________
dropout_8 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_16 (Dense)             (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9561446771619315, 2.282469901273765, 1.749474505841381, 1.3836730281649467, 1.1468793086509705, 0.9841505447311588, 0.8660466863480435, 0.740499566239964, 0.6792810544578941, 0.5881087628910885, 0.5360580618754384, 0.48560082447736164, 0.44070053163263817, 0.3966254491894226, 0.3670059239941147, 0.3236195602314465, 0.30372892156374676]
[0.15660235, 0.36557254, 0.5219243, 0.61112505, 0.6880481, 0.7306439, 0.7612127, 0.7945377, 0.81232774, 0.83638185, 0.84665495, 0.8634427, 0.8719619, 0.8895014, 0.8950138, 0.9135555, 0.90879476]
[2.347107717544848, 1.6469970816714168, 1.2770017529512419, 1.034576213117451, 0.8760542252599902, 0.7843179700589669, 0.7260415048403547, 0.6473828952976066, 0.6272651004801966, 0.5740045088634229, 0.5555473626203797, 0.5509597102181348, 0.5531381843984634, 0.5526802602158366, 0.5704813442706126, 0.5691879914339381, 0.555971080809519]
[0.41172051429748535, 0.5484598278999329, 0.6288504600524902, 0.6942148804664612, 0.7422990202903748, 0.757325291633606, 0.7731029391288757, 0.8144252300262451, 0.8069121241569519, 0.8256949782371521, 0.8317055106163025, 0.8392186164855957, 0.8347107172012329, 0.8294515609741211, 0.8174304962158203, 0.8166791796684265, 0.8241923451423645]

  32/2898 [..............................] - ETA: 27s
 256/2898 [=>............................] - ETA: 3s
 576/2898 [====>.........................] - ETA: 1s
1024/2898 [=========>....................] - ETA: 0s
1408/2898 [=============>................] - ETA: 0s
1664/2898 [================>.............] - ETA: 0s
1824/2898 [=================>............] - ETA: 0s
2048/2898 [====================>.........] - ETA: 0s
2208/2898 [=====================>........] - ETA: 0s
2464/2898 [========================>.....] - ETA: 0s
2752/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 1s 301us/step
Accuracies per class for CNN
[0.75 0.9  0.98 0.95 0.88 0.64 0.9  0.95 0.54 0.   0.65 0.25 0.   0.72
 0.67 0.74 0.06 0.   0.9  0.83 0.38 0.82]
[2.9561446771619315, 2.282469901273765, 1.749474505841381, 1.3836730281649467, 1.1468793086509705, 0.9841505447311588, 0.8660466863480435, 0.740499566239964, 0.6792810544578941, 0.5881087628910885, 0.5360580618754384, 0.48560082447736164, 0.44070053163263817, 0.3966254491894226, 0.3670059239941147, 0.3236195602314465, 0.30372892156374676]
[0.15660235, 0.36557254, 0.5219243, 0.61112505, 0.6880481, 0.7306439, 0.7612127, 0.7945377, 0.81232774, 0.83638185, 0.84665495, 0.8634427, 0.8719619, 0.8895014, 0.8950138, 0.9135555, 0.90879476]
[2.347107717544848, 1.6469970816714168, 1.2770017529512419, 1.034576213117451, 0.8760542252599902, 0.7843179700589669, 0.7260415048403547, 0.6473828952976066, 0.6272651004801966, 0.5740045088634229, 0.5555473626203797, 0.5509597102181348, 0.5531381843984634, 0.5526802602158366, 0.5704813442706126, 0.5691879914339381, 0.555971080809519]
[0.41172051429748535, 0.5484598278999329, 0.6288504600524902, 0.6942148804664612, 0.7422990202903748, 0.757325291633606, 0.7731029391288757, 0.8144252300262451, 0.8069121241569519, 0.8256949782371521, 0.8317055106163025, 0.8392186164855957, 0.8347107172012329, 0.8294515609741211, 0.8174304962158203, 0.8166791796684265, 0.8241923451423645]
recall 0.7706
precision 0.7599
f1 0.7488
mcc 0.7121
RMSE: 3.702
classification report:
              precision    recall  f1-score   support

           1       0.59      0.75      0.66        72
           2       0.50      0.90      0.65        68
           3       0.91      0.98      0.94       252
           4       0.84      0.95      0.89       132
           5       0.92      0.88      0.90       191
           6       0.88      0.64      0.74       150
           7       0.62      0.90      0.74       240
           9       0.54      0.95      0.69       100
          10       0.61      0.54      0.57        61
          11       0.00      0.00      0.00        87
          12       0.91      0.65      0.76       595
          13       0.34      0.25      0.29        59
          14       0.00      0.00      0.00         6
          15       0.62      0.72      0.67        72
          16       0.80      0.67      0.73        30
          17       0.72      0.74      0.73       156
          18       0.11      0.06      0.07        36
          19       0.00      0.00      0.00        54
          21       0.82      0.90      0.86       222
          22       0.56      0.83      0.67       168
          23       0.83      0.38      0.53        65
          24       0.59      0.82      0.68        82

   micro avg       0.73      0.73      0.73      2898
   macro avg       0.58      0.61      0.58      2898
weighted avg       0.72      0.73      0.71      2898

>#8: 73.154
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 7s - loss: 2.9340 - acc: 0.1797 - val_loss: 2.4767 - val_acc: 0.3366
Epoch 2/50
 - 7s - loss: 2.3231 - acc: 0.3470 - val_loss: 1.8538 - val_acc: 0.4853
Epoch 3/50
 - 7s - loss: 1.8524 - acc: 0.4876 - val_loss: 1.4693 - val_acc: 0.5943
Epoch 4/50
 - 7s - loss: 1.5194 - acc: 0.5866 - val_loss: 1.2275 - val_acc: 0.6732
Epoch 5/50
 - 8s - loss: 1.2831 - acc: 0.6432 - val_loss: 0.9941 - val_acc: 0.7446
Epoch 6/50
 - 7s - loss: 1.0780 - acc: 0.7011 - val_loss: 0.8597 - val_acc: 0.7776
Epoch 7/50
 - 5s - loss: 0.9300 - acc: 0.7377 - val_loss: 0.7557 - val_acc: 0.7866
Epoch 8/50
 - 6s - loss: 0.8096 - acc: 0.7730 - val_loss: 0.6791 - val_acc: 0.8234
Epoch 9/50
 - 8s - loss: 0.7159 - acc: 0.7998 - val_loss: 0.6281 - val_acc: 0.8234
Epoch 10/50
 - 7s - loss: 0.6374 - acc: 0.8254 - val_loss: 0.5941 - val_acc: 0.8415
Epoch 11/50
 - 6s - loss: 0.5761 - acc: 0.8384 - val_loss: 0.5522 - val_acc: 0.8580
Epoch 12/50
 - 5s - loss: 0.5201 - acc: 0.8554 - val_loss: 0.5399 - val_acc: 0.8550
Epoch 13/50
 - 6s - loss: 0.4723 - acc: 0.8692 - val_loss: 0.5085 - val_acc: 0.8685
Epoch 14/50
 - 5s - loss: 0.4423 - acc: 0.8772 - val_loss: 0.5072 - val_acc: 0.8640
Epoch 15/50
 - 8s - loss: 0.3980 - acc: 0.8870 - val_loss: 0.5145 - val_acc: 0.8618
Epoch 16/50
 - 7s - loss: 0.3633 - acc: 0.9038 - val_loss: 0.5132 - val_acc: 0.8625
Epoch 17/50
 - 9s - loss: 0.3468 - acc: 0.9005 - val_loss: 0.4970 - val_acc: 0.8708
Epoch 18/50
 - 7s - loss: 0.3102 - acc: 0.9108 - val_loss: 0.4925 - val_acc: 0.8640
Epoch 19/50
 - 6s - loss: 0.2747 - acc: 0.9198 - val_loss: 0.5305 - val_acc: 0.8550
Epoch 20/50
 - 7s - loss: 0.2597 - acc: 0.9226 - val_loss: 0.5047 - val_acc: 0.8625
Epoch 21/50
 - 6s - loss: 0.2442 - acc: 0.9298 - val_loss: 0.5095 - val_acc: 0.8595
Epoch 22/50
 - 7s - loss: 0.2341 - acc: 0.9308 - val_loss: 0.5159 - val_acc: 0.8603
Epoch 23/50
 - 5s - loss: 0.2010 - acc: 0.9459 - val_loss: 0.5002 - val_acc: 0.8655
Epoch 00023: early stopping
CNN training time: 155.04785442352295s
Model: "sequential_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_9 (Conv1D)            (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_9 (MaxPooling1 (None, 20, 64)            0
_________________________________________________________________
flatten_9 (Flatten)          (None, 1280)              0
_________________________________________________________________
dense_17 (Dense)             (None, 64)                81984
_________________________________________________________________
dropout_9 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_18 (Dense)             (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9339806062779132, 2.3231340263928426, 1.8523760975056858, 1.519422860640171, 1.2831106017220255, 1.0780164249921818, 0.9300048887798251, 0.8095838032340604, 0.7158885955734939, 0.6373750071640544, 0.5761056100312935, 0.5201414486446304, 0.47226189433841353, 0.44229228288723765, 0.39801714256495757, 0.36329365481240516, 0.3468114555901989, 0.31023222195389777, 0.27471508171510883, 0.25970022469301013, 0.24422956799371062, 0.23407007000003305, 0.20100761729307842]
[0.17965423, 0.34703082, 0.4875971, 0.5865698, 0.6431972, 0.7010774, 0.73765975, 0.7729892, 0.79979956, 0.8253571, 0.83838636, 0.8554247, 0.8692057, 0.87722373, 0.88699573, 0.9037835, 0.90052617, 0.9107993, 0.9198196, 0.9225758, 0.9298422, 0.9308444, 0.9458782]
[2.4767066321365823, 1.8537763692242681, 1.4692852664976797, 1.227473018840683, 0.9941170611310327, 0.8596566999184176, 0.7556698027547591, 0.6790924203560417, 0.628120798914631, 0.5941249313907716, 0.5522204346426187, 0.5398626296956283, 0.5085220056374128, 0.5071909898253729, 0.5145437269858004, 0.5132431595545219, 0.49699612097144874, 0.4924597407128762, 0.5305009842925488, 0.5046753156589315, 0.5094656927793738, 0.5158972670282437, 0.5001725728732648]
[0.3365890383720398, 0.4853493571281433, 0.594290018081665, 0.6731780767440796, 0.7445529699325562, 0.7776108384132385, 0.7866265773773193, 0.8234410285949707, 0.8234410285949707, 0.8414725661277771, 0.8580015301704407, 0.8549962639808655, 0.8685199022293091, 0.8640120029449463, 0.8617580533027649, 0.8625093698501587, 0.8707738518714905, 0.8640120029449463, 0.8549962639808655, 0.8625093698501587, 0.8595041036605835, 0.8602554202079773, 0.8655146360397339]

  32/2898 [..............................] - ETA: 59s
 256/2898 [=>............................] - ETA: 7s
 512/2898 [====>.........................] - ETA: 3s
 832/2898 [=======>......................] - ETA: 2s
 992/2898 [=========>....................] - ETA: 1s
1184/2898 [===========>..................] - ETA: 1s
1344/2898 [============>.................] - ETA: 1s
1568/2898 [===============>..............] - ETA: 0s
1728/2898 [================>.............] - ETA: 0s
2016/2898 [===================>..........] - ETA: 0s
2208/2898 [=====================>........] - ETA: 0s
2496/2898 [========================>.....] - ETA: 0s
2656/2898 [==========================>...] - ETA: 0s
2784/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 1s 497us/step
Accuracies per class for CNN
[0.64 0.87 0.98 0.98 0.82 0.36 0.86 0.93 0.38 0.   0.61 0.36 0.   0.72
 0.73 0.81 0.   0.    nan 0.92 0.88 0.31 0.76]
[2.9339806062779132, 2.3231340263928426, 1.8523760975056858, 1.519422860640171, 1.2831106017220255, 1.0780164249921818, 0.9300048887798251, 0.8095838032340604, 0.7158885955734939, 0.6373750071640544, 0.5761056100312935, 0.5201414486446304, 0.47226189433841353, 0.44229228288723765, 0.39801714256495757, 0.36329365481240516, 0.3468114555901989, 0.31023222195389777, 0.27471508171510883, 0.25970022469301013, 0.24422956799371062, 0.23407007000003305, 0.20100761729307842]
[0.17965423, 0.34703082, 0.4875971, 0.5865698, 0.6431972, 0.7010774, 0.73765975, 0.7729892, 0.79979956, 0.8253571, 0.83838636, 0.8554247, 0.8692057, 0.87722373, 0.88699573, 0.9037835, 0.90052617, 0.9107993, 0.9198196, 0.9225758, 0.9298422, 0.9308444, 0.9458782]
[2.4767066321365823, 1.8537763692242681, 1.4692852664976797, 1.227473018840683, 0.9941170611310327, 0.8596566999184176, 0.7556698027547591, 0.6790924203560417, 0.628120798914631, 0.5941249313907716, 0.5522204346426187, 0.5398626296956283, 0.5085220056374128, 0.5071909898253729, 0.5145437269858004, 0.5132431595545219, 0.49699612097144874, 0.4924597407128762, 0.5305009842925488, 0.5046753156589315, 0.5094656927793738, 0.5158972670282437, 0.5001725728732648]
[0.3365890383720398, 0.4853493571281433, 0.594290018081665, 0.6731780767440796, 0.7445529699325562, 0.7776108384132385, 0.7866265773773193, 0.8234410285949707, 0.8234410285949707, 0.8414725661277771, 0.8580015301704407, 0.8549962639808655, 0.8685199022293091, 0.8640120029449463, 0.8617580533027649, 0.8625093698501587, 0.8707738518714905, 0.8640120029449463, 0.8549962639808655, 0.8625093698501587, 0.8595041036605835, 0.8602554202079773, 0.8655146360397339]
recall 0.7167
precision 0.7115
f1 0.6911
mcc 0.6803
RMSE: 3.930
classification report:
              precision    recall  f1-score   support

           1       0.88      0.64      0.74        72
           2       0.45      0.87      0.59        68
           3       0.89      0.98      0.94       252
           4       0.75      0.98      0.85       132
           5       0.80      0.82      0.81       191
           6       0.86      0.36      0.51       150
           7       0.63      0.86      0.73       240
           9       0.54      0.93      0.68       100
          10       0.51      0.38      0.43        61
          11       0.00      0.00      0.00        87
          12       0.86      0.61      0.71       595
          13       0.47      0.36      0.40        59
          14       0.00      0.00      0.00         6
          15       0.74      0.72      0.73        72
          16       0.92      0.73      0.81        30
          17       0.67      0.81      0.73       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.78      0.92      0.85       222
          22       0.54      0.88      0.67       168
          23       0.80      0.31      0.44        65
          24       0.53      0.76      0.62        82

   micro avg       0.70      0.70      0.70      2898
   macro avg       0.55      0.56      0.53      2898
weighted avg       0.70      0.70      0.68      2898

>#9: 70.186
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 8s - loss: 2.9177 - acc: 0.1967 - val_loss: 2.3899 - val_acc: 0.3539
Epoch 2/50
 - 10s - loss: 2.2444 - acc: 0.3786 - val_loss: 1.7919 - val_acc: 0.4936
Epoch 3/50
 - 9s - loss: 1.8076 - acc: 0.5014 - val_loss: 1.4456 - val_acc: 0.5778
Epoch 4/50
 - 7s - loss: 1.4667 - acc: 0.5913 - val_loss: 1.2060 - val_acc: 0.6341
Epoch 5/50
 - 7s - loss: 1.2242 - acc: 0.6605 - val_loss: 1.0436 - val_acc: 0.6769
Epoch 6/50
 - 6s - loss: 1.0589 - acc: 0.7003 - val_loss: 0.9246 - val_acc: 0.7273
Epoch 7/50
 - 6s - loss: 0.9094 - acc: 0.7457 - val_loss: 0.8441 - val_acc: 0.7355
Epoch 8/50
 - 7s - loss: 0.7941 - acc: 0.7742 - val_loss: 0.7903 - val_acc: 0.7468
Epoch 9/50
 - 7s - loss: 0.6994 - acc: 0.7988 - val_loss: 0.7378 - val_acc: 0.7724
Epoch 10/50
 - 5s - loss: 0.6350 - acc: 0.8231 - val_loss: 0.7009 - val_acc: 0.7904
Epoch 11/50
 - 4s - loss: 0.5643 - acc: 0.8401 - val_loss: 0.6684 - val_acc: 0.8092
Epoch 12/50
 - 4s - loss: 0.5253 - acc: 0.8589 - val_loss: 0.6666 - val_acc: 0.8002
Epoch 13/50
 - 4s - loss: 0.4832 - acc: 0.8612 - val_loss: 0.6651 - val_acc: 0.8009
Epoch 14/50
 - 4s - loss: 0.4262 - acc: 0.8800 - val_loss: 0.6625 - val_acc: 0.7844
Epoch 15/50
 - 5s - loss: 0.3983 - acc: 0.8877 - val_loss: 0.6415 - val_acc: 0.8099
Epoch 16/50
 - 5s - loss: 0.3619 - acc: 0.8988 - val_loss: 0.6624 - val_acc: 0.7956
Epoch 17/50
 - 6s - loss: 0.3324 - acc: 0.9060 - val_loss: 0.6427 - val_acc: 0.7949
Epoch 18/50
 - 5s - loss: 0.3133 - acc: 0.9108 - val_loss: 0.6599 - val_acc: 0.7806
Epoch 19/50
 - 6s - loss: 0.2873 - acc: 0.9171 - val_loss: 0.6258 - val_acc: 0.8122
Epoch 20/50
 - 5s - loss: 0.2623 - acc: 0.9266 - val_loss: 0.6462 - val_acc: 0.8017
Epoch 21/50
 - 5s - loss: 0.2451 - acc: 0.9321 - val_loss: 0.6435 - val_acc: 0.8062
Epoch 22/50
 - 7s - loss: 0.2332 - acc: 0.9349 - val_loss: 0.6510 - val_acc: 0.8054
Epoch 23/50
 - 5s - loss: 0.2122 - acc: 0.9394 - val_loss: 0.6916 - val_acc: 0.7881
Epoch 24/50
 - 5s - loss: 0.2042 - acc: 0.9409 - val_loss: 0.6141 - val_acc: 0.8242
Epoch 25/50
 - 6s - loss: 0.1911 - acc: 0.9444 - val_loss: 0.6366 - val_acc: 0.8062
Epoch 26/50
 - 5s - loss: 0.1811 - acc: 0.9484 - val_loss: 0.6635 - val_acc: 0.8099
Epoch 27/50
 - 5s - loss: 0.1714 - acc: 0.9484 - val_loss: 0.6638 - val_acc: 0.8017
Epoch 28/50
 - 6s - loss: 0.1580 - acc: 0.9557 - val_loss: 0.6744 - val_acc: 0.8182
Epoch 29/50
 - 4s - loss: 0.1482 - acc: 0.9551 - val_loss: 0.6687 - val_acc: 0.8204
Epoch 00029: early stopping
CNN training time: 171.85695147514343s
Model: "sequential_10"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d_10 (Conv1D)           (None, 40, 64)            10688
_________________________________________________________________
max_pooling1d_10 (MaxPooling (None, 20, 64)            0
_________________________________________________________________
flatten_10 (Flatten)         (None, 1280)              0
_________________________________________________________________
dense_19 (Dense)             (None, 64)                81984
_________________________________________________________________
dropout_10 (Dropout)         (None, 64)                0
_________________________________________________________________
dense_20 (Dense)             (None, 24)                1560
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9177415352646077, 2.2444011586450134, 1.8075604744235008, 1.4666702037551624, 1.2241968006147406, 1.058905847989484, 0.9094078060738001, 0.79414273634265, 0.6993725351563161, 0.6350089698599148, 0.5643474867005499, 0.5253010692474779, 0.48316625504472355, 0.4261568979521731, 0.3982972347821656, 0.36192784404436773, 0.3324076862515652, 0.3133144305052353, 0.28726456497016406, 0.2622864051927146, 0.2450827331004228, 0.23316994135197983, 0.21223087694502452, 0.20424114951863126, 0.19110306850867656, 0.18106409900036696, 0.17136640584074336, 0.1580379290105801, 0.14819771415282862]
[0.19669256, 0.37860185, 0.5013781, 0.59133047, 0.6604861, 0.7003257, 0.74567777, 0.77424204, 0.7987973, 0.823102, 0.84014034, 0.8589326, 0.8611877, 0.87997997, 0.8877474, 0.89877224, 0.9060386, 0.9107993, 0.9170634, 0.92658484, 0.9320972, 0.93485343, 0.93936354, 0.94086695, 0.94437486, 0.94838387, 0.94838387, 0.9556502, 0.9551491]
[2.3898992862672754, 1.7919492744060497, 1.4455707745387265, 1.206040737085509, 1.0435766031745903, 0.924601216116888, 0.8441326801506291, 0.7902736588448404, 0.7378387021755899, 0.7009143596450336, 0.6683877411256121, 0.6665745079457014, 0.6650932233456653, 0.6624621807668511, 0.6414512575217735, 0.6624493807161792, 0.6427069201182163, 0.6598958202837064, 0.6257739764876848, 0.6461671317180395, 0.6434764898749905, 0.6509539611545881, 0.6915589253354192, 0.6140756500737524, 0.6365963267012557, 0.6634996911034817, 0.663794250424874, 0.6743971560353712, 0.6686800780679277]
[0.3538692593574524, 0.4936138093471527, 0.5777610540390015, 0.6341096758842468, 0.6769346594810486, 0.7272727489471436, 0.7355371713638306, 0.7468069195747375, 0.7723516225814819, 0.7903831601142883, 0.8091660141944885, 0.8001502752304077, 0.8009015917778015, 0.7843726277351379, 0.8099173307418823, 0.7956423759460449, 0.7948910593986511, 0.7806161046028137, 0.8121712803840637, 0.8016529083251953, 0.8061608076095581, 0.8054094910621643, 0.7881292104721069, 0.8241923451423645, 0.8061608076095581, 0.8099173307418823, 0.8016529083251953, 0.8181818127632141, 0.8204357624053955]

  32/2898 [..............................] - ETA: 41s
 416/2898 [===>..........................] - ETA: 3s
 640/2898 [=====>........................] - ETA: 2s
 928/2898 [========>.....................] - ETA: 1s
1120/2898 [==========>...................] - ETA: 1s
1472/2898 [==============>...............] - ETA: 0s
1728/2898 [================>.............] - ETA: 0s
1824/2898 [=================>............] - ETA: 0s
1984/2898 [===================>..........] - ETA: 0s
2080/2898 [====================>.........] - ETA: 0s
2240/2898 [======================>.......] - ETA: 0s
2464/2898 [========================>.....] - ETA: 0s
2688/2898 [==========================>...] - ETA: 0s
2848/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 423us/step
Accuracies per class for CNN
[0.86 0.87 0.98 0.76 0.72 0.58 0.85 0.88 0.82 0.   0.54 0.37 0.   0.81
 0.7  0.81 0.14 0.    nan 0.87 0.84 0.28 0.73]
[2.9177415352646077, 2.2444011586450134, 1.8075604744235008, 1.4666702037551624, 1.2241968006147406, 1.058905847989484, 0.9094078060738001, 0.79414273634265, 0.6993725351563161, 0.6350089698599148, 0.5643474867005499, 0.5253010692474779, 0.48316625504472355, 0.4261568979521731, 0.3982972347821656, 0.36192784404436773, 0.3324076862515652, 0.3133144305052353, 0.28726456497016406, 0.2622864051927146, 0.2450827331004228, 0.23316994135197983, 0.21223087694502452, 0.20424114951863126, 0.19110306850867656, 0.18106409900036696, 0.17136640584074336, 0.1580379290105801, 0.14819771415282862]
[0.19669256, 0.37860185, 0.5013781, 0.59133047, 0.6604861, 0.7003257, 0.74567777, 0.77424204, 0.7987973, 0.823102, 0.84014034, 0.8589326, 0.8611877, 0.87997997, 0.8877474, 0.89877224, 0.9060386, 0.9107993, 0.9170634, 0.92658484, 0.9320972, 0.93485343, 0.93936354, 0.94086695, 0.94437486, 0.94838387, 0.94838387, 0.9556502, 0.9551491]
[2.3898992862672754, 1.7919492744060497, 1.4455707745387265, 1.206040737085509, 1.0435766031745903, 0.924601216116888, 0.8441326801506291, 0.7902736588448404, 0.7378387021755899, 0.7009143596450336, 0.6683877411256121, 0.6665745079457014, 0.6650932233456653, 0.6624621807668511, 0.6414512575217735, 0.6624493807161792, 0.6427069201182163, 0.6598958202837064, 0.6257739764876848, 0.6461671317180395, 0.6434764898749905, 0.6509539611545881, 0.6915589253354192, 0.6140756500737524, 0.6365963267012557, 0.6634996911034817, 0.663794250424874, 0.6743971560353712, 0.6686800780679277]
[0.3538692593574524, 0.4936138093471527, 0.5777610540390015, 0.6341096758842468, 0.6769346594810486, 0.7272727489471436, 0.7355371713638306, 0.7468069195747375, 0.7723516225814819, 0.7903831601142883, 0.8091660141944885, 0.8001502752304077, 0.8009015917778015, 0.7843726277351379, 0.8099173307418823, 0.7956423759460449, 0.7948910593986511, 0.7806161046028137, 0.8121712803840637, 0.8016529083251953, 0.8061608076095581, 0.8054094910621643, 0.7881292104721069, 0.8241923451423645, 0.8061608076095581, 0.8099173307418823, 0.8016529083251953, 0.8181818127632141, 0.8204357624053955]
recall 0.704
precision 0.7198
f1 0.6864
mcc 0.6702
RMSE: 4.050
classification report:
              precision    recall  f1-score   support

           1       0.57      0.86      0.69        72
           2       0.50      0.87      0.63        68
           3       0.88      0.98      0.92       252
           4       0.90      0.76      0.82       132
           5       0.82      0.72      0.77       191
           6       0.55      0.58      0.56       150
           7       0.63      0.85      0.72       240
           9       0.50      0.88      0.64       100
          10       0.71      0.82      0.76        61
          11       0.00      0.00      0.00        87
          12       0.96      0.54      0.69       595
          13       0.67      0.37      0.48        59
          14       0.00      0.00      0.00         6
          15       0.79      0.81      0.80        72
          16       0.88      0.70      0.78        30
          17       0.70      0.81      0.75       156
          18       0.28      0.14      0.19        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.78      0.87      0.82       222
          22       0.45      0.84      0.59       168
          23       0.64      0.28      0.39        65
          24       0.52      0.73      0.61        82

   micro avg       0.69      0.69      0.69      2898
   macro avg       0.55      0.58      0.55      2898
weighted avg       0.70      0.69      0.67      2898

>#10: 68.944
Output from summarize_results:
Accuracy: 0.701% (+/-0.023)
Loss: 1.193% (+/-0.112)
Recall: 0.719% (+/-0.029)
Precision: 0.734% (+/-0.023)
F1: 0.698% (+/-0.030)
MCC: 0.681% (+/-0.022)
RMSE: 3.837% (+/-0.295)
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 17s - loss: 3.1608 - acc: 0.0688 - val_loss: 3.1092 - val_acc: 0.1052
Epoch 2/50
 - 14s - loss: 3.0689 - acc: 0.1315 - val_loss: 2.9954 - val_acc: 0.1446
Epoch 3/50
 - 17s - loss: 2.9330 - acc: 0.1557 - val_loss: 2.8585 - val_acc: 0.1737
Epoch 4/50
 - 16s - loss: 2.7958 - acc: 0.1938 - val_loss: 2.7269 - val_acc: 0.2066
Epoch 5/50
 - 16s - loss: 2.6586 - acc: 0.2210 - val_loss: 2.6053 - val_acc: 0.2441
Epoch 6/50
 - 20s - loss: 2.5199 - acc: 0.2636 - val_loss: 2.4951 - val_acc: 0.2920
Epoch 7/50
 - 32s - loss: 2.3910 - acc: 0.2995 - val_loss: 2.4011 - val_acc: 0.3183
Epoch 8/50
 - 26s - loss: 2.2673 - acc: 0.3491 - val_loss: 2.3063 - val_acc: 0.3399
Epoch 9/50
 - 23s - loss: 2.1437 - acc: 0.3827 - val_loss: 2.2275 - val_acc: 0.3549
Epoch 10/50
 - 30s - loss: 2.0346 - acc: 0.4186 - val_loss: 2.1607 - val_acc: 0.3897
Epoch 11/50
 - 25s - loss: 1.9396 - acc: 0.4503 - val_loss: 2.0998 - val_acc: 0.4094
Epoch 12/50
 - 24s - loss: 1.8565 - acc: 0.4658 - val_loss: 2.0478 - val_acc: 0.3906
Epoch 13/50
 - 23s - loss: 1.7842 - acc: 0.4825 - val_loss: 2.0006 - val_acc: 0.3906
Epoch 14/50
 - 23s - loss: 1.7198 - acc: 0.5034 - val_loss: 1.9539 - val_acc: 0.4019
Epoch 15/50
 - 17s - loss: 1.6610 - acc: 0.5206 - val_loss: 1.9096 - val_acc: 0.4197
Epoch 16/50
 - 16s - loss: 1.6064 - acc: 0.5314 - val_loss: 1.8720 - val_acc: 0.4347
Epoch 17/50
 - 18s - loss: 1.5577 - acc: 0.5502 - val_loss: 1.8515 - val_acc: 0.4451
Epoch 18/50
 - 15s - loss: 1.5122 - acc: 0.5689 - val_loss: 1.8554 - val_acc: 0.4592
Epoch 19/50
 - 16s - loss: 1.4677 - acc: 0.5913 - val_loss: 1.8542 - val_acc: 0.4798
Epoch 20/50
 - 17s - loss: 1.4261 - acc: 0.6044 - val_loss: 1.8509 - val_acc: 0.4854
Epoch 21/50
 - 17s - loss: 1.3884 - acc: 0.6115 - val_loss: 1.8154 - val_acc: 0.5023
Epoch 22/50
 - 17s - loss: 1.3539 - acc: 0.6234 - val_loss: 1.7679 - val_acc: 0.5146
Epoch 23/50
 - 20s - loss: 1.3129 - acc: 0.6333 - val_loss: 1.8202 - val_acc: 0.4977
Epoch 24/50
 - 18s - loss: 1.2765 - acc: 0.6469 - val_loss: 1.8174 - val_acc: 0.4967
Epoch 25/50
 - 14s - loss: 1.2418 - acc: 0.6638 - val_loss: 1.7841 - val_acc: 0.5052
Epoch 26/50
 - 15s - loss: 1.2062 - acc: 0.6758 - val_loss: 1.7385 - val_acc: 0.5174
Epoch 27/50
 - 15s - loss: 1.1779 - acc: 0.6765 - val_loss: 1.7513 - val_acc: 0.5108
Epoch 28/50
 - 15s - loss: 1.1438 - acc: 0.6843 - val_loss: 1.7634 - val_acc: 0.5146
Epoch 29/50
 - 16s - loss: 1.1110 - acc: 0.7052 - val_loss: 1.6791 - val_acc: 0.5390
Epoch 30/50
 - 16s - loss: 1.0798 - acc: 0.7146 - val_loss: 1.6480 - val_acc: 0.5427
Epoch 31/50
 - 15s - loss: 1.0518 - acc: 0.7256 - val_loss: 1.6663 - val_acc: 0.5408
Epoch 32/50
 - 16s - loss: 1.0237 - acc: 0.7353 - val_loss: 1.6428 - val_acc: 0.5521
Epoch 33/50
 - 14s - loss: 0.9953 - acc: 0.7437 - val_loss: 1.6815 - val_acc: 0.5371
Epoch 34/50
 - 17s - loss: 0.9747 - acc: 0.7404 - val_loss: 1.7792 - val_acc: 0.5352
Epoch 35/50
 - 17s - loss: 0.9416 - acc: 0.7630 - val_loss: 1.6652 - val_acc: 0.5502
Epoch 36/50
 - 17s - loss: 0.9157 - acc: 0.7679 - val_loss: 1.6510 - val_acc: 0.5596
Epoch 37/50
 - 14s - loss: 0.8986 - acc: 0.7677 - val_loss: 1.7350 - val_acc: 0.5559
Epoch 00037: early stopping
LSTM training time: 680.2143912315369s
Model: "sequential_11"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_1 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_21 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_22 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1607894285872757, 3.0688610128990104, 2.9329948923188476, 2.795752634037709, 2.658640826003272, 2.5199044749914328, 2.3909736687178786, 2.267344133042382, 2.143685062362298, 2.034574797995247, 1.9395675016228011, 1.8564594550846272, 1.7841709853058723, 1.7197711063657872, 1.660969468876317, 1.6064341413266283, 1.5576907199454582, 1.5121974787410783, 1.4676512211932244, 1.426126901115474, 1.388371401586974, 1.3539477924883911, 1.3128503809407588, 1.2765281315921813, 1.2417536004975225, 1.2062416368872733, 1.1778991980146067, 1.1438208992125032, 1.1109859292947382, 1.0797754424206942, 1.051826360865829, 1.0236742809285349, 0.9952915196746959, 0.9746984771111775, 0.9415662694851054, 0.9156749932270866, 0.8986272722139643]
[0.068827815, 0.13154803, 0.15574348, 0.19379845, 0.22104768, 0.2635659, 0.2995067, 0.34907213, 0.38266385, 0.41860464, 0.4503171, 0.465821, 0.48249942, 0.50340617, 0.52055436, 0.5313601, 0.5501527, 0.5689453, 0.59126145, 0.60441625, 0.6114635, 0.6234437, 0.63330984, 0.64693445, 0.6638478, 0.67582804, 0.67653275, 0.6842847, 0.70519143, 0.71458775, 0.7256284, 0.7352596, 0.74371624, 0.74042755, 0.7629786, 0.7679117, 0.7676768]
[3.109175677814394, 2.9953744509970077, 2.8585235871059793, 2.726884225388648, 2.605298557639682, 2.495062737845479, 2.401140820364437, 2.306326028214934, 2.22746672876564, 2.1607200009162435, 2.0997693725594893, 2.047756692501301, 2.000563743528626, 1.9538708693544629, 1.9096295013673987, 1.8719957427799423, 1.8515223726980004, 1.8553939655353207, 1.8542497094248382, 1.850869682473196, 1.8153835311182227, 1.7678834591673014, 1.8202121262818995, 1.8173529052398574, 1.7840663774472447, 1.7384662580602046, 1.7513347727871837, 1.763400368091646, 1.6790901242287506, 1.6480205748562522, 1.666260832240324, 1.642801398934333, 1.6815133156630915, 1.779207393569006, 1.6651971360327493, 1.6510471845736525, 1.7350311566406573]
[0.10516431927680969, 0.14460094273090363, 0.17370891571044922, 0.2065727710723877, 0.24413146078586578, 0.2920187711715698, 0.31830987334251404, 0.33990609645843506, 0.3549295663833618, 0.38967135548591614, 0.4093896746635437, 0.39061033725738525, 0.39061033725738525, 0.4018779397010803, 0.41971829533576965, 0.4347417950630188, 0.44507041573524475, 0.4591549336910248, 0.4798122048377991, 0.4854460060596466, 0.5023474097251892, 0.514553964138031, 0.4976525902748108, 0.4967136085033417, 0.5051643252372742, 0.517370879650116, 0.5107980966567993, 0.514553964138031, 0.5389671325683594, 0.5427230000495911, 0.5408450961112976, 0.5521126985549927, 0.5370892286300659, 0.5352112650871277, 0.5502347350120544, 0.559624433517456, 0.5558685660362244]

  32/2898 [..............................] - ETA: 35s
 384/2898 [==>...........................] - ETA: 2s
 672/2898 [=====>........................] - ETA: 1s
1024/2898 [=========>....................] - ETA: 1s
1376/2898 [=============>................] - ETA: 0s
1568/2898 [===============>..............] - ETA: 0s
1760/2898 [=================>............] - ETA: 0s
2016/2898 [===================>..........] - ETA: 0s
2240/2898 [======================>.......] - ETA: 0s
2432/2898 [========================>.....] - ETA: 0s
2688/2898 [==========================>...] - ETA: 0s
2898/2898 [==============================] - 1s 336us/step
Accuracies per class for LSTM
[0.   0.49 0.9  0.95 0.28 0.77 0.62 0.96 0.   0.   0.21 0.   0.   0.24
 0.1  0.79 0.   0.    nan 0.87 0.65 0.   0.71]
[3.1607894285872757, 3.0688610128990104, 2.9329948923188476, 2.795752634037709, 2.658640826003272, 2.5199044749914328, 2.3909736687178786, 2.267344133042382, 2.143685062362298, 2.034574797995247, 1.9395675016228011, 1.8564594550846272, 1.7841709853058723, 1.7197711063657872, 1.660969468876317, 1.6064341413266283, 1.5576907199454582, 1.5121974787410783, 1.4676512211932244, 1.426126901115474, 1.388371401586974, 1.3539477924883911, 1.3128503809407588, 1.2765281315921813, 1.2417536004975225, 1.2062416368872733, 1.1778991980146067, 1.1438208992125032, 1.1109859292947382, 1.0797754424206942, 1.051826360865829, 1.0236742809285349, 0.9952915196746959, 0.9746984771111775, 0.9415662694851054, 0.9156749932270866, 0.8986272722139643]
[0.068827815, 0.13154803, 0.15574348, 0.19379845, 0.22104768, 0.2635659, 0.2995067, 0.34907213, 0.38266385, 0.41860464, 0.4503171, 0.465821, 0.48249942, 0.50340617, 0.52055436, 0.5313601, 0.5501527, 0.5689453, 0.59126145, 0.60441625, 0.6114635, 0.6234437, 0.63330984, 0.64693445, 0.6638478, 0.67582804, 0.67653275, 0.6842847, 0.70519143, 0.71458775, 0.7256284, 0.7352596, 0.74371624, 0.74042755, 0.7629786, 0.7679117, 0.7676768]
[3.109175677814394, 2.9953744509970077, 2.8585235871059793, 2.726884225388648, 2.605298557639682, 2.495062737845479, 2.401140820364437, 2.306326028214934, 2.22746672876564, 2.1607200009162435, 2.0997693725594893, 2.047756692501301, 2.000563743528626, 1.9538708693544629, 1.9096295013673987, 1.8719957427799423, 1.8515223726980004, 1.8553939655353207, 1.8542497094248382, 1.850869682473196, 1.8153835311182227, 1.7678834591673014, 1.8202121262818995, 1.8173529052398574, 1.7840663774472447, 1.7384662580602046, 1.7513347727871837, 1.763400368091646, 1.6790901242287506, 1.6480205748562522, 1.666260832240324, 1.642801398934333, 1.6815133156630915, 1.779207393569006, 1.6651971360327493, 1.6510471845736525, 1.7350311566406573]
[0.10516431927680969, 0.14460094273090363, 0.17370891571044922, 0.2065727710723877, 0.24413146078586578, 0.2920187711715698, 0.31830987334251404, 0.33990609645843506, 0.3549295663833618, 0.38967135548591614, 0.4093896746635437, 0.39061033725738525, 0.39061033725738525, 0.4018779397010803, 0.41971829533576965, 0.4347417950630188, 0.44507041573524475, 0.4591549336910248, 0.4798122048377991, 0.4854460060596466, 0.5023474097251892, 0.514553964138031, 0.4976525902748108, 0.4967136085033417, 0.5051643252372742, 0.517370879650116, 0.5107980966567993, 0.514553964138031, 0.5389671325683594, 0.5427230000495911, 0.5408450961112976, 0.5521126985549927, 0.5370892286300659, 0.5352112650871277, 0.5502347350120544, 0.559624433517456, 0.5558685660362244]
recall 0.5436
precision 0.5715
f1 0.4754
mcc 0.4685
RMSE: 4.775
classification report:
              precision    recall  f1-score   support

           1       0.00      0.00      0.00        72
           2       0.41      0.49      0.45        68
           3       0.70      0.90      0.79       252
           4       0.48      0.95      0.64       132
           5       0.53      0.28      0.37       191
           6       0.38      0.77      0.51       150
           7       0.38      0.62      0.47       240
           9       0.36      0.96      0.52       100
          10       0.00      0.00      0.00        61
          11       0.00      0.00      0.00        87
          12       0.94      0.21      0.34       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.12      0.24      0.16        72
          16       1.00      0.10      0.18        30
          17       0.44      0.79      0.57       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.63      0.87      0.73       222
          22       0.56      0.65      0.60       168
          23       0.00      0.00      0.00        65
          24       0.59      0.71      0.64        82

   micro avg       0.49      0.49      0.49      2898
   macro avg       0.33      0.37      0.30      2898
weighted avg       0.52      0.49      0.43      2898

>#1: 49.275
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 16s - loss: 3.1492 - acc: 0.0937 - val_loss: 3.1314 - val_acc: 0.1362
Epoch 2/50
 - 14s - loss: 3.0345 - acc: 0.1842 - val_loss: 3.0091 - val_acc: 0.1822
Epoch 3/50
 - 13s - loss: 2.8672 - acc: 0.1811 - val_loss: 2.8786 - val_acc: 0.1812
Epoch 4/50
 - 13s - loss: 2.7091 - acc: 0.1910 - val_loss: 2.7640 - val_acc: 0.1934
Epoch 5/50
 - 13s - loss: 2.5626 - acc: 0.2260 - val_loss: 2.6575 - val_acc: 0.1840
Epoch 6/50
 - 13s - loss: 2.4378 - acc: 0.2227 - val_loss: 2.5676 - val_acc: 0.1934
Epoch 7/50
 - 12s - loss: 2.3284 - acc: 0.2708 - val_loss: 2.4737 - val_acc: 0.2188
Epoch 8/50
 - 16s - loss: 2.2262 - acc: 0.3359 - val_loss: 2.3705 - val_acc: 0.2479
Epoch 9/50
 - 17s - loss: 2.1307 - acc: 0.3693 - val_loss: 2.2685 - val_acc: 0.2714
Epoch 10/50
 - 15s - loss: 2.0397 - acc: 0.3878 - val_loss: 2.1843 - val_acc: 0.3192
Epoch 11/50
 - 20s - loss: 1.9536 - acc: 0.4104 - val_loss: 2.1088 - val_acc: 0.3493
Epoch 12/50
 - 18s - loss: 1.8730 - acc: 0.4365 - val_loss: 2.0328 - val_acc: 0.3831
Epoch 13/50
 - 17s - loss: 1.7983 - acc: 0.4689 - val_loss: 1.9637 - val_acc: 0.4366
Epoch 14/50
 - 17s - loss: 1.7288 - acc: 0.5022 - val_loss: 1.9016 - val_acc: 0.5183
Epoch 15/50
 - 19s - loss: 1.6646 - acc: 0.5450 - val_loss: 1.8464 - val_acc: 0.5568
Epoch 16/50
 - 19s - loss: 1.6059 - acc: 0.5624 - val_loss: 1.7947 - val_acc: 0.5803
Epoch 17/50
 - 17s - loss: 1.5480 - acc: 0.5819 - val_loss: 1.7450 - val_acc: 0.6019
Epoch 18/50
 - 17s - loss: 1.4954 - acc: 0.5931 - val_loss: 1.7035 - val_acc: 0.6038
Epoch 19/50
 - 20s - loss: 1.4428 - acc: 0.6047 - val_loss: 1.6600 - val_acc: 0.6075
Epoch 20/50
 - 18s - loss: 1.3947 - acc: 0.6122 - val_loss: 1.6253 - val_acc: 0.6085
Epoch 21/50
 - 20s - loss: 1.3468 - acc: 0.6185 - val_loss: 1.5907 - val_acc: 0.6113
Epoch 22/50
 - 18s - loss: 1.3054 - acc: 0.6218 - val_loss: 1.5669 - val_acc: 0.6075
Epoch 23/50
 - 17s - loss: 1.2605 - acc: 0.6258 - val_loss: 1.5344 - val_acc: 0.6122
Epoch 24/50
 - 17s - loss: 1.2227 - acc: 0.6307 - val_loss: 1.5119 - val_acc: 0.6103
Epoch 25/50
 - 19s - loss: 1.1849 - acc: 0.6347 - val_loss: 1.4893 - val_acc: 0.6085
Epoch 26/50
 - 17s - loss: 1.1492 - acc: 0.6429 - val_loss: 1.4693 - val_acc: 0.6122
Epoch 27/50
 - 19s - loss: 1.1145 - acc: 0.6554 - val_loss: 1.4505 - val_acc: 0.6122
Epoch 28/50
 - 21s - loss: 1.0829 - acc: 0.6636 - val_loss: 1.4387 - val_acc: 0.6131
Epoch 29/50
 - 20s - loss: 1.0511 - acc: 0.6751 - val_loss: 1.4166 - val_acc: 0.6235
Epoch 30/50
 - 23s - loss: 1.0296 - acc: 0.6782 - val_loss: 1.4011 - val_acc: 0.6272
Epoch 31/50
 - 22s - loss: 0.9980 - acc: 0.6883 - val_loss: 1.3832 - val_acc: 0.6423
Epoch 32/50
 - 23s - loss: 0.9735 - acc: 0.6958 - val_loss: 1.3707 - val_acc: 0.6498
Epoch 33/50
 - 29s - loss: 0.9482 - acc: 0.7059 - val_loss: 1.3548 - val_acc: 0.6592
Epoch 34/50
 - 23s - loss: 0.9241 - acc: 0.7136 - val_loss: 1.3434 - val_acc: 0.6638
Epoch 35/50
 - 19s - loss: 0.9036 - acc: 0.7235 - val_loss: 1.3456 - val_acc: 0.6601
Epoch 36/50
 - 15s - loss: 0.8782 - acc: 0.7353 - val_loss: 1.3047 - val_acc: 0.6714
Epoch 37/50
 - 17s - loss: 0.8625 - acc: 0.7425 - val_loss: 1.3073 - val_acc: 0.6732
Epoch 38/50
 - 14s - loss: 0.8379 - acc: 0.7573 - val_loss: 1.2838 - val_acc: 0.6798
Epoch 39/50
 - 17s - loss: 0.8189 - acc: 0.7630 - val_loss: 1.2724 - val_acc: 0.6845
Epoch 40/50
 - 18s - loss: 0.7992 - acc: 0.7672 - val_loss: 1.2646 - val_acc: 0.6836
Epoch 41/50
 - 15s - loss: 0.7805 - acc: 0.7733 - val_loss: 1.2455 - val_acc: 0.6854
Epoch 42/50
 - 18s - loss: 0.7629 - acc: 0.7775 - val_loss: 1.2389 - val_acc: 0.6854
Epoch 43/50
 - 17s - loss: 0.7449 - acc: 0.7794 - val_loss: 1.2232 - val_acc: 0.6854
Epoch 44/50
 - 17s - loss: 0.7286 - acc: 0.7829 - val_loss: 1.2141 - val_acc: 0.6873
Epoch 45/50
 - 18s - loss: 0.7114 - acc: 0.7837 - val_loss: 1.2099 - val_acc: 0.6864
Epoch 46/50
 - 16s - loss: 0.6956 - acc: 0.7872 - val_loss: 1.1966 - val_acc: 0.6883
Epoch 47/50
 - 18s - loss: 0.6797 - acc: 0.7895 - val_loss: 1.1939 - val_acc: 0.6854
Epoch 48/50
 - 17s - loss: 0.6650 - acc: 0.7930 - val_loss: 1.1837 - val_acc: 0.6845
Epoch 49/50
 - 16s - loss: 0.6499 - acc: 0.7961 - val_loss: 1.1858 - val_acc: 0.6845
Epoch 50/50
 - 16s - loss: 0.6355 - acc: 0.7987 - val_loss: 1.1804 - val_acc: 0.6826
LSTM training time: 883.5541865825653s
Model: "sequential_12"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_2 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_23 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_24 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1492239253353507, 3.034512739212882, 2.8671857336190825, 2.709068134297556, 2.5626117605711616, 2.4377798476296464, 2.3283716522810014, 2.226177038120389, 2.1307100463647193, 2.0396833788040856, 1.9535947461516918, 1.873016848428613, 1.798332101306866, 1.7288373258434777, 1.6646090709801875, 1.6059491148094687, 1.5479650595027403, 1.4954101261130404, 1.44277081041233, 1.3946834885516803, 1.3468257042620702, 1.3053945381512013, 1.2605164396894462, 1.2227392396597563, 1.1849226855198374, 1.1492112749460748, 1.1144635290824558, 1.0829085581440419, 1.0510761948616987, 1.0295978360943931, 0.9980466676928892, 0.9734956654677347, 0.9482000541079344, 0.9241363516513635, 0.9035732111783551, 0.8782337403770677, 0.8624844682176039, 0.8378897364119394, 0.8188551008449296, 0.7992081498844903, 0.7805307411289226, 0.7628801177757459, 0.744906291477636, 0.7285615427759519, 0.7114074675758764, 0.6956187596840844, 0.6797411119837086, 0.6650302653630379, 0.6499268826021116, 0.6354502457263381]
[0.093727976, 0.18416725, 0.18111347, 0.19097957, 0.22598074, 0.22269204, 0.270848, 0.33591732, 0.36927414, 0.3878318, 0.4103829, 0.4364576, 0.46887478, 0.5022316, 0.54498476, 0.56236786, 0.5818652, 0.5931407, 0.60465115, 0.6121682, 0.61851066, 0.6217994, 0.6257928, 0.63072586, 0.6347193, 0.64294106, 0.6553911, 0.6636129, 0.67512333, 0.6781771, 0.68827814, 0.6957952, 0.7058962, 0.7136481, 0.7235142, 0.7352596, 0.7425417, 0.75734085, 0.7629786, 0.76720697, 0.77331454, 0.7775429, 0.7794221, 0.78294575, 0.78365046, 0.78717405, 0.7895231, 0.7930468, 0.79610056, 0.79868454]
[3.131436755623616, 3.0091054522375544, 2.87859602399835, 2.7639711149421657, 2.657480425006347, 2.5675709932622777, 2.473686282287741, 2.3705279612205397, 2.2685071777290022, 2.184284852144304, 2.108799036120025, 2.032840452283761, 1.9637322797461854, 1.9015699890297904, 1.8464138791035039, 1.7946891734857515, 1.7449867536204522, 1.703521991895398, 1.6600430333558382, 1.6252789105048202, 1.5906695320572652, 1.5669330024943104, 1.5344055129328804, 1.5119399885056723, 1.4892791827519736, 1.4693072293286034, 1.450489082190912, 1.4387120870917056, 1.416583745020656, 1.4011212525793084, 1.3831843928272176, 1.3707369174755795, 1.3547920447000315, 1.343387852308336, 1.3455566477048004, 1.3047491075287403, 1.307282357666414, 1.2837747795341161, 1.2723783743913184, 1.2645876841645844, 1.245537594478455, 1.238923197397044, 1.223177906255207, 1.2140849482425502, 1.2098758359610196, 1.1965999326655563, 1.1939028616802234, 1.1837436143921014, 1.1857612849541113, 1.1803597654535176]
[0.13615024089813232, 0.18215961754322052, 0.1812206506729126, 0.19342723488807678, 0.18403755128383636, 0.19342723488807678, 0.21877934038639069, 0.24788732826709747, 0.27136150002479553, 0.31924882531166077, 0.3492957651615143, 0.3830986022949219, 0.43661972880363464, 0.5183098316192627, 0.5568075180053711, 0.580281674861908, 0.6018779277801514, 0.6037558913230896, 0.6075117588043213, 0.608450710773468, 0.611267626285553, 0.6075117588043213, 0.6122065782546997, 0.6103286147117615, 0.608450710773468, 0.6122065782546997, 0.6122065782546997, 0.6131455302238464, 0.6234741806983948, 0.6272300481796265, 0.6422535181045532, 0.6497652530670166, 0.6591549515724182, 0.6638497710227966, 0.6600939035415649, 0.67136150598526, 0.6732394099235535, 0.6798121929168701, 0.6845070719718933, 0.6835680603981018, 0.68544602394104, 0.68544602394104, 0.68544602394104, 0.6873239278793335, 0.6863849759101868, 0.688262939453125, 0.68544602394104, 0.6845070719718933, 0.6845070719718933, 0.6826291084289551]

  32/2898 [..............................] - ETA: 40s
 352/2898 [==>...........................] - ETA: 3s
 672/2898 [=====>........................] - ETA: 1s
 992/2898 [=========>....................] - ETA: 1s
1312/2898 [============>.................] - ETA: 0s
1600/2898 [===============>..............] - ETA: 0s
1888/2898 [==================>...........] - ETA: 0s
2208/2898 [=====================>........] - ETA: 0s
2528/2898 [=========================>....] - ETA: 0s
2784/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 1s 331us/step
Accuracies per class for LSTM
[0.58 0.59 0.71 0.61 0.56 0.9  0.6  0.89 0.07 0.   0.06 0.   0.   0.82
 0.47 0.67 0.   0.   0.75 0.9  0.05 0.57]
[3.1492239253353507, 3.034512739212882, 2.8671857336190825, 2.709068134297556, 2.5626117605711616, 2.4377798476296464, 2.3283716522810014, 2.226177038120389, 2.1307100463647193, 2.0396833788040856, 1.9535947461516918, 1.873016848428613, 1.798332101306866, 1.7288373258434777, 1.6646090709801875, 1.6059491148094687, 1.5479650595027403, 1.4954101261130404, 1.44277081041233, 1.3946834885516803, 1.3468257042620702, 1.3053945381512013, 1.2605164396894462, 1.2227392396597563, 1.1849226855198374, 1.1492112749460748, 1.1144635290824558, 1.0829085581440419, 1.0510761948616987, 1.0295978360943931, 0.9980466676928892, 0.9734956654677347, 0.9482000541079344, 0.9241363516513635, 0.9035732111783551, 0.8782337403770677, 0.8624844682176039, 0.8378897364119394, 0.8188551008449296, 0.7992081498844903, 0.7805307411289226, 0.7628801177757459, 0.744906291477636, 0.7285615427759519, 0.7114074675758764, 0.6956187596840844, 0.6797411119837086, 0.6650302653630379, 0.6499268826021116, 0.6354502457263381]
[0.093727976, 0.18416725, 0.18111347, 0.19097957, 0.22598074, 0.22269204, 0.270848, 0.33591732, 0.36927414, 0.3878318, 0.4103829, 0.4364576, 0.46887478, 0.5022316, 0.54498476, 0.56236786, 0.5818652, 0.5931407, 0.60465115, 0.6121682, 0.61851066, 0.6217994, 0.6257928, 0.63072586, 0.6347193, 0.64294106, 0.6553911, 0.6636129, 0.67512333, 0.6781771, 0.68827814, 0.6957952, 0.7058962, 0.7136481, 0.7235142, 0.7352596, 0.7425417, 0.75734085, 0.7629786, 0.76720697, 0.77331454, 0.7775429, 0.7794221, 0.78294575, 0.78365046, 0.78717405, 0.7895231, 0.7930468, 0.79610056, 0.79868454]
[3.131436755623616, 3.0091054522375544, 2.87859602399835, 2.7639711149421657, 2.657480425006347, 2.5675709932622777, 2.473686282287741, 2.3705279612205397, 2.2685071777290022, 2.184284852144304, 2.108799036120025, 2.032840452283761, 1.9637322797461854, 1.9015699890297904, 1.8464138791035039, 1.7946891734857515, 1.7449867536204522, 1.703521991895398, 1.6600430333558382, 1.6252789105048202, 1.5906695320572652, 1.5669330024943104, 1.5344055129328804, 1.5119399885056723, 1.4892791827519736, 1.4693072293286034, 1.450489082190912, 1.4387120870917056, 1.416583745020656, 1.4011212525793084, 1.3831843928272176, 1.3707369174755795, 1.3547920447000315, 1.343387852308336, 1.3455566477048004, 1.3047491075287403, 1.307282357666414, 1.2837747795341161, 1.2723783743913184, 1.2645876841645844, 1.245537594478455, 1.238923197397044, 1.223177906255207, 1.2140849482425502, 1.2098758359610196, 1.1965999326655563, 1.1939028616802234, 1.1837436143921014, 1.1857612849541113, 1.1803597654535176]
[0.13615024089813232, 0.18215961754322052, 0.1812206506729126, 0.19342723488807678, 0.18403755128383636, 0.19342723488807678, 0.21877934038639069, 0.24788732826709747, 0.27136150002479553, 0.31924882531166077, 0.3492957651615143, 0.3830986022949219, 0.43661972880363464, 0.5183098316192627, 0.5568075180053711, 0.580281674861908, 0.6018779277801514, 0.6037558913230896, 0.6075117588043213, 0.608450710773468, 0.611267626285553, 0.6075117588043213, 0.6122065782546997, 0.6103286147117615, 0.608450710773468, 0.6122065782546997, 0.6122065782546997, 0.6131455302238464, 0.6234741806983948, 0.6272300481796265, 0.6422535181045532, 0.6497652530670166, 0.6591549515724182, 0.6638497710227966, 0.6600939035415649, 0.67136150598526, 0.6732394099235535, 0.6798121929168701, 0.6845070719718933, 0.6835680603981018, 0.68544602394104, 0.68544602394104, 0.68544602394104, 0.6873239278793335, 0.6863849759101868, 0.688262939453125, 0.68544602394104, 0.6845070719718933, 0.6845070719718933, 0.6826291084289551]
recall 0.5129
precision 0.591
f1 0.4491
mcc 0.4683
RMSE: 5.829
classification report:
              precision    recall  f1-score   support

           1       0.35      0.58      0.44        72
           2       0.17      0.59      0.26        68
           3       0.65      0.71      0.68       252
           4       0.74      0.61      0.67       132
           5       0.69      0.56      0.62       191
           6       0.32      0.90      0.47       150
           7       0.48      0.60      0.53       240
           9       0.38      0.89      0.53       100
          10       0.15      0.07      0.09        61
          11       0.00      0.00      0.00        87
          12       0.83      0.06      0.12       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.66      0.82      0.73        72
          16       0.70      0.47      0.56        30
          17       0.43      0.67      0.53       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.66      0.75      0.70       222
          22       0.55      0.90      0.68       168
          23       1.00      0.05      0.09        65
          24       0.60      0.57      0.59        82

   micro avg       0.49      0.49      0.49      2898
   macro avg       0.43      0.45      0.38      2898
weighted avg       0.56      0.49      0.43      2898

>#2: 48.551
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 20s - loss: 3.1433 - acc: 0.0817 - val_loss: 3.0824 - val_acc: 0.1315
Epoch 2/50
 - 21s - loss: 3.0297 - acc: 0.1438 - val_loss: 2.9392 - val_acc: 0.1155
Epoch 3/50
 - 18s - loss: 2.8616 - acc: 0.1635 - val_loss: 2.7984 - val_acc: 0.1465
Epoch 4/50
 - 15s - loss: 2.7096 - acc: 0.1839 - val_loss: 2.6712 - val_acc: 0.1925
Epoch 5/50
 - 15s - loss: 2.5754 - acc: 0.2572 - val_loss: 2.5659 - val_acc: 0.2498
Epoch 6/50
 - 17s - loss: 2.4572 - acc: 0.3037 - val_loss: 2.4599 - val_acc: 0.2911
Epoch 7/50
 - 16s - loss: 2.3502 - acc: 0.3329 - val_loss: 2.3676 - val_acc: 0.3286
Epoch 8/50
 - 15s - loss: 2.2598 - acc: 0.3611 - val_loss: 2.2873 - val_acc: 0.3606
Epoch 9/50
 - 22s - loss: 2.1775 - acc: 0.3944 - val_loss: 2.2097 - val_acc: 0.3756
Epoch 10/50
 - 13s - loss: 2.1026 - acc: 0.4167 - val_loss: 2.1425 - val_acc: 0.3812
Epoch 11/50
 - 12s - loss: 2.0319 - acc: 0.4339 - val_loss: 2.1014 - val_acc: 0.3765
Epoch 12/50
 - 12s - loss: 1.9650 - acc: 0.4433 - val_loss: 2.0526 - val_acc: 0.3709
Epoch 13/50
 - 12s - loss: 1.9037 - acc: 0.4484 - val_loss: 2.0018 - val_acc: 0.3793
Epoch 14/50
 - 12s - loss: 1.8427 - acc: 0.4562 - val_loss: 1.9499 - val_acc: 0.3897
Epoch 15/50
 - 12s - loss: 1.7816 - acc: 0.4682 - val_loss: 1.8995 - val_acc: 0.4188
Epoch 16/50
 - 12s - loss: 1.7239 - acc: 0.4863 - val_loss: 1.8530 - val_acc: 0.4451
Epoch 17/50
 - 12s - loss: 1.6689 - acc: 0.5128 - val_loss: 1.8133 - val_acc: 0.4620
Epoch 18/50
 - 14s - loss: 1.6169 - acc: 0.5328 - val_loss: 1.7776 - val_acc: 0.4779
Epoch 19/50
 - 13s - loss: 1.5659 - acc: 0.5492 - val_loss: 1.7445 - val_acc: 0.5023
Epoch 20/50
 - 13s - loss: 1.5170 - acc: 0.5605 - val_loss: 1.7050 - val_acc: 0.5202
Epoch 21/50
 - 13s - loss: 1.4690 - acc: 0.5696 - val_loss: 1.6717 - val_acc: 0.5371
Epoch 22/50
 - 12s - loss: 1.4244 - acc: 0.5852 - val_loss: 1.6390 - val_acc: 0.5549
Epoch 23/50
 - 13s - loss: 1.3799 - acc: 0.6030 - val_loss: 1.6169 - val_acc: 0.5587
Epoch 24/50
 - 12s - loss: 1.3381 - acc: 0.6145 - val_loss: 1.5928 - val_acc: 0.5643
Epoch 25/50
 - 12s - loss: 1.2968 - acc: 0.6225 - val_loss: 1.5708 - val_acc: 0.5681
Epoch 26/50
 - 12s - loss: 1.2566 - acc: 0.6284 - val_loss: 1.5517 - val_acc: 0.5671
Epoch 27/50
 - 12s - loss: 1.2181 - acc: 0.6350 - val_loss: 1.5517 - val_acc: 0.5577
Epoch 28/50
 - 12s - loss: 1.1818 - acc: 0.6411 - val_loss: 1.5499 - val_acc: 0.5521
Epoch 29/50
 - 12s - loss: 1.1476 - acc: 0.6469 - val_loss: 1.5822 - val_acc: 0.5408
Epoch 30/50
 - 12s - loss: 1.1156 - acc: 0.6528 - val_loss: 1.6500 - val_acc: 0.5324
Epoch 31/50
 - 12s - loss: 1.0851 - acc: 0.6627 - val_loss: 1.6649 - val_acc: 0.5268
Epoch 32/50
 - 12s - loss: 1.0550 - acc: 0.6721 - val_loss: 1.7201 - val_acc: 0.5221
Epoch 33/50
 - 12s - loss: 1.0262 - acc: 0.6789 - val_loss: 1.6468 - val_acc: 0.5315
Epoch 00033: early stopping
LSTM training time: 455.2774121761322s
Model: "sequential_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_3 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_25 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_26 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.143281255049523, 3.029661423896214, 2.861649431419507, 2.7095707299593386, 2.5753503170040313, 2.4572482135952822, 2.350243643748808, 2.259801485405748, 2.1774618561233687, 2.1026436995250224, 2.0318851123708557, 1.9650381760107734, 1.9037133406271025, 1.842722541602083, 1.7815826950651226, 1.723942870457519, 1.6688893465528696, 1.6168521090463375, 1.5658893395903974, 1.517034996303561, 1.4689607317105464, 1.4244399513049055, 1.3799445069057437, 1.3381083467798052, 1.296800649702367, 1.2565932335751646, 1.2181020160292078, 1.1817798569836975, 1.1475599782061403, 1.115629056669441, 1.0851008053995899, 1.0550448472983742, 1.0261864687128193]
[0.08174771, 0.14376321, 0.16349542, 0.18393235, 0.2572234, 0.30373502, 0.3328635, 0.3610524, 0.3944092, 0.4167254, 0.43387362, 0.4432699, 0.44843787, 0.4561898, 0.46817008, 0.48625794, 0.5128024, 0.53276956, 0.54921305, 0.5604886, 0.56965, 0.5851539, 0.60300684, 0.6145173, 0.6225041, 0.6283768, 0.6349542, 0.6410618, 0.64693445, 0.6528071, 0.66267323, 0.67206955, 0.6788818]
[3.082401813363805, 2.9392461156621223, 2.79837589666877, 2.6712275285676053, 2.565896386607712, 2.4598980657371556, 2.3675766416558637, 2.287250279260913, 2.2096586890063934, 2.1424908922311845, 2.101423891721197, 2.052620604116592, 2.001758869824835, 1.949867596200934, 1.8994997528237356, 1.852968205420624, 1.8133070403981097, 1.7775928784983819, 1.744490531912432, 1.7050262997967536, 1.671699956101431, 1.6390054936700025, 1.6169175547053556, 1.5927538439141753, 1.5708075905349892, 1.55166129399913, 1.5516651379390503, 1.549902489347637, 1.5821790479438405, 1.650047316069894, 1.6648709814313432, 1.7201343703577776, 1.6467713693357968]
[0.13145539164543152, 0.11549295485019684, 0.14647887647151947, 0.19248826801776886, 0.24976526200771332, 0.2910798192024231, 0.32863849401474, 0.36056336760520935, 0.3755868673324585, 0.38122066855430603, 0.3765258193016052, 0.3708920180797577, 0.3793427348136902, 0.38967135548591614, 0.4187793433666229, 0.44507041573524475, 0.46197181940078735, 0.4779342710971832, 0.5023474097251892, 0.5201877951622009, 0.5370892286300659, 0.5549295544624329, 0.5586854219436646, 0.5643192529678345, 0.5680751204490662, 0.5671361684799194, 0.5577464699745178, 0.5521126985549927, 0.5408450961112976, 0.5323943495750427, 0.5267605781555176, 0.5220656991004944, 0.531455397605896]

  32/2898 [..............................] - ETA: 37s
 416/2898 [===>..........................] - ETA: 2s
 800/2898 [=======>......................] - ETA: 1s
1184/2898 [===========>..................] - ETA: 0s
1536/2898 [==============>...............] - ETA: 0s
1920/2898 [==================>...........] - ETA: 0s
2272/2898 [======================>.......] - ETA: 0s
2656/2898 [==========================>...] - ETA: 0s
2898/2898 [==============================] - 1s 283us/step
Accuracies per class for LSTM
[0.   0.81 0.8  0.02 0.86 0.19 0.46 0.95 0.   0.   0.13 0.   0.   0.47
 0.   0.69 0.   0.   0.14 0.9  0.   0.04]
[3.143281255049523, 3.029661423896214, 2.861649431419507, 2.7095707299593386, 2.5753503170040313, 2.4572482135952822, 2.350243643748808, 2.259801485405748, 2.1774618561233687, 2.1026436995250224, 2.0318851123708557, 1.9650381760107734, 1.9037133406271025, 1.842722541602083, 1.7815826950651226, 1.723942870457519, 1.6688893465528696, 1.6168521090463375, 1.5658893395903974, 1.517034996303561, 1.4689607317105464, 1.4244399513049055, 1.3799445069057437, 1.3381083467798052, 1.296800649702367, 1.2565932335751646, 1.2181020160292078, 1.1817798569836975, 1.1475599782061403, 1.115629056669441, 1.0851008053995899, 1.0550448472983742, 1.0261864687128193]
[0.08174771, 0.14376321, 0.16349542, 0.18393235, 0.2572234, 0.30373502, 0.3328635, 0.3610524, 0.3944092, 0.4167254, 0.43387362, 0.4432699, 0.44843787, 0.4561898, 0.46817008, 0.48625794, 0.5128024, 0.53276956, 0.54921305, 0.5604886, 0.56965, 0.5851539, 0.60300684, 0.6145173, 0.6225041, 0.6283768, 0.6349542, 0.6410618, 0.64693445, 0.6528071, 0.66267323, 0.67206955, 0.6788818]
[3.082401813363805, 2.9392461156621223, 2.79837589666877, 2.6712275285676053, 2.565896386607712, 2.4598980657371556, 2.3675766416558637, 2.287250279260913, 2.2096586890063934, 2.1424908922311845, 2.101423891721197, 2.052620604116592, 2.001758869824835, 1.949867596200934, 1.8994997528237356, 1.852968205420624, 1.8133070403981097, 1.7775928784983819, 1.744490531912432, 1.7050262997967536, 1.671699956101431, 1.6390054936700025, 1.6169175547053556, 1.5927538439141753, 1.5708075905349892, 1.55166129399913, 1.5516651379390503, 1.549902489347637, 1.5821790479438405, 1.650047316069894, 1.6648709814313432, 1.7201343703577776, 1.6467713693357968]
[0.13145539164543152, 0.11549295485019684, 0.14647887647151947, 0.19248826801776886, 0.24976526200771332, 0.2910798192024231, 0.32863849401474, 0.36056336760520935, 0.3755868673324585, 0.38122066855430603, 0.3765258193016052, 0.3708920180797577, 0.3793427348136902, 0.38967135548591614, 0.4187793433666229, 0.44507041573524475, 0.46197181940078735, 0.4779342710971832, 0.5023474097251892, 0.5201877951622009, 0.5370892286300659, 0.5549295544624329, 0.5586854219436646, 0.5643192529678345, 0.5680751204490662, 0.5671361684799194, 0.5577464699745178, 0.5521126985549927, 0.5408450961112976, 0.5323943495750427, 0.5267605781555176, 0.5220656991004944, 0.531455397605896]
recall 0.4317
precision 0.3922
f1 0.3425
mcc 0.3315
RMSE: 6.884
classification report:
              precision    recall  f1-score   support

           1       0.00      0.00      0.00        72
           2       0.26      0.81      0.39        68
           3       0.43      0.80      0.56       252
           4       0.20      0.02      0.03       132
           5       0.27      0.86      0.41       191
           6       0.24      0.19      0.21       150
           7       0.59      0.46      0.51       240
           9       0.46      0.95      0.62       100
          10       0.00      0.00      0.00        61
          11       0.00      0.00      0.00        87
          12       0.55      0.13      0.21       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.42      0.47      0.45        72
          16       0.00      0.00      0.00        30
          17       0.39      0.69      0.50       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.14      0.14      0.14       222
          22       0.48      0.90      0.63       168
          23       0.00      0.00      0.00        65
          24       0.08      0.04      0.05        82

   micro avg       0.37      0.37      0.37      2898
   macro avg       0.21      0.29      0.21      2898
weighted avg       0.33      0.37      0.29      2898

>#3: 36.611
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1756 - acc: 0.0590 - val_loss: 3.1488 - val_acc: 0.0648
Epoch 2/50
 - 12s - loss: 3.1082 - acc: 0.0973 - val_loss: 3.0532 - val_acc: 0.1268
Epoch 3/50
 - 12s - loss: 2.9833 - acc: 0.1245 - val_loss: 2.9093 - val_acc: 0.1324
Epoch 4/50
 - 12s - loss: 2.8301 - acc: 0.1736 - val_loss: 2.7957 - val_acc: 0.2038
Epoch 5/50
 - 12s - loss: 2.6981 - acc: 0.2438 - val_loss: 2.6998 - val_acc: 0.2085
Epoch 6/50
 - 12s - loss: 2.5831 - acc: 0.2654 - val_loss: 2.6117 - val_acc: 0.2066
Epoch 7/50
 - 12s - loss: 2.4768 - acc: 0.2988 - val_loss: 2.5301 - val_acc: 0.2188
Epoch 8/50
 - 12s - loss: 2.3770 - acc: 0.3305 - val_loss: 2.4520 - val_acc: 0.2366
Epoch 9/50
 - 12s - loss: 2.2880 - acc: 0.3481 - val_loss: 2.4004 - val_acc: 0.2394
Epoch 10/50
 - 12s - loss: 2.2088 - acc: 0.3704 - val_loss: 2.3295 - val_acc: 0.2685
Epoch 11/50
 - 12s - loss: 2.1345 - acc: 0.3935 - val_loss: 2.2787 - val_acc: 0.3052
Epoch 12/50
 - 12s - loss: 2.0614 - acc: 0.4094 - val_loss: 2.2128 - val_acc: 0.3484
Epoch 13/50
 - 12s - loss: 1.9929 - acc: 0.4313 - val_loss: 2.1636 - val_acc: 0.3643
Epoch 14/50
 - 12s - loss: 1.9249 - acc: 0.4513 - val_loss: 2.1145 - val_acc: 0.3756
Epoch 15/50
 - 12s - loss: 1.8564 - acc: 0.4649 - val_loss: 2.0564 - val_acc: 0.3784
Epoch 16/50
 - 12s - loss: 1.7869 - acc: 0.4790 - val_loss: 1.9958 - val_acc: 0.3887
Epoch 17/50
 - 12s - loss: 1.7195 - acc: 0.4895 - val_loss: 1.9449 - val_acc: 0.3944
Epoch 18/50
 - 12s - loss: 1.6532 - acc: 0.5086 - val_loss: 1.8851 - val_acc: 0.4056
Epoch 19/50
 - 12s - loss: 1.5869 - acc: 0.5361 - val_loss: 1.8490 - val_acc: 0.4103
Epoch 20/50
 - 12s - loss: 1.5263 - acc: 0.5520 - val_loss: 1.7895 - val_acc: 0.4254
Epoch 21/50
 - 12s - loss: 1.4670 - acc: 0.5751 - val_loss: 1.7514 - val_acc: 0.4244
Epoch 22/50
 - 12s - loss: 1.4112 - acc: 0.5976 - val_loss: 1.7246 - val_acc: 0.4291
Epoch 23/50
 - 12s - loss: 1.3627 - acc: 0.6115 - val_loss: 1.6821 - val_acc: 0.4347
Epoch 24/50
 - 12s - loss: 1.3194 - acc: 0.6234 - val_loss: 1.6664 - val_acc: 0.4300
Epoch 25/50
 - 12s - loss: 1.2791 - acc: 0.6310 - val_loss: 1.6415 - val_acc: 0.4329
Epoch 26/50
 - 12s - loss: 1.2433 - acc: 0.6394 - val_loss: 1.6320 - val_acc: 0.4357
Epoch 27/50
 - 12s - loss: 1.2070 - acc: 0.6446 - val_loss: 1.5958 - val_acc: 0.4423
Epoch 28/50
 - 12s - loss: 1.1765 - acc: 0.6549 - val_loss: 1.5974 - val_acc: 0.4554
Epoch 29/50
 - 12s - loss: 1.1426 - acc: 0.6697 - val_loss: 1.5639 - val_acc: 0.4667
Epoch 30/50
 - 12s - loss: 1.1113 - acc: 0.6859 - val_loss: 1.5403 - val_acc: 0.4742
Epoch 31/50
 - 12s - loss: 1.0824 - acc: 0.7000 - val_loss: 1.5232 - val_acc: 0.4817
Epoch 32/50
 - 12s - loss: 1.0522 - acc: 0.7101 - val_loss: 1.5164 - val_acc: 0.4901
Epoch 33/50
 - 12s - loss: 1.0246 - acc: 0.7139 - val_loss: 1.5136 - val_acc: 0.4977
Epoch 34/50
 - 12s - loss: 0.9975 - acc: 0.7240 - val_loss: 1.4985 - val_acc: 0.5014
Epoch 35/50
 - 12s - loss: 0.9715 - acc: 0.7282 - val_loss: 1.4798 - val_acc: 0.5070
Epoch 36/50
 - 12s - loss: 0.9505 - acc: 0.7350 - val_loss: 1.4924 - val_acc: 0.5089
Epoch 37/50
 - 12s - loss: 0.9221 - acc: 0.7416 - val_loss: 1.4335 - val_acc: 0.5249
Epoch 38/50
 - 12s - loss: 0.9034 - acc: 0.7451 - val_loss: 1.4425 - val_acc: 0.5164
Epoch 39/50
 - 12s - loss: 0.8770 - acc: 0.7522 - val_loss: 1.4032 - val_acc: 0.5296
Epoch 40/50
 - 13s - loss: 0.8572 - acc: 0.7566 - val_loss: 1.4031 - val_acc: 0.5352
Epoch 41/50
 - 14s - loss: 0.8336 - acc: 0.7710 - val_loss: 1.3790 - val_acc: 0.5521
Epoch 42/50
 - 12s - loss: 0.8181 - acc: 0.7768 - val_loss: 1.4084 - val_acc: 0.5512
Epoch 43/50
 - 12s - loss: 0.7943 - acc: 0.7862 - val_loss: 1.3940 - val_acc: 0.5549
Epoch 44/50
 - 12s - loss: 0.7775 - acc: 0.7895 - val_loss: 1.3794 - val_acc: 0.5690
Epoch 45/50
 - 12s - loss: 0.7586 - acc: 0.7956 - val_loss: 1.3909 - val_acc: 0.5624
Epoch 46/50
 - 12s - loss: 0.7422 - acc: 0.8001 - val_loss: 1.3869 - val_acc: 0.5737
Epoch 00046: early stopping
LSTM training time: 559.1931366920471s
Model: "sequential_14"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_4 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_27 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_28 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.175566883526247, 3.1082068680876827, 2.983298124605035, 2.830116320642082, 2.6981046101227597, 2.5831460296925677, 2.476768882951743, 2.377041116465362, 2.288020378775459, 2.2087598946947486, 2.1344775043800737, 2.0613861983444646, 1.9929477631667705, 1.9248697401241206, 1.8564203966663622, 1.7868803602892496, 1.7195276005715587, 1.6531694431189559, 1.5868982713435889, 1.5262657761265757, 1.4669975226463665, 1.4111969499680983, 1.3627302917007105, 1.3194429457873729, 1.2791136552645115, 1.2432937552762586, 1.2070256505657704, 1.1764927364260986, 1.142602389691048, 1.111336292943569, 1.0823814928783675, 1.0522300167657244, 1.0246130435965781, 0.9975209839052233, 0.9714898656645038, 0.9504873419776891, 0.9220704418380187, 0.9033628483074272, 0.8770374059747131, 0.857225435624275, 0.8336431010910007, 0.8180646535892119, 0.7943390989341393, 0.777519713657575, 0.758597550348816, 0.7422407461929299]
[0.05896171, 0.09725159, 0.12450082, 0.17359643, 0.24383369, 0.26544514, 0.29880196, 0.33051446, 0.3481325, 0.37044868, 0.39346957, 0.40944326, 0.43128964, 0.45125675, 0.46488136, 0.4789758, 0.48954663, 0.5085741, 0.53605825, 0.55203193, 0.57505286, 0.5976039, 0.6114635, 0.6234437, 0.63096076, 0.6394174, 0.6445854, 0.6549213, 0.6697205, 0.68592906, 0.7000235, 0.7101245, 0.71388304, 0.723984, 0.72821236, 0.7350247, 0.74160206, 0.74512565, 0.7521729, 0.75663614, 0.77096546, 0.7768381, 0.78623444, 0.7895231, 0.79563075, 0.80009395]
[3.148787391913329, 3.0532256831585523, 2.9092656464643882, 2.795733592655737, 2.6998285709972114, 2.6117072944909756, 2.5301250865201994, 2.4519741353854325, 2.4004268352974187, 2.329521980419965, 2.278700846797424, 2.2128187034051745, 2.1635515518591437, 2.1145481536086175, 2.056356752422494, 1.9958434479897011, 1.944937124610507, 1.8850909473190844, 1.8489524706988267, 1.7895241989216335, 1.751387097410193, 1.7245844570124094, 1.6821383416932514, 1.6664337285807436, 1.641494720474655, 1.6319879067615723, 1.5957958269566996, 1.5974176286811559, 1.5638754225952525, 1.540303738520179, 1.5231921082930946, 1.516371514651697, 1.5136081870732734, 1.4984563073641817, 1.4798387192504507, 1.4923888835268961, 1.4334977633096802, 1.442517104660961, 1.4031928941957268, 1.4030572685837186, 1.378999004061793, 1.408391722435123, 1.3940365616284625, 1.3794408884546567, 1.3908765992788081, 1.3869207604950022]
[0.06478872895240784, 0.1267605572938919, 0.13239437341690063, 0.20375587046146393, 0.20845070481300354, 0.2065727710723877, 0.21877934038639069, 0.2366197109222412, 0.23943662643432617, 0.26854461431503296, 0.30516430735588074, 0.34835681319236755, 0.36431923508644104, 0.3755868673324585, 0.37840375304222107, 0.3887324035167694, 0.39436620473861694, 0.405633807182312, 0.41032862663269043, 0.4253521263599396, 0.42441314458847046, 0.42910799384117126, 0.4347417950630188, 0.430046945810318, 0.43286386132240295, 0.4356807470321655, 0.4422535300254822, 0.4553990662097931, 0.46666666865348816, 0.47417840361595154, 0.4816901385784149, 0.4901408553123474, 0.4976525902748108, 0.5014084577560425, 0.5070422291755676, 0.5089201927185059, 0.5248826146125793, 0.5164319276809692, 0.5295774936676025, 0.5352112650871277, 0.5521126985549927, 0.5511736869812012, 0.5549295544624329, 0.5690140724182129, 0.5624412894248962, 0.5737088918685913]

  32/2898 [..............................] - ETA: 42s
 384/2898 [==>...........................] - ETA: 3s
 768/2898 [======>.......................] - ETA: 1s
1120/2898 [==========>...................] - ETA: 0s
1504/2898 [==============>...............] - ETA: 0s
1856/2898 [==================>...........] - ETA: 0s
2240/2898 [======================>.......] - ETA: 0s
2592/2898 [=========================>....] - ETA: 0s
2898/2898 [==============================] - 1s 304us/step
Accuracies per class for LSTM
[0.15 0.88 0.88 0.8  0.85 0.28 0.59  nan 0.18 0.   0.   0.06 0.   0.
 0.1  0.1  0.75 0.   0.   0.85 0.38 0.   0.85]
[3.175566883526247, 3.1082068680876827, 2.983298124605035, 2.830116320642082, 2.6981046101227597, 2.5831460296925677, 2.476768882951743, 2.377041116465362, 2.288020378775459, 2.2087598946947486, 2.1344775043800737, 2.0613861983444646, 1.9929477631667705, 1.9248697401241206, 1.8564203966663622, 1.7868803602892496, 1.7195276005715587, 1.6531694431189559, 1.5868982713435889, 1.5262657761265757, 1.4669975226463665, 1.4111969499680983, 1.3627302917007105, 1.3194429457873729, 1.2791136552645115, 1.2432937552762586, 1.2070256505657704, 1.1764927364260986, 1.142602389691048, 1.111336292943569, 1.0823814928783675, 1.0522300167657244, 1.0246130435965781, 0.9975209839052233, 0.9714898656645038, 0.9504873419776891, 0.9220704418380187, 0.9033628483074272, 0.8770374059747131, 0.857225435624275, 0.8336431010910007, 0.8180646535892119, 0.7943390989341393, 0.777519713657575, 0.758597550348816, 0.7422407461929299]
[0.05896171, 0.09725159, 0.12450082, 0.17359643, 0.24383369, 0.26544514, 0.29880196, 0.33051446, 0.3481325, 0.37044868, 0.39346957, 0.40944326, 0.43128964, 0.45125675, 0.46488136, 0.4789758, 0.48954663, 0.5085741, 0.53605825, 0.55203193, 0.57505286, 0.5976039, 0.6114635, 0.6234437, 0.63096076, 0.6394174, 0.6445854, 0.6549213, 0.6697205, 0.68592906, 0.7000235, 0.7101245, 0.71388304, 0.723984, 0.72821236, 0.7350247, 0.74160206, 0.74512565, 0.7521729, 0.75663614, 0.77096546, 0.7768381, 0.78623444, 0.7895231, 0.79563075, 0.80009395]
[3.148787391913329, 3.0532256831585523, 2.9092656464643882, 2.795733592655737, 2.6998285709972114, 2.6117072944909756, 2.5301250865201994, 2.4519741353854325, 2.4004268352974187, 2.329521980419965, 2.278700846797424, 2.2128187034051745, 2.1635515518591437, 2.1145481536086175, 2.056356752422494, 1.9958434479897011, 1.944937124610507, 1.8850909473190844, 1.8489524706988267, 1.7895241989216335, 1.751387097410193, 1.7245844570124094, 1.6821383416932514, 1.6664337285807436, 1.641494720474655, 1.6319879067615723, 1.5957958269566996, 1.5974176286811559, 1.5638754225952525, 1.540303738520179, 1.5231921082930946, 1.516371514651697, 1.5136081870732734, 1.4984563073641817, 1.4798387192504507, 1.4923888835268961, 1.4334977633096802, 1.442517104660961, 1.4031928941957268, 1.4030572685837186, 1.378999004061793, 1.408391722435123, 1.3940365616284625, 1.3794408884546567, 1.3908765992788081, 1.3869207604950022]
[0.06478872895240784, 0.1267605572938919, 0.13239437341690063, 0.20375587046146393, 0.20845070481300354, 0.2065727710723877, 0.21877934038639069, 0.2366197109222412, 0.23943662643432617, 0.26854461431503296, 0.30516430735588074, 0.34835681319236755, 0.36431923508644104, 0.3755868673324585, 0.37840375304222107, 0.3887324035167694, 0.39436620473861694, 0.405633807182312, 0.41032862663269043, 0.4253521263599396, 0.42441314458847046, 0.42910799384117126, 0.4347417950630188, 0.430046945810318, 0.43286386132240295, 0.4356807470321655, 0.4422535300254822, 0.4553990662097931, 0.46666666865348816, 0.47417840361595154, 0.4816901385784149, 0.4901408553123474, 0.4976525902748108, 0.5014084577560425, 0.5070422291755676, 0.5089201927185059, 0.5248826146125793, 0.5164319276809692, 0.5295774936676025, 0.5352112650871277, 0.5521126985549927, 0.5511736869812012, 0.5549295544624329, 0.5690140724182129, 0.5624412894248962, 0.5737088918685913]
recall 0.4925
precision 0.5623
f1 0.4232
mcc 0.4081
RMSE: 6.488
classification report:
              precision    recall  f1-score   support

           1       0.55      0.15      0.24        72
           2       0.41      0.88      0.56        68
           3       0.82      0.88      0.85       252
           4       0.43      0.80      0.56       132
           5       0.59      0.85      0.70       191
           6       0.22      0.28      0.25       150
           7       0.19      0.59      0.29       240
           8       0.00      0.00      0.00         0
           9       0.19      0.18      0.18       100
          10       0.00      0.00      0.00        61
          11       0.00      0.00      0.00        87
          12       0.78      0.06      0.11       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.25      0.10      0.14        72
          16       0.50      0.10      0.17        30
          17       0.49      0.75      0.59       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.82      0.85      0.83       222
          22       0.63      0.38      0.47       168
          23       0.00      0.00      0.00        65
          24       0.25      0.85      0.38        82

   micro avg       0.43      0.43      0.43      2898
   macro avg       0.31      0.33      0.28      2898
weighted avg       0.49      0.43      0.37      2898

>#4: 42.995
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1514 - acc: 0.0792 - val_loss: 3.0762 - val_acc: 0.1277
Epoch 2/50
 - 12s - loss: 3.0168 - acc: 0.1567 - val_loss: 2.9124 - val_acc: 0.2038
Epoch 3/50
 - 12s - loss: 2.8358 - acc: 0.2420 - val_loss: 2.7563 - val_acc: 0.1972
Epoch 4/50
 - 12s - loss: 2.6598 - acc: 0.2514 - val_loss: 2.6302 - val_acc: 0.1944
Epoch 5/50
 - 12s - loss: 2.5159 - acc: 0.2556 - val_loss: 2.5308 - val_acc: 0.1972
Epoch 6/50
 - 12s - loss: 2.4050 - acc: 0.2570 - val_loss: 2.4593 - val_acc: 0.2085
Epoch 7/50
 - 12s - loss: 2.3160 - acc: 0.2657 - val_loss: 2.4021 - val_acc: 0.2244
Epoch 8/50
 - 12s - loss: 2.2420 - acc: 0.2781 - val_loss: 2.3501 - val_acc: 0.2376
Epoch 9/50
 - 12s - loss: 2.1775 - acc: 0.3044 - val_loss: 2.3004 - val_acc: 0.2601
Epoch 10/50
 - 12s - loss: 2.1183 - acc: 0.3261 - val_loss: 2.2519 - val_acc: 0.2723
Epoch 11/50
 - 12s - loss: 2.0623 - acc: 0.3521 - val_loss: 2.2032 - val_acc: 0.3089
Epoch 12/50
 - 12s - loss: 2.0069 - acc: 0.3789 - val_loss: 2.1589 - val_acc: 0.3333
Epoch 13/50
 - 12s - loss: 1.9510 - acc: 0.4109 - val_loss: 2.1158 - val_acc: 0.3521
Epoch 14/50
 - 12s - loss: 1.8948 - acc: 0.4409 - val_loss: 2.0739 - val_acc: 0.3840
Epoch 15/50
 - 12s - loss: 1.8376 - acc: 0.4672 - val_loss: 2.0285 - val_acc: 0.4028
Epoch 16/50
 - 12s - loss: 1.7808 - acc: 0.4820 - val_loss: 1.9868 - val_acc: 0.4169
Epoch 17/50
 - 12s - loss: 1.7238 - acc: 0.5001 - val_loss: 1.9459 - val_acc: 0.4310
Epoch 18/50
 - 12s - loss: 1.6672 - acc: 0.5206 - val_loss: 1.9105 - val_acc: 0.4582
Epoch 19/50
 - 12s - loss: 1.6111 - acc: 0.5408 - val_loss: 1.8760 - val_acc: 0.4723
Epoch 20/50
 - 12s - loss: 1.5561 - acc: 0.5544 - val_loss: 1.8474 - val_acc: 0.4798
Epoch 21/50
 - 12s - loss: 1.5026 - acc: 0.5774 - val_loss: 1.8236 - val_acc: 0.4854
Epoch 22/50
 - 12s - loss: 1.4506 - acc: 0.5934 - val_loss: 1.7982 - val_acc: 0.4939
Epoch 23/50
 - 12s - loss: 1.4015 - acc: 0.6117 - val_loss: 1.7715 - val_acc: 0.5061
Epoch 24/50
 - 12s - loss: 1.3547 - acc: 0.6253 - val_loss: 1.7435 - val_acc: 0.5211
Epoch 25/50
 - 12s - loss: 1.3101 - acc: 0.6373 - val_loss: 1.7149 - val_acc: 0.5333
Epoch 26/50
 - 12s - loss: 1.2680 - acc: 0.6474 - val_loss: 1.6869 - val_acc: 0.5484
Epoch 27/50
 - 12s - loss: 1.2280 - acc: 0.6521 - val_loss: 1.6563 - val_acc: 0.5615
Epoch 28/50
 - 12s - loss: 1.1901 - acc: 0.6582 - val_loss: 1.6261 - val_acc: 0.5653
Epoch 29/50
 - 12s - loss: 1.1545 - acc: 0.6638 - val_loss: 1.5957 - val_acc: 0.5737
Epoch 30/50
 - 12s - loss: 1.1208 - acc: 0.6683 - val_loss: 1.5645 - val_acc: 0.5765
Epoch 31/50
 - 12s - loss: 1.0894 - acc: 0.6742 - val_loss: 1.5349 - val_acc: 0.5793
Epoch 32/50
 - 12s - loss: 1.0598 - acc: 0.6808 - val_loss: 1.5036 - val_acc: 0.5850
Epoch 33/50
 - 12s - loss: 1.0320 - acc: 0.6866 - val_loss: 1.4753 - val_acc: 0.5869
Epoch 34/50
 - 12s - loss: 1.0058 - acc: 0.6913 - val_loss: 1.4460 - val_acc: 0.5878
Epoch 35/50
 - 12s - loss: 0.9810 - acc: 0.6986 - val_loss: 1.4187 - val_acc: 0.5934
Epoch 36/50
 - 12s - loss: 0.9577 - acc: 0.7028 - val_loss: 1.3938 - val_acc: 0.5934
Epoch 37/50
 - 12s - loss: 0.9356 - acc: 0.7064 - val_loss: 1.3701 - val_acc: 0.6009
Epoch 38/50
 - 12s - loss: 0.9147 - acc: 0.7097 - val_loss: 1.3490 - val_acc: 0.6009
Epoch 39/50
 - 12s - loss: 0.8945 - acc: 0.7155 - val_loss: 1.3281 - val_acc: 0.6075
Epoch 40/50
 - 12s - loss: 0.8755 - acc: 0.7216 - val_loss: 1.3088 - val_acc: 0.6103
Epoch 41/50
 - 12s - loss: 0.8571 - acc: 0.7249 - val_loss: 1.2903 - val_acc: 0.6141
Epoch 42/50
 - 12s - loss: 0.8396 - acc: 0.7308 - val_loss: 1.2736 - val_acc: 0.6160
Epoch 43/50
 - 12s - loss: 0.8226 - acc: 0.7390 - val_loss: 1.2565 - val_acc: 0.6169
Epoch 44/50
 - 12s - loss: 0.8064 - acc: 0.7454 - val_loss: 1.2408 - val_acc: 0.6216
Epoch 45/50
 - 12s - loss: 0.7908 - acc: 0.7545 - val_loss: 1.2247 - val_acc: 0.6244
Epoch 46/50
 - 12s - loss: 0.7759 - acc: 0.7618 - val_loss: 1.2109 - val_acc: 0.6272
Epoch 47/50
 - 12s - loss: 0.7613 - acc: 0.7667 - val_loss: 1.1952 - val_acc: 0.6310
Epoch 48/50
 - 12s - loss: 0.7474 - acc: 0.7726 - val_loss: 1.1834 - val_acc: 0.6366
Epoch 49/50
 - 12s - loss: 0.7340 - acc: 0.7790 - val_loss: 1.1723 - val_acc: 0.6366
Epoch 50/50
 - 12s - loss: 0.7209 - acc: 0.7853 - val_loss: 1.1600 - val_acc: 0.6357
LSTM training time: 604.894243478775s
Model: "sequential_15"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_5 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_29 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_30 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1513965826569965, 3.016790818182381, 2.835768728993268, 2.659754600439773, 2.5158952479354646, 2.40503344906589, 2.31598366004132, 2.242036565749724, 2.177497120678691, 2.1183389301783944, 2.0622746108534304, 2.006912948801284, 1.950975310418817, 1.8947726092709323, 1.8376239628642832, 1.7807769369345359, 1.7238208741012637, 1.6672160260017024, 1.611093638535254, 1.5561143504864794, 1.5026022291866747, 1.4506491587311554, 1.4015080794107475, 1.35467026618436, 1.3100795528265352, 1.2679560454288838, 1.2279658990900981, 1.190068041239374, 1.1544785667031423, 1.1207563936892377, 1.0893630290748313, 1.0597930923996886, 1.031968588654861, 1.0057557465354126, 0.9809822719548548, 0.95769797421297, 0.9355813704775845, 0.9146769976417084, 0.8945059341767709, 0.8754941796105541, 0.8571341381348548, 0.8396414443447973, 0.82259336412163, 0.8063860221391612, 0.7907745603814158, 0.7758993744108076, 0.7613228274772278, 0.7474015925638772, 0.7339863084223396, 0.7208848301516415]
[0.07916373, 0.15668312, 0.24195443, 0.25135073, 0.25557905, 0.2569885, 0.26568004, 0.27813014, 0.30443975, 0.3260512, 0.3521259, 0.37890533, 0.4108527, 0.44092083, 0.46723044, 0.4820296, 0.5001175, 0.52055436, 0.5407564, 0.554381, 0.57740194, 0.5933756, 0.6116984, 0.625323, 0.6373033, 0.64740425, 0.6521024, 0.65821, 0.6638478, 0.668311, 0.6741837, 0.6807611, 0.68663377, 0.6913319, 0.69861406, 0.70284235, 0.706366, 0.7096547, 0.71552736, 0.7216349, 0.7249237, 0.73079634, 0.7390181, 0.74536055, 0.75452197, 0.7618041, 0.76673716, 0.77260983, 0.7789523, 0.78529483]
[3.0762284572135674, 2.912404309975709, 2.756319046468242, 2.6301871792251514, 2.5308206923131094, 2.4593304759459875, 2.402120986007189, 2.3501084571713013, 2.300403399982363, 2.2519370938690617, 2.2031989594580423, 2.1588796615600585, 2.115783808600735, 2.073889081131125, 2.028486085609651, 1.9867505536952488, 1.9459156803122148, 1.9105283922992402, 1.8760350221759277, 1.8474191436185523, 1.8236358804881851, 1.7982218876690932, 1.771514925486605, 1.7435061464846973, 1.7149412533486952, 1.686921115548398, 1.6562753361155729, 1.6260994575393033, 1.5957275757767224, 1.564548636042456, 1.5349107290657473, 1.5035944620488395, 1.4752564845790326, 1.4460455095264273, 1.4186696486853658, 1.3937552275232306, 1.3700559685767537, 1.3489717685000997, 1.3281495131377323, 1.3088438899304387, 1.29032650475211, 1.2736384101316962, 1.2564907119587554, 1.2408085140963676, 1.2247077312687753, 1.2109323447299116, 1.1952418513421161, 1.1833947559197744, 1.1722573950676851, 1.1600442277993395]
[0.12769952416419983, 0.20375587046146393, 0.19718310236930847, 0.1943662017583847, 0.19718310236930847, 0.20845070481300354, 0.22441314160823822, 0.23755869269371033, 0.26009389758110046, 0.27230048179626465, 0.3089201748371124, 0.3333333432674408, 0.35211268067359924, 0.3840375542640686, 0.40281689167022705, 0.4169014096260071, 0.4309859275817871, 0.45821595191955566, 0.4723004698753357, 0.4798122048377991, 0.4854460060596466, 0.4938967227935791, 0.5061032772064209, 0.5211267471313477, 0.5333333611488342, 0.548356831073761, 0.5615023374557495, 0.5652582049369812, 0.5737088918685913, 0.5765258073806763, 0.5793427228927612, 0.5849765539169312, 0.5868544578552246, 0.5877934098243713, 0.5934272408485413, 0.5934272408485413, 0.6009389758110046, 0.6009389758110046, 0.6075117588043213, 0.6103286147117615, 0.6140844821929932, 0.6159624457359314, 0.6169013977050781, 0.6215962171554565, 0.6244131326675415, 0.6272300481796265, 0.6309859156608582, 0.6366197466850281, 0.6366197466850281, 0.6356807351112366]

  32/2898 [..............................] - ETA: 46s
 416/2898 [===>..........................] - ETA: 3s
 800/2898 [=======>......................] - ETA: 1s
1184/2898 [===========>..................] - ETA: 0s
1536/2898 [==============>...............] - ETA: 0s
1888/2898 [==================>...........] - ETA: 0s
2272/2898 [======================>.......] - ETA: 0s
2624/2898 [==========================>...] - ETA: 0s
2898/2898 [==============================] - 1s 319us/step
Accuracies per class for LSTM
[0.24 0.57 0.94 0.07 0.6  0.87 0.49 0.94 0.11 0.   0.14 0.02 0.   0.36
 0.13 0.79 0.   0.   0.45 0.67 0.06 0.68]
[3.1513965826569965, 3.016790818182381, 2.835768728993268, 2.659754600439773, 2.5158952479354646, 2.40503344906589, 2.31598366004132, 2.242036565749724, 2.177497120678691, 2.1183389301783944, 2.0622746108534304, 2.006912948801284, 1.950975310418817, 1.8947726092709323, 1.8376239628642832, 1.7807769369345359, 1.7238208741012637, 1.6672160260017024, 1.611093638535254, 1.5561143504864794, 1.5026022291866747, 1.4506491587311554, 1.4015080794107475, 1.35467026618436, 1.3100795528265352, 1.2679560454288838, 1.2279658990900981, 1.190068041239374, 1.1544785667031423, 1.1207563936892377, 1.0893630290748313, 1.0597930923996886, 1.031968588654861, 1.0057557465354126, 0.9809822719548548, 0.95769797421297, 0.9355813704775845, 0.9146769976417084, 0.8945059341767709, 0.8754941796105541, 0.8571341381348548, 0.8396414443447973, 0.82259336412163, 0.8063860221391612, 0.7907745603814158, 0.7758993744108076, 0.7613228274772278, 0.7474015925638772, 0.7339863084223396, 0.7208848301516415]
[0.07916373, 0.15668312, 0.24195443, 0.25135073, 0.25557905, 0.2569885, 0.26568004, 0.27813014, 0.30443975, 0.3260512, 0.3521259, 0.37890533, 0.4108527, 0.44092083, 0.46723044, 0.4820296, 0.5001175, 0.52055436, 0.5407564, 0.554381, 0.57740194, 0.5933756, 0.6116984, 0.625323, 0.6373033, 0.64740425, 0.6521024, 0.65821, 0.6638478, 0.668311, 0.6741837, 0.6807611, 0.68663377, 0.6913319, 0.69861406, 0.70284235, 0.706366, 0.7096547, 0.71552736, 0.7216349, 0.7249237, 0.73079634, 0.7390181, 0.74536055, 0.75452197, 0.7618041, 0.76673716, 0.77260983, 0.7789523, 0.78529483]
[3.0762284572135674, 2.912404309975709, 2.756319046468242, 2.6301871792251514, 2.5308206923131094, 2.4593304759459875, 2.402120986007189, 2.3501084571713013, 2.300403399982363, 2.2519370938690617, 2.2031989594580423, 2.1588796615600585, 2.115783808600735, 2.073889081131125, 2.028486085609651, 1.9867505536952488, 1.9459156803122148, 1.9105283922992402, 1.8760350221759277, 1.8474191436185523, 1.8236358804881851, 1.7982218876690932, 1.771514925486605, 1.7435061464846973, 1.7149412533486952, 1.686921115548398, 1.6562753361155729, 1.6260994575393033, 1.5957275757767224, 1.564548636042456, 1.5349107290657473, 1.5035944620488395, 1.4752564845790326, 1.4460455095264273, 1.4186696486853658, 1.3937552275232306, 1.3700559685767537, 1.3489717685000997, 1.3281495131377323, 1.3088438899304387, 1.29032650475211, 1.2736384101316962, 1.2564907119587554, 1.2408085140963676, 1.2247077312687753, 1.2109323447299116, 1.1952418513421161, 1.1833947559197744, 1.1722573950676851, 1.1600442277993395]
[0.12769952416419983, 0.20375587046146393, 0.19718310236930847, 0.1943662017583847, 0.19718310236930847, 0.20845070481300354, 0.22441314160823822, 0.23755869269371033, 0.26009389758110046, 0.27230048179626465, 0.3089201748371124, 0.3333333432674408, 0.35211268067359924, 0.3840375542640686, 0.40281689167022705, 0.4169014096260071, 0.4309859275817871, 0.45821595191955566, 0.4723004698753357, 0.4798122048377991, 0.4854460060596466, 0.4938967227935791, 0.5061032772064209, 0.5211267471313477, 0.5333333611488342, 0.548356831073761, 0.5615023374557495, 0.5652582049369812, 0.5737088918685913, 0.5765258073806763, 0.5793427228927612, 0.5849765539169312, 0.5868544578552246, 0.5877934098243713, 0.5934272408485413, 0.5934272408485413, 0.6009389758110046, 0.6009389758110046, 0.6075117588043213, 0.6103286147117615, 0.6140844821929932, 0.6159624457359314, 0.6169013977050781, 0.6215962171554565, 0.6244131326675415, 0.6272300481796265, 0.6309859156608582, 0.6366197466850281, 0.6366197466850281, 0.6356807351112366]
recall 0.4656
precision 0.5429
f1 0.4126
mcc 0.4149
RMSE: 6.574
classification report:
              precision    recall  f1-score   support

           1       0.12      0.24      0.16        72
           2       0.55      0.57      0.56        68
           3       0.56      0.94      0.70       252
           4       0.16      0.07      0.09       132
           5       0.48      0.60      0.53       191
           6       0.35      0.87      0.50       150
           7       0.26      0.49      0.34       240
           9       0.50      0.94      0.65       100
          10       0.21      0.11      0.15        61
          11       0.00      0.00      0.00        87
          12       0.80      0.14      0.25       595
          13       0.50      0.02      0.03        59
          14       0.00      0.00      0.00         6
          15       0.18      0.36      0.24        72
          16       0.80      0.13      0.23        30
          17       0.49      0.79      0.60       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.79      0.45      0.58       222
          22       0.62      0.67      0.64       168
          23       1.00      0.06      0.12        65
          24       0.64      0.68      0.66        82

   micro avg       0.44      0.44      0.44      2898
   macro avg       0.41      0.37      0.32      2898
weighted avg       0.52      0.44      0.39      2898

>#5: 44.203
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1572 - acc: 0.0810 - val_loss: 3.1155 - val_acc: 0.1728
Epoch 2/50
 - 12s - loss: 3.0715 - acc: 0.1506 - val_loss: 2.9870 - val_acc: 0.2141
Epoch 3/50
 - 12s - loss: 2.9217 - acc: 0.2185 - val_loss: 2.8180 - val_acc: 0.2113
Epoch 4/50
 - 12s - loss: 2.7588 - acc: 0.2467 - val_loss: 2.6835 - val_acc: 0.2066
Epoch 5/50
 - 12s - loss: 2.6242 - acc: 0.2584 - val_loss: 2.5843 - val_acc: 0.2103
Epoch 6/50
 - 12s - loss: 2.5143 - acc: 0.2589 - val_loss: 2.5010 - val_acc: 0.2103
Epoch 7/50
 - 12s - loss: 2.4189 - acc: 0.2572 - val_loss: 2.4237 - val_acc: 0.2038
Epoch 8/50
 - 12s - loss: 2.3356 - acc: 0.2584 - val_loss: 2.3592 - val_acc: 0.2038
Epoch 9/50
 - 12s - loss: 2.2628 - acc: 0.2629 - val_loss: 2.3058 - val_acc: 0.2122
Epoch 10/50
 - 12s - loss: 2.1955 - acc: 0.2802 - val_loss: 2.2604 - val_acc: 0.2413
Epoch 11/50
 - 12s - loss: 2.1330 - acc: 0.3091 - val_loss: 2.2104 - val_acc: 0.2808
Epoch 12/50
 - 12s - loss: 2.0681 - acc: 0.3517 - val_loss: 2.1599 - val_acc: 0.2986
Epoch 13/50
 - 12s - loss: 2.0041 - acc: 0.3789 - val_loss: 2.1078 - val_acc: 0.3089
Epoch 14/50
 - 12s - loss: 1.9432 - acc: 0.3968 - val_loss: 2.0651 - val_acc: 0.3183
Epoch 15/50
 - 12s - loss: 1.8856 - acc: 0.4092 - val_loss: 2.0285 - val_acc: 0.3211
Epoch 16/50
 - 12s - loss: 1.8295 - acc: 0.4205 - val_loss: 1.9793 - val_acc: 0.3268
Epoch 17/50
 - 12s - loss: 1.7762 - acc: 0.4353 - val_loss: 1.9434 - val_acc: 0.3315
Epoch 18/50
 - 12s - loss: 1.7246 - acc: 0.4614 - val_loss: 1.9077 - val_acc: 0.3362
Epoch 19/50
 - 12s - loss: 1.6760 - acc: 0.4823 - val_loss: 1.8789 - val_acc: 0.3446
Epoch 20/50
 - 12s - loss: 1.6292 - acc: 0.5006 - val_loss: 1.8445 - val_acc: 0.3577
Epoch 21/50
 - 12s - loss: 1.5844 - acc: 0.5184 - val_loss: 1.8131 - val_acc: 0.3690
Epoch 22/50
 - 12s - loss: 1.5411 - acc: 0.5396 - val_loss: 1.7824 - val_acc: 0.3925
Epoch 23/50
 - 12s - loss: 1.4993 - acc: 0.5572 - val_loss: 1.7569 - val_acc: 0.4056
Epoch 24/50
 - 12s - loss: 1.4589 - acc: 0.5699 - val_loss: 1.7337 - val_acc: 0.4150
Epoch 25/50
 - 12s - loss: 1.4194 - acc: 0.5830 - val_loss: 1.7052 - val_acc: 0.4216
Epoch 26/50
 - 12s - loss: 1.3808 - acc: 0.5983 - val_loss: 1.6575 - val_acc: 0.4394
Epoch 27/50
 - 12s - loss: 1.3443 - acc: 0.6150 - val_loss: 1.6462 - val_acc: 0.4347
Epoch 28/50
 - 12s - loss: 1.3060 - acc: 0.6368 - val_loss: 1.6289 - val_acc: 0.4601
Epoch 29/50
 - 12s - loss: 1.2694 - acc: 0.6568 - val_loss: 1.6053 - val_acc: 0.4808
Epoch 30/50
 - 12s - loss: 1.2334 - acc: 0.6732 - val_loss: 1.5696 - val_acc: 0.4892
Epoch 31/50
 - 12s - loss: 1.1979 - acc: 0.6873 - val_loss: 1.5571 - val_acc: 0.4892
Epoch 32/50
 - 12s - loss: 1.1613 - acc: 0.6972 - val_loss: 1.5360 - val_acc: 0.4873
Epoch 33/50
 - 12s - loss: 1.1245 - acc: 0.7019 - val_loss: 1.5188 - val_acc: 0.4854
Epoch 34/50
 - 12s - loss: 1.0870 - acc: 0.7094 - val_loss: 1.5010 - val_acc: 0.4873
Epoch 35/50
 - 12s - loss: 1.0513 - acc: 0.7186 - val_loss: 1.5050 - val_acc: 0.4808
Epoch 36/50
 - 12s - loss: 1.0167 - acc: 0.7310 - val_loss: 1.4957 - val_acc: 0.4948
Epoch 37/50
 - 12s - loss: 0.9842 - acc: 0.7416 - val_loss: 1.4809 - val_acc: 0.4939
Epoch 38/50
 - 12s - loss: 0.9538 - acc: 0.7543 - val_loss: 1.4621 - val_acc: 0.4939
Epoch 39/50
 - 12s - loss: 0.9246 - acc: 0.7656 - val_loss: 1.4160 - val_acc: 0.5136
Epoch 40/50
 - 12s - loss: 0.8977 - acc: 0.7719 - val_loss: 1.4342 - val_acc: 0.5136
Epoch 41/50
 - 12s - loss: 0.8695 - acc: 0.7820 - val_loss: 1.4058 - val_acc: 0.5164
Epoch 42/50
 - 12s - loss: 0.8439 - acc: 0.7869 - val_loss: 1.3980 - val_acc: 0.5211
Epoch 43/50
 - 12s - loss: 0.8189 - acc: 0.7954 - val_loss: 1.3820 - val_acc: 0.5362
Epoch 44/50
 - 12s - loss: 0.7959 - acc: 0.7996 - val_loss: 1.3723 - val_acc: 0.5446
Epoch 45/50
 - 12s - loss: 0.7732 - acc: 0.8076 - val_loss: 1.3574 - val_acc: 0.5512
Epoch 46/50
 - 12s - loss: 0.7516 - acc: 0.8158 - val_loss: 1.3466 - val_acc: 0.5512
Epoch 47/50
 - 12s - loss: 0.7311 - acc: 0.8205 - val_loss: 1.3345 - val_acc: 0.5549
Epoch 48/50
 - 12s - loss: 0.7117 - acc: 0.8273 - val_loss: 1.2933 - val_acc: 0.5577
Epoch 49/50
 - 12s - loss: 0.6952 - acc: 0.8349 - val_loss: 1.3208 - val_acc: 0.5624
Epoch 50/50
 - 12s - loss: 0.6747 - acc: 0.8396 - val_loss: 1.3010 - val_acc: 0.5634
LSTM training time: 611.263839006424s
Model: "sequential_16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_6 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_31 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_32 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.157152710594928, 3.071509653059619, 2.9216904734288245, 2.7588321259807524, 2.62415885477243, 2.5142707788302974, 2.4188860244025134, 2.335647926102061, 2.2628326862945447, 2.1955151671615036, 2.1330186537461913, 2.0680755174353225, 2.00406441656054, 1.9431671430898267, 1.8855615236234406, 1.8295400691250834, 1.776220820418876, 1.7246368682549709, 1.6759898837166602, 1.6291528573639942, 1.5843696524230477, 1.541061454377097, 1.499261346345276, 1.4589298084948805, 1.4193808263614476, 1.3808221348804406, 1.3442952478186805, 1.3060429242048182, 1.2693809402753469, 1.2333989543746215, 1.197945272646685, 1.1613280546407563, 1.1245410510994946, 1.0870322048181038, 1.0513005040713759, 1.016656542607728, 0.9842086694723344, 0.9537751062993911, 0.9246401374703258, 0.8976697794863784, 0.8694750316439309, 0.8439229724586136, 0.8189122092389209, 0.7959005753038242, 0.7731932197689366, 0.7516055536194534, 0.7311292437388915, 0.7116874435193274, 0.695234537915722, 0.6746803207634927]
[0.08104299, 0.15057552, 0.2184637, 0.24665257, 0.25839794, 0.25886774, 0.2572234, 0.25839794, 0.26286116, 0.2802443, 0.30913788, 0.3516561, 0.37890533, 0.3967583, 0.40920836, 0.42048392, 0.43528306, 0.46135777, 0.48226452, 0.5005873, 0.5184402, 0.5395819, 0.5571999, 0.5698849, 0.5830397, 0.5983087, 0.6149871, 0.6368334, 0.65680057, 0.67324406, 0.68733853, 0.6972046, 0.70190275, 0.7094198, 0.71858114, 0.73103124, 0.74160206, 0.75428706, 0.7655626, 0.7719051, 0.7820061, 0.78693914, 0.7953958, 0.79962415, 0.807611, 0.81583273, 0.8205309, 0.8273432, 0.8348602, 0.83955836]
[3.1155288425409737, 2.9869866042069986, 2.8179644459290123, 2.683475861974725, 2.5842620415306987, 2.501004225995059, 2.4236993371041167, 2.359196176439384, 2.3058377017437572, 2.260370901940574, 2.2104295992515457, 2.1599399443523426, 2.107813499679028, 2.0651446485743277, 2.0285014172674902, 1.9793036728397781, 1.9434267150963975, 1.907699032232795, 1.878942129085881, 1.844519545102903, 1.8130586923567902, 1.7824414440723653, 1.756881026482918, 1.7336859227346142, 1.705182528104021, 1.657522959104726, 1.6461993149188763, 1.6289017801833265, 1.6053485675037187, 1.5696236534857415, 1.5570973875936769, 1.5360030330962418, 1.5188424886112482, 1.5010200710643624, 1.5050031208096535, 1.4956890824814917, 1.480911036864133, 1.4621069946899101, 1.4160278007877825, 1.4342359594336138, 1.4057500227096495, 1.398030172799115, 1.3820345454792462, 1.3722691542106056, 1.357383233602618, 1.3466477623987645, 1.334466593590141, 1.2933460863123478, 1.3207741793052692, 1.3010329252677344]
[0.1727699488401413, 0.21408450603485107, 0.2112676054239273, 0.2065727710723877, 0.21032863855361938, 0.21032863855361938, 0.20375587046146393, 0.20375587046146393, 0.21220657229423523, 0.24131456017494202, 0.28075116872787476, 0.2985915541648865, 0.3089201748371124, 0.31830987334251404, 0.3211267590522766, 0.32676056027412415, 0.33145540952682495, 0.33615022897720337, 0.34460094571113586, 0.3577464818954468, 0.36901408433914185, 0.3924882709980011, 0.405633807182312, 0.41502347588539124, 0.4215962588787079, 0.4394366145133972, 0.4347417950630188, 0.4600938856601715, 0.4807511866092682, 0.4892018735408783, 0.4892018735408783, 0.48732393980026245, 0.4854460060596466, 0.48732393980026245, 0.4807511866092682, 0.49483567476272583, 0.4938967227935791, 0.4938967227935791, 0.5136150121688843, 0.5136150121688843, 0.5164319276809692, 0.5211267471313477, 0.5361502170562744, 0.5446009635925293, 0.5511736869812012, 0.5511736869812012, 0.5549295544624329, 0.5577464699745178, 0.5624412894248962, 0.5633803009986877]

  32/2898 [..............................] - ETA: 51s
 384/2898 [==>...........................] - ETA: 4s
 768/2898 [======>.......................] - ETA: 1s
1120/2898 [==========>...................] - ETA: 1s
1472/2898 [==============>...............] - ETA: 0s
1824/2898 [=================>............] - ETA: 0s
2176/2898 [=====================>........] - ETA: 0s
2560/2898 [=========================>....] - ETA: 0s
2898/2898 [==============================] - 1s 343us/step
Accuracies per class for LSTM
[0.47 0.66 0.81 0.37 0.53 0.59 0.7   nan 0.39 0.3  0.   0.3  0.02 0.
 0.24 0.   0.83 0.19 0.   0.71 0.82 0.   0.5 ]
[3.157152710594928, 3.071509653059619, 2.9216904734288245, 2.7588321259807524, 2.62415885477243, 2.5142707788302974, 2.4188860244025134, 2.335647926102061, 2.2628326862945447, 2.1955151671615036, 2.1330186537461913, 2.0680755174353225, 2.00406441656054, 1.9431671430898267, 1.8855615236234406, 1.8295400691250834, 1.776220820418876, 1.7246368682549709, 1.6759898837166602, 1.6291528573639942, 1.5843696524230477, 1.541061454377097, 1.499261346345276, 1.4589298084948805, 1.4193808263614476, 1.3808221348804406, 1.3442952478186805, 1.3060429242048182, 1.2693809402753469, 1.2333989543746215, 1.197945272646685, 1.1613280546407563, 1.1245410510994946, 1.0870322048181038, 1.0513005040713759, 1.016656542607728, 0.9842086694723344, 0.9537751062993911, 0.9246401374703258, 0.8976697794863784, 0.8694750316439309, 0.8439229724586136, 0.8189122092389209, 0.7959005753038242, 0.7731932197689366, 0.7516055536194534, 0.7311292437388915, 0.7116874435193274, 0.695234537915722, 0.6746803207634927]
[0.08104299, 0.15057552, 0.2184637, 0.24665257, 0.25839794, 0.25886774, 0.2572234, 0.25839794, 0.26286116, 0.2802443, 0.30913788, 0.3516561, 0.37890533, 0.3967583, 0.40920836, 0.42048392, 0.43528306, 0.46135777, 0.48226452, 0.5005873, 0.5184402, 0.5395819, 0.5571999, 0.5698849, 0.5830397, 0.5983087, 0.6149871, 0.6368334, 0.65680057, 0.67324406, 0.68733853, 0.6972046, 0.70190275, 0.7094198, 0.71858114, 0.73103124, 0.74160206, 0.75428706, 0.7655626, 0.7719051, 0.7820061, 0.78693914, 0.7953958, 0.79962415, 0.807611, 0.81583273, 0.8205309, 0.8273432, 0.8348602, 0.83955836]
[3.1155288425409737, 2.9869866042069986, 2.8179644459290123, 2.683475861974725, 2.5842620415306987, 2.501004225995059, 2.4236993371041167, 2.359196176439384, 2.3058377017437572, 2.260370901940574, 2.2104295992515457, 2.1599399443523426, 2.107813499679028, 2.0651446485743277, 2.0285014172674902, 1.9793036728397781, 1.9434267150963975, 1.907699032232795, 1.878942129085881, 1.844519545102903, 1.8130586923567902, 1.7824414440723653, 1.756881026482918, 1.7336859227346142, 1.705182528104021, 1.657522959104726, 1.6461993149188763, 1.6289017801833265, 1.6053485675037187, 1.5696236534857415, 1.5570973875936769, 1.5360030330962418, 1.5188424886112482, 1.5010200710643624, 1.5050031208096535, 1.4956890824814917, 1.480911036864133, 1.4621069946899101, 1.4160278007877825, 1.4342359594336138, 1.4057500227096495, 1.398030172799115, 1.3820345454792462, 1.3722691542106056, 1.357383233602618, 1.3466477623987645, 1.334466593590141, 1.2933460863123478, 1.3207741793052692, 1.3010329252677344]
[0.1727699488401413, 0.21408450603485107, 0.2112676054239273, 0.2065727710723877, 0.21032863855361938, 0.21032863855361938, 0.20375587046146393, 0.20375587046146393, 0.21220657229423523, 0.24131456017494202, 0.28075116872787476, 0.2985915541648865, 0.3089201748371124, 0.31830987334251404, 0.3211267590522766, 0.32676056027412415, 0.33145540952682495, 0.33615022897720337, 0.34460094571113586, 0.3577464818954468, 0.36901408433914185, 0.3924882709980011, 0.405633807182312, 0.41502347588539124, 0.4215962588787079, 0.4394366145133972, 0.4347417950630188, 0.4600938856601715, 0.4807511866092682, 0.4892018735408783, 0.4892018735408783, 0.48732393980026245, 0.4854460060596466, 0.48732393980026245, 0.4807511866092682, 0.49483567476272583, 0.4938967227935791, 0.4938967227935791, 0.5136150121688843, 0.5136150121688843, 0.5164319276809692, 0.5211267471313477, 0.5361502170562744, 0.5446009635925293, 0.5511736869812012, 0.5511736869812012, 0.5549295544624329, 0.5577464699745178, 0.5624412894248962, 0.5633803009986877]
recall 0.5159
precision 0.5894
f1 0.4972
mcc 0.4581
RMSE: 5.377
classification report:
              precision    recall  f1-score   support

           1       0.34      0.47      0.40        72
           2       0.39      0.66      0.49        68
           3       0.62      0.81      0.70       252
           4       0.39      0.37      0.38       132
           5       0.78      0.53      0.63       191
           6       0.35      0.59      0.44       150
           7       0.55      0.70      0.61       240
           8       0.00      0.00      0.00         0
           9       0.19      0.39      0.26       100
          10       0.67      0.30      0.41        61
          11       0.00      0.00      0.00        87
          12       0.84      0.30      0.44       595
          13       1.00      0.02      0.03        59
          14       0.00      0.00      0.00         6
          15       0.46      0.24      0.31        72
          16       0.00      0.00      0.00        30
          17       0.54      0.83      0.65       156
          18       0.70      0.19      0.30        36
          19       0.00      0.00      0.00        54
          21       0.70      0.71      0.70       222
          22       0.41      0.82      0.54       168
          23       0.00      0.00      0.00        65
          24       0.52      0.50      0.51        82

   micro avg       0.49      0.49      0.49      2898
   macro avg       0.41      0.37      0.34      2898
weighted avg       0.56      0.49      0.47      2898

>#6: 48.827
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.2009 - acc: 0.0171 - val_loss: 3.1603 - val_acc: 0.0977
Epoch 2/50
 - 12s - loss: 3.1379 - acc: 0.0646 - val_loss: 3.1088 - val_acc: 0.0939
Epoch 3/50
 - 12s - loss: 3.0806 - acc: 0.0707 - val_loss: 3.0483 - val_acc: 0.0967
Epoch 4/50
 - 12s - loss: 2.9988 - acc: 0.0872 - val_loss: 2.9586 - val_acc: 0.0911
Epoch 5/50
 - 12s - loss: 2.8783 - acc: 0.1388 - val_loss: 2.8410 - val_acc: 0.1315
Epoch 6/50
 - 12s - loss: 2.7451 - acc: 0.1569 - val_loss: 2.7326 - val_acc: 0.1531
Epoch 7/50
 - 12s - loss: 2.6303 - acc: 0.1644 - val_loss: 2.6508 - val_acc: 0.1577
Epoch 8/50
 - 12s - loss: 2.5390 - acc: 0.1689 - val_loss: 2.5863 - val_acc: 0.1643
Epoch 9/50
 - 12s - loss: 2.4630 - acc: 0.1717 - val_loss: 2.5296 - val_acc: 0.1728
Epoch 10/50
 - 12s - loss: 2.3941 - acc: 0.1804 - val_loss: 2.4767 - val_acc: 0.1906
Epoch 11/50
 - 12s - loss: 2.3280 - acc: 0.2025 - val_loss: 2.4285 - val_acc: 0.2282
Epoch 12/50
 - 12s - loss: 2.2629 - acc: 0.2241 - val_loss: 2.3801 - val_acc: 0.2582
Epoch 13/50
 - 12s - loss: 2.1938 - acc: 0.2434 - val_loss: 2.3314 - val_acc: 0.3305
Epoch 14/50
 - 12s - loss: 2.1240 - acc: 0.2918 - val_loss: 2.2827 - val_acc: 0.3446
Epoch 15/50
 - 12s - loss: 2.0546 - acc: 0.3390 - val_loss: 2.2334 - val_acc: 0.3531
Epoch 16/50
 - 12s - loss: 1.9874 - acc: 0.4012 - val_loss: 2.1846 - val_acc: 0.3690
Epoch 17/50
 - 12s - loss: 1.9250 - acc: 0.4334 - val_loss: 2.1191 - val_acc: 0.3869
Epoch 18/50
 - 12s - loss: 1.8688 - acc: 0.4428 - val_loss: 2.0639 - val_acc: 0.3850
Epoch 19/50
 - 12s - loss: 1.8097 - acc: 0.4515 - val_loss: 2.0411 - val_acc: 0.3887
Epoch 20/50
 - 12s - loss: 1.7575 - acc: 0.4585 - val_loss: 2.0112 - val_acc: 0.3925
Epoch 21/50
 - 12s - loss: 1.7100 - acc: 0.4646 - val_loss: 1.9791 - val_acc: 0.3925
Epoch 22/50
 - 12s - loss: 1.6620 - acc: 0.4712 - val_loss: 1.9683 - val_acc: 0.3962
Epoch 23/50
 - 12s - loss: 1.6202 - acc: 0.4773 - val_loss: 1.9420 - val_acc: 0.3972
Epoch 24/50
 - 12s - loss: 1.5787 - acc: 0.4851 - val_loss: 1.9424 - val_acc: 0.3991
Epoch 25/50
 - 12s - loss: 1.5427 - acc: 0.4905 - val_loss: 1.9023 - val_acc: 0.3934
Epoch 26/50
 - 12s - loss: 1.5031 - acc: 0.4987 - val_loss: 1.9268 - val_acc: 0.4038
Epoch 27/50
 - 12s - loss: 1.4709 - acc: 0.5020 - val_loss: 1.9165 - val_acc: 0.4056
Epoch 28/50
 - 12s - loss: 1.4378 - acc: 0.5074 - val_loss: 1.8841 - val_acc: 0.4131
Epoch 29/50
 - 12s - loss: 1.4039 - acc: 0.5154 - val_loss: 1.9000 - val_acc: 0.4207
Epoch 30/50
 - 12s - loss: 1.3735 - acc: 0.5208 - val_loss: 1.8946 - val_acc: 0.4235
Epoch 31/50
 - 12s - loss: 1.3444 - acc: 0.5278 - val_loss: 1.9058 - val_acc: 0.4310
Epoch 32/50
 - 12s - loss: 1.3148 - acc: 0.5346 - val_loss: 1.9137 - val_acc: 0.4376
Epoch 33/50
 - 12s - loss: 1.2884 - acc: 0.5384 - val_loss: 1.8714 - val_acc: 0.4441
Epoch 34/50
 - 13s - loss: 1.2589 - acc: 0.5478 - val_loss: 1.8757 - val_acc: 0.4451
Epoch 35/50
 - 12s - loss: 1.2325 - acc: 0.5549 - val_loss: 1.8488 - val_acc: 0.4507
Epoch 36/50
 - 12s - loss: 1.2066 - acc: 0.5584 - val_loss: 1.8207 - val_acc: 0.4601
Epoch 37/50
 - 12s - loss: 1.1822 - acc: 0.5657 - val_loss: 1.7774 - val_acc: 0.4620
Epoch 38/50
 - 12s - loss: 1.1584 - acc: 0.5725 - val_loss: 1.7691 - val_acc: 0.4714
Epoch 39/50
 - 12s - loss: 1.1331 - acc: 0.5823 - val_loss: 1.7411 - val_acc: 0.4732
Epoch 40/50
 - 12s - loss: 1.1090 - acc: 0.5896 - val_loss: 1.7208 - val_acc: 0.4732
Epoch 41/50
 - 12s - loss: 1.0856 - acc: 0.6091 - val_loss: 1.7061 - val_acc: 0.4789
Epoch 42/50
 - 12s - loss: 1.0639 - acc: 0.6246 - val_loss: 1.6953 - val_acc: 0.4967
Epoch 43/50
 - 12s - loss: 1.0397 - acc: 0.6519 - val_loss: 1.6813 - val_acc: 0.5183
Epoch 44/50
 - 12s - loss: 1.0166 - acc: 0.6711 - val_loss: 1.6675 - val_acc: 0.5333
Epoch 45/50
 - 12s - loss: 0.9936 - acc: 0.6895 - val_loss: 1.6629 - val_acc: 0.5399
Epoch 46/50
 - 12s - loss: 0.9701 - acc: 0.7047 - val_loss: 1.6509 - val_acc: 0.5512
Epoch 47/50
 - 12s - loss: 0.9472 - acc: 0.7174 - val_loss: 1.6448 - val_acc: 0.5568
Epoch 48/50
 - 12s - loss: 0.9219 - acc: 0.7280 - val_loss: 1.6398 - val_acc: 0.5653
Epoch 49/50
 - 12s - loss: 0.8982 - acc: 0.7367 - val_loss: 1.6352 - val_acc: 0.5709
Epoch 50/50
 - 12s - loss: 0.8756 - acc: 0.7458 - val_loss: 1.6348 - val_acc: 0.5662
LSTM training time: 602.2912576198578s
Model: "sequential_17"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_7 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_33 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_34 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.2008757251849724, 3.1378694785881804, 3.0805909603393102, 2.9988219565507364, 2.878314008504425, 2.745099325764759, 2.6302607225817303, 2.5389572744613695, 2.4630162160889992, 2.394071429666847, 2.3280175682634208, 2.262944452354318, 2.193779482145894, 2.1240221193930564, 2.0545561933058205, 1.987395839061652, 1.925004886186316, 1.8688087424174535, 1.809738744388927, 1.7574648283612764, 1.7099982484114515, 1.661986272716455, 1.6201598988134929, 1.5787038612679358, 1.5426901505254988, 1.5031464720993275, 1.4708633504620678, 1.4377853536874665, 1.4039353677188675, 1.373491168526212, 1.344392369379873, 1.3148423245682916, 1.2884450885984633, 1.2588891673877762, 1.232530713361391, 1.2066472067025338, 1.182180619769066, 1.1584234066151777, 1.1330573681601062, 1.109032247571674, 1.0856400454075146, 1.063873115282034, 1.0397157767918404, 1.016577468824017, 0.9935607677170276, 0.9700606186498575, 0.947240480506871, 0.9218824229205134, 0.8982262321696866, 0.8756432495249923]
[0.017148226, 0.064599484, 0.07070707, 0.087150574, 0.13883016, 0.15691802, 0.16443504, 0.16889828, 0.17171717, 0.18040873, 0.20249002, 0.22410148, 0.24336387, 0.29175475, 0.3389711, 0.4012215, 0.43340382, 0.4428001, 0.45149165, 0.4585389, 0.46464646, 0.47122386, 0.47733146, 0.4850834, 0.49048626, 0.498708, 0.5019967, 0.50739956, 0.5153864, 0.52078927, 0.5278365, 0.53464884, 0.5384073, 0.54780364, 0.5548508, 0.55837446, 0.56565654, 0.5724689, 0.582335, 0.5896171, 0.6091144, 0.6246183, 0.6518675, 0.6711299, 0.68945265, 0.7047216, 0.71740663, 0.72797745, 0.736669, 0.7458304]
[3.160261311105719, 3.108750637036534, 3.0482926769435688, 2.9586253083367864, 2.8410445396888986, 2.732574621164743, 2.6508244339848908, 2.586296801052183, 2.529640207156329, 2.476686517509496, 2.428456496296914, 2.3801016185205306, 2.3314316368998496, 2.282715224324258, 2.233424933975291, 2.1845786314055395, 2.1191061333311554, 2.063916099799071, 2.0410659696014832, 2.011213626324291, 1.9790586142472817, 1.9682676877214316, 1.942035153662095, 1.94244147623089, 1.9023397479258792, 1.926760154151021, 1.9164840863903922, 1.8841255529385776, 1.9000354171358924, 1.8946222813476419, 1.9057519685494508, 1.9137225379406566, 1.8713592489000777, 1.8756507989946105, 1.8488042018782924, 1.8206888187659178, 1.7774298166445164, 1.769127155693484, 1.741058961718295, 1.7208243947073887, 1.706141915847438, 1.6952808977852405, 1.6812554656619756, 1.6675184336066806, 1.6628549756578437, 1.6509306115163884, 1.6447675662421284, 1.6398251997473094, 1.6351561467412492, 1.6348431448981235]
[0.09765258431434631, 0.09389671683311462, 0.09671361744403839, 0.09107980877161026, 0.13145539164543152, 0.15305164456367493, 0.15774647891521454, 0.16431924700737, 0.1727699488401413, 0.19061033427715302, 0.2281690090894699, 0.2582159638404846, 0.33051642775535583, 0.34460094571113586, 0.35305163264274597, 0.36901408433914185, 0.38685446977615356, 0.3849765360355377, 0.3887324035167694, 0.3924882709980011, 0.3924882709980011, 0.3962441384792328, 0.3971830904483795, 0.39906102418899536, 0.3934272229671478, 0.40375587344169617, 0.405633807182312, 0.4131455421447754, 0.42065727710723877, 0.42347419261932373, 0.4309859275817871, 0.43755868077278137, 0.444131463766098, 0.44507041573524475, 0.4507042169570923, 0.4600938856601715, 0.46197181940078735, 0.4713614881038666, 0.4732394218444824, 0.4732394218444824, 0.47887325286865234, 0.4967136085033417, 0.5183098316192627, 0.5333333611488342, 0.5399060845375061, 0.5511736869812012, 0.5568075180053711, 0.5652582049369812, 0.5708920359611511, 0.5661971569061279]

  32/2898 [..............................] - ETA: 58s
 384/2898 [==>...........................] - ETA: 4s
 736/2898 [======>.......................] - ETA: 2s
1088/2898 [==========>...................] - ETA: 1s
1472/2898 [==============>...............] - ETA: 0s
1824/2898 [=================>............] - ETA: 0s
2176/2898 [=====================>........] - ETA: 0s
2560/2898 [=========================>....] - ETA: 0s
2898/2898 [==============================] - 1s 366us/step
Accuracies per class for LSTM
[0.1  0.85 0.67 0.   0.53 0.77 0.27  nan 0.94 0.87 0.49 0.01 0.42 0.
 0.39 0.57 0.56 0.   0.   0.5  0.89 0.   0.2 ]
[3.2008757251849724, 3.1378694785881804, 3.0805909603393102, 2.9988219565507364, 2.878314008504425, 2.745099325764759, 2.6302607225817303, 2.5389572744613695, 2.4630162160889992, 2.394071429666847, 2.3280175682634208, 2.262944452354318, 2.193779482145894, 2.1240221193930564, 2.0545561933058205, 1.987395839061652, 1.925004886186316, 1.8688087424174535, 1.809738744388927, 1.7574648283612764, 1.7099982484114515, 1.661986272716455, 1.6201598988134929, 1.5787038612679358, 1.5426901505254988, 1.5031464720993275, 1.4708633504620678, 1.4377853536874665, 1.4039353677188675, 1.373491168526212, 1.344392369379873, 1.3148423245682916, 1.2884450885984633, 1.2588891673877762, 1.232530713361391, 1.2066472067025338, 1.182180619769066, 1.1584234066151777, 1.1330573681601062, 1.109032247571674, 1.0856400454075146, 1.063873115282034, 1.0397157767918404, 1.016577468824017, 0.9935607677170276, 0.9700606186498575, 0.947240480506871, 0.9218824229205134, 0.8982262321696866, 0.8756432495249923]
[0.017148226, 0.064599484, 0.07070707, 0.087150574, 0.13883016, 0.15691802, 0.16443504, 0.16889828, 0.17171717, 0.18040873, 0.20249002, 0.22410148, 0.24336387, 0.29175475, 0.3389711, 0.4012215, 0.43340382, 0.4428001, 0.45149165, 0.4585389, 0.46464646, 0.47122386, 0.47733146, 0.4850834, 0.49048626, 0.498708, 0.5019967, 0.50739956, 0.5153864, 0.52078927, 0.5278365, 0.53464884, 0.5384073, 0.54780364, 0.5548508, 0.55837446, 0.56565654, 0.5724689, 0.582335, 0.5896171, 0.6091144, 0.6246183, 0.6518675, 0.6711299, 0.68945265, 0.7047216, 0.71740663, 0.72797745, 0.736669, 0.7458304]
[3.160261311105719, 3.108750637036534, 3.0482926769435688, 2.9586253083367864, 2.8410445396888986, 2.732574621164743, 2.6508244339848908, 2.586296801052183, 2.529640207156329, 2.476686517509496, 2.428456496296914, 2.3801016185205306, 2.3314316368998496, 2.282715224324258, 2.233424933975291, 2.1845786314055395, 2.1191061333311554, 2.063916099799071, 2.0410659696014832, 2.011213626324291, 1.9790586142472817, 1.9682676877214316, 1.942035153662095, 1.94244147623089, 1.9023397479258792, 1.926760154151021, 1.9164840863903922, 1.8841255529385776, 1.9000354171358924, 1.8946222813476419, 1.9057519685494508, 1.9137225379406566, 1.8713592489000777, 1.8756507989946105, 1.8488042018782924, 1.8206888187659178, 1.7774298166445164, 1.769127155693484, 1.741058961718295, 1.7208243947073887, 1.706141915847438, 1.6952808977852405, 1.6812554656619756, 1.6675184336066806, 1.6628549756578437, 1.6509306115163884, 1.6447675662421284, 1.6398251997473094, 1.6351561467412492, 1.6348431448981235]
[0.09765258431434631, 0.09389671683311462, 0.09671361744403839, 0.09107980877161026, 0.13145539164543152, 0.15305164456367493, 0.15774647891521454, 0.16431924700737, 0.1727699488401413, 0.19061033427715302, 0.2281690090894699, 0.2582159638404846, 0.33051642775535583, 0.34460094571113586, 0.35305163264274597, 0.36901408433914185, 0.38685446977615356, 0.3849765360355377, 0.3887324035167694, 0.3924882709980011, 0.3924882709980011, 0.3962441384792328, 0.3971830904483795, 0.39906102418899536, 0.3934272229671478, 0.40375587344169617, 0.405633807182312, 0.4131455421447754, 0.42065727710723877, 0.42347419261932373, 0.4309859275817871, 0.43755868077278137, 0.444131463766098, 0.44507041573524475, 0.4507042169570923, 0.4600938856601715, 0.46197181940078735, 0.4713614881038666, 0.4732394218444824, 0.4732394218444824, 0.47887325286865234, 0.4967136085033417, 0.5183098316192627, 0.5333333611488342, 0.5399060845375061, 0.5511736869812012, 0.5568075180053711, 0.5652582049369812, 0.5708920359611511, 0.5661971569061279]
recall 0.4288
precision 0.5734
f1 0.3827
mcc 0.3803
RMSE: 6.762
classification report:
              precision    recall  f1-score   support

           1       0.06      0.10      0.07        72
           2       0.69      0.85      0.76        68
           3       0.46      0.67      0.55       252
           4       0.00      0.00      0.00       132
           5       0.44      0.53      0.48       191
           6       0.18      0.77      0.29       150
           7       0.67      0.27      0.39       240
           8       0.00      0.00      0.00         0
           9       0.77      0.94      0.85       100
          10       0.23      0.87      0.36        61
          11       1.00      0.49      0.66        87
          12       0.70      0.01      0.02       595
          13       0.49      0.42      0.45        59
          14       0.00      0.00      0.00         6
          15       0.11      0.39      0.18        72
          16       0.50      0.57      0.53        30
          17       0.55      0.56      0.56       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.93      0.50      0.65       222
          22       0.52      0.89      0.65       168
          23       0.00      0.00      0.00        65
          24       0.80      0.20      0.31        82

   micro avg       0.40      0.40      0.40      2898
   macro avg       0.40      0.39      0.34      2898
weighted avg       0.53      0.40      0.35      2898

>#7: 39.510
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1785 - acc: 0.0350 - val_loss: 3.1447 - val_acc: 0.0451
Epoch 2/50
 - 12s - loss: 3.1294 - acc: 0.0716 - val_loss: 3.0827 - val_acc: 0.1070
Epoch 3/50
 - 13s - loss: 3.0728 - acc: 0.1111 - val_loss: 3.0116 - val_acc: 0.1014
Epoch 4/50
 - 12s - loss: 3.0009 - acc: 0.1325 - val_loss: 2.9289 - val_acc: 0.0986
Epoch 5/50
 - 12s - loss: 2.9134 - acc: 0.1555 - val_loss: 2.8404 - val_acc: 0.1746
Epoch 6/50
 - 12s - loss: 2.8191 - acc: 0.2100 - val_loss: 2.7653 - val_acc: 0.2263
Epoch 7/50
 - 12s - loss: 2.7298 - acc: 0.2415 - val_loss: 2.6964 - val_acc: 0.2413
Epoch 8/50
 - 12s - loss: 2.6385 - acc: 0.2723 - val_loss: 2.6268 - val_acc: 0.2761
Epoch 9/50
 - 12s - loss: 2.5465 - acc: 0.3091 - val_loss: 2.5600 - val_acc: 0.2995
Epoch 10/50
 - 12s - loss: 2.4610 - acc: 0.3329 - val_loss: 2.4959 - val_acc: 0.3192
Epoch 11/50
 - 12s - loss: 2.3814 - acc: 0.3519 - val_loss: 2.4377 - val_acc: 0.3408
Epoch 12/50
 - 12s - loss: 2.3062 - acc: 0.3643 - val_loss: 2.3799 - val_acc: 0.3643
Epoch 13/50
 - 12s - loss: 2.2352 - acc: 0.3723 - val_loss: 2.3236 - val_acc: 0.3624
Epoch 14/50
 - 12s - loss: 2.1647 - acc: 0.3867 - val_loss: 2.2807 - val_acc: 0.3803
Epoch 15/50
 - 12s - loss: 2.0942 - acc: 0.4019 - val_loss: 2.2268 - val_acc: 0.3981
Epoch 16/50
 - 12s - loss: 2.0269 - acc: 0.4120 - val_loss: 2.1740 - val_acc: 0.4075
Epoch 17/50
 - 12s - loss: 1.9617 - acc: 0.4257 - val_loss: 2.1235 - val_acc: 0.4131
Epoch 18/50
 - 12s - loss: 1.8997 - acc: 0.4430 - val_loss: 2.0767 - val_acc: 0.4235
Epoch 19/50
 - 12s - loss: 1.8422 - acc: 0.4599 - val_loss: 2.0315 - val_acc: 0.4347
Epoch 20/50
 - 12s - loss: 1.7856 - acc: 0.4691 - val_loss: 1.9845 - val_acc: 0.4479
Epoch 21/50
 - 12s - loss: 1.7292 - acc: 0.4893 - val_loss: 1.9433 - val_acc: 0.4460
Epoch 22/50
 - 12s - loss: 1.6744 - acc: 0.5053 - val_loss: 1.9168 - val_acc: 0.4460
Epoch 23/50
 - 12s - loss: 1.6254 - acc: 0.5281 - val_loss: 1.8846 - val_acc: 0.4507
Epoch 24/50
 - 12s - loss: 1.5752 - acc: 0.5466 - val_loss: 1.8796 - val_acc: 0.4507
Epoch 25/50
 - 12s - loss: 1.5282 - acc: 0.5619 - val_loss: 1.8611 - val_acc: 0.4441
Epoch 26/50
 - 12s - loss: 1.4830 - acc: 0.5755 - val_loss: 1.8436 - val_acc: 0.4451
Epoch 27/50
 - 12s - loss: 1.4409 - acc: 0.5819 - val_loss: 1.8108 - val_acc: 0.4685
Epoch 28/50
 - 12s - loss: 1.3986 - acc: 0.5880 - val_loss: 1.8037 - val_acc: 0.4554
Epoch 29/50
 - 12s - loss: 1.3618 - acc: 0.5948 - val_loss: 1.7708 - val_acc: 0.4873
Epoch 30/50
 - 12s - loss: 1.3237 - acc: 0.5995 - val_loss: 1.7598 - val_acc: 0.4939
Epoch 31/50
 - 12s - loss: 1.2873 - acc: 0.6058 - val_loss: 1.7488 - val_acc: 0.4883
Epoch 32/50
 - 12s - loss: 1.2569 - acc: 0.6136 - val_loss: 1.7203 - val_acc: 0.5005
Epoch 33/50
 - 13s - loss: 1.2237 - acc: 0.6263 - val_loss: 1.7117 - val_acc: 0.4986
Epoch 34/50
 - 12s - loss: 1.1938 - acc: 0.6444 - val_loss: 1.7136 - val_acc: 0.4873
Epoch 35/50
 - 12s - loss: 1.1672 - acc: 0.6540 - val_loss: 1.7242 - val_acc: 0.4873
Epoch 36/50
 - 12s - loss: 1.1341 - acc: 0.6667 - val_loss: 1.7068 - val_acc: 0.4892
Epoch 37/50
 - 12s - loss: 1.1030 - acc: 0.6737 - val_loss: 1.6772 - val_acc: 0.4995
Epoch 38/50
 - 12s - loss: 1.0751 - acc: 0.6869 - val_loss: 1.7020 - val_acc: 0.4901
Epoch 39/50
 - 12s - loss: 1.0469 - acc: 0.6953 - val_loss: 1.6818 - val_acc: 0.4948
Epoch 40/50
 - 12s - loss: 1.0193 - acc: 0.7035 - val_loss: 1.7319 - val_acc: 0.4864
Epoch 41/50
 - 12s - loss: 0.9987 - acc: 0.7089 - val_loss: 1.6789 - val_acc: 0.4939
Epoch 42/50
 - 12s - loss: 0.9680 - acc: 0.7179 - val_loss: 1.7114 - val_acc: 0.5014
Epoch 00042: early stopping
LSTM training time: 518.2776236534119s
Model: "sequential_18"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_8 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_35 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_36 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1784861176288937, 3.129445405977557, 3.072782955232405, 3.0008962346489843, 2.9133811950235544, 2.8190793167237023, 2.729759877070256, 2.6385490111910403, 2.5464962354310106, 2.4609590368649474, 2.3813623243972173, 2.306200323906868, 2.2351846724069535, 2.1646998096638117, 2.0941898714692373, 2.0268698702514913, 1.961722104684383, 1.8997066073051383, 1.8422121111148675, 1.7856362103463788, 1.7291865240443836, 1.6744360576248347, 1.6253550832249732, 1.5751846280208854, 1.5282393767852192, 1.483012470528081, 1.4408584425419688, 1.398565911367406, 1.3617999324194836, 1.3236782957417097, 1.2872573227453377, 1.2569000882975843, 1.2236753395809543, 1.1938197374875932, 1.167195494665852, 1.1341042476541032, 1.102990492985846, 1.0750896857326402, 1.0468724173700863, 1.0192688899278808, 0.9986575181450058, 0.9679512746084288]
[0.035001174, 0.0716467, 0.11111111, 0.13248767, 0.15550858, 0.21000704, 0.24148461, 0.27225745, 0.30913788, 0.3328635, 0.351891, 0.36434108, 0.37232792, 0.38665727, 0.40192625, 0.41202724, 0.42565188, 0.443035, 0.45994833, 0.4691097, 0.48931172, 0.50528544, 0.5280714, 0.5466291, 0.56189805, 0.57552266, 0.5818652, 0.58797276, 0.59478503, 0.5994832, 0.6058257, 0.61357766, 0.6262626, 0.64435047, 0.6539817, 0.6666667, 0.67371386, 0.68686867, 0.6953254, 0.7035471, 0.70895, 0.71787643]
[3.1446512826731507, 3.082724483472081, 3.0115502621646217, 2.9289346710616995, 2.8404146145207223, 2.765259652742198, 2.6963960132688425, 2.6267730256201514, 2.5599948287569863, 2.495892623220811, 2.4376996071685646, 2.379940720902922, 2.323599205330504, 2.2806680182336083, 2.2267700656478953, 2.173965109122191, 2.1235274234288175, 2.076708946989176, 2.0314691427168152, 1.9845174856588874, 1.9433190030111394, 1.9167503208061898, 1.8845878959261755, 1.8795662705327423, 1.8610782634484375, 1.843628071731245, 1.8108279477822389, 1.8037316664843492, 1.7708406003987844, 1.7597506188450844, 1.7487894701286102, 1.720317495764701, 1.7117163068811658, 1.7135716189800854, 1.7242311821856968, 1.706820298920215, 1.6771786439586693, 1.7020186485259186, 1.6817814620167997, 1.7319086345148758, 1.6788689912764678, 1.7114240601588862]
[0.04507042095065117, 0.10704225301742554, 0.101408451795578, 0.09859155118465424, 0.17464788258075714, 0.22629107534885406, 0.24131456017494202, 0.27605634927749634, 0.2995305061340332, 0.31924882531166077, 0.3408450782299042, 0.36431923508644104, 0.3624413013458252, 0.3802816867828369, 0.39812207221984863, 0.40751174092292786, 0.4131455421447754, 0.42347419261932373, 0.4347417950630188, 0.4478873312473297, 0.44600939750671387, 0.44600939750671387, 0.4507042169570923, 0.4507042169570923, 0.444131463766098, 0.44507041573524475, 0.468544602394104, 0.4553990662097931, 0.48732393980026245, 0.4938967227935791, 0.48826292157173157, 0.5004695057868958, 0.4985915422439575, 0.48732393980026245, 0.48732393980026245, 0.4892018735408783, 0.49953052401542664, 0.4901408553123474, 0.49483567476272583, 0.4863849878311157, 0.4938967227935791, 0.5014084577560425]

  32/2898 [..............................] - ETA: 1:02
 384/2898 [==>...........................] - ETA: 4s
 736/2898 [======>.......................] - ETA: 2s
1088/2898 [==========>...................] - ETA: 1s
1440/2898 [=============>................] - ETA: 0s
1792/2898 [=================>............] - ETA: 0s
2144/2898 [=====================>........] - ETA: 0s
2528/2898 [=========================>....] - ETA: 0s
2880/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 387us/step
Accuracies per class for LSTM
[0.08 0.93 0.21 0.86 0.81 0.05 0.65 0.93 0.   0.   0.21 0.   0.   0.22
 0.07 0.8  0.   0.   0.   0.69 0.   0.  ]
[3.1784861176288937, 3.129445405977557, 3.072782955232405, 3.0008962346489843, 2.9133811950235544, 2.8190793167237023, 2.729759877070256, 2.6385490111910403, 2.5464962354310106, 2.4609590368649474, 2.3813623243972173, 2.306200323906868, 2.2351846724069535, 2.1646998096638117, 2.0941898714692373, 2.0268698702514913, 1.961722104684383, 1.8997066073051383, 1.8422121111148675, 1.7856362103463788, 1.7291865240443836, 1.6744360576248347, 1.6253550832249732, 1.5751846280208854, 1.5282393767852192, 1.483012470528081, 1.4408584425419688, 1.398565911367406, 1.3617999324194836, 1.3236782957417097, 1.2872573227453377, 1.2569000882975843, 1.2236753395809543, 1.1938197374875932, 1.167195494665852, 1.1341042476541032, 1.102990492985846, 1.0750896857326402, 1.0468724173700863, 1.0192688899278808, 0.9986575181450058, 0.9679512746084288]
[0.035001174, 0.0716467, 0.11111111, 0.13248767, 0.15550858, 0.21000704, 0.24148461, 0.27225745, 0.30913788, 0.3328635, 0.351891, 0.36434108, 0.37232792, 0.38665727, 0.40192625, 0.41202724, 0.42565188, 0.443035, 0.45994833, 0.4691097, 0.48931172, 0.50528544, 0.5280714, 0.5466291, 0.56189805, 0.57552266, 0.5818652, 0.58797276, 0.59478503, 0.5994832, 0.6058257, 0.61357766, 0.6262626, 0.64435047, 0.6539817, 0.6666667, 0.67371386, 0.68686867, 0.6953254, 0.7035471, 0.70895, 0.71787643]
[3.1446512826731507, 3.082724483472081, 3.0115502621646217, 2.9289346710616995, 2.8404146145207223, 2.765259652742198, 2.6963960132688425, 2.6267730256201514, 2.5599948287569863, 2.495892623220811, 2.4376996071685646, 2.379940720902922, 2.323599205330504, 2.2806680182336083, 2.2267700656478953, 2.173965109122191, 2.1235274234288175, 2.076708946989176, 2.0314691427168152, 1.9845174856588874, 1.9433190030111394, 1.9167503208061898, 1.8845878959261755, 1.8795662705327423, 1.8610782634484375, 1.843628071731245, 1.8108279477822389, 1.8037316664843492, 1.7708406003987844, 1.7597506188450844, 1.7487894701286102, 1.720317495764701, 1.7117163068811658, 1.7135716189800854, 1.7242311821856968, 1.706820298920215, 1.6771786439586693, 1.7020186485259186, 1.6817814620167997, 1.7319086345148758, 1.6788689912764678, 1.7114240601588862]
[0.04507042095065117, 0.10704225301742554, 0.101408451795578, 0.09859155118465424, 0.17464788258075714, 0.22629107534885406, 0.24131456017494202, 0.27605634927749634, 0.2995305061340332, 0.31924882531166077, 0.3408450782299042, 0.36431923508644104, 0.3624413013458252, 0.3802816867828369, 0.39812207221984863, 0.40751174092292786, 0.4131455421447754, 0.42347419261932373, 0.4347417950630188, 0.4478873312473297, 0.44600939750671387, 0.44600939750671387, 0.4507042169570923, 0.4507042169570923, 0.444131463766098, 0.44507041573524475, 0.468544602394104, 0.4553990662097931, 0.48732393980026245, 0.4938967227935791, 0.48826292157173157, 0.5004695057868958, 0.4985915422439575, 0.48732393980026245, 0.48732393980026245, 0.4892018735408783, 0.49953052401542664, 0.4901408553123474, 0.49483567476272583, 0.4863849878311157, 0.4938967227935791, 0.5014084577560425]
recall 0.4512
precision 0.4566
f1 0.3623
mcc 0.3233
RMSE: 7.137
classification report:
              precision    recall  f1-score   support

           1       0.67      0.08      0.15        72
           2       0.34      0.93      0.50        68
           3       0.73      0.21      0.33       252
           4       0.24      0.86      0.38       132
           5       0.32      0.81      0.46       191
           6       0.05      0.05      0.05       150
           7       0.41      0.65      0.51       240
           9       0.62      0.93      0.75       100
          10       0.00      0.00      0.00        61
          11       0.00      0.00      0.00        87
          12       0.66      0.21      0.32       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.33      0.22      0.26        72
          16       0.14      0.07      0.09        30
          17       0.30      0.80      0.44       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.00      0.00      0.00       222
          22       0.37      0.69      0.48       168
          23       0.00      0.00      0.00        65
          24       0.00      0.00      0.00        82

   micro avg       0.36      0.36      0.36      2898
   macro avg       0.24      0.30      0.21      2898
weighted avg       0.36      0.36      0.29      2898

>#8: 35.611
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1544 - acc: 0.0987 - val_loss: 3.1240 - val_acc: 0.1211
Epoch 2/50
 - 12s - loss: 3.0743 - acc: 0.1323 - val_loss: 3.0353 - val_acc: 0.1258
Epoch 3/50
 - 12s - loss: 2.9684 - acc: 0.1414 - val_loss: 2.9265 - val_acc: 0.1371
Epoch 4/50
 - 12s - loss: 2.8455 - acc: 0.1555 - val_loss: 2.8150 - val_acc: 0.2056
Epoch 5/50
 - 12s - loss: 2.7201 - acc: 0.2201 - val_loss: 2.7128 - val_acc: 0.2385
Epoch 6/50
 - 12s - loss: 2.6041 - acc: 0.2225 - val_loss: 2.6221 - val_acc: 0.2394
Epoch 7/50
 - 12s - loss: 2.5004 - acc: 0.2281 - val_loss: 2.5448 - val_acc: 0.2432
Epoch 8/50
 - 12s - loss: 2.4116 - acc: 0.2356 - val_loss: 2.4775 - val_acc: 0.2488
Epoch 9/50
 - 12s - loss: 2.3304 - acc: 0.2436 - val_loss: 2.4149 - val_acc: 0.2535
Epoch 10/50
 - 12s - loss: 2.2554 - acc: 0.2537 - val_loss: 2.3572 - val_acc: 0.2638
Epoch 11/50
 - 12s - loss: 2.1856 - acc: 0.2711 - val_loss: 2.3073 - val_acc: 0.2808
Epoch 12/50
 - 12s - loss: 2.1198 - acc: 0.2824 - val_loss: 2.2600 - val_acc: 0.3136
Epoch 13/50
 - 12s - loss: 2.0582 - acc: 0.3063 - val_loss: 2.2192 - val_acc: 0.4009
Epoch 14/50
 - 12s - loss: 2.0017 - acc: 0.3780 - val_loss: 2.1813 - val_acc: 0.4235
Epoch 15/50
 - 12s - loss: 1.9484 - acc: 0.4047 - val_loss: 2.1452 - val_acc: 0.4263
Epoch 16/50
 - 13s - loss: 1.8979 - acc: 0.4141 - val_loss: 2.1126 - val_acc: 0.4282
Epoch 17/50
 - 13s - loss: 1.8484 - acc: 0.4261 - val_loss: 2.0816 - val_acc: 0.4357
Epoch 18/50
 - 12s - loss: 1.8005 - acc: 0.4409 - val_loss: 2.0533 - val_acc: 0.4423
Epoch 19/50
 - 12s - loss: 1.7552 - acc: 0.4506 - val_loss: 2.0257 - val_acc: 0.4498
Epoch 20/50
 - 12s - loss: 1.7119 - acc: 0.4623 - val_loss: 1.9998 - val_acc: 0.4516
Epoch 21/50
 - 12s - loss: 1.6713 - acc: 0.4755 - val_loss: 1.9710 - val_acc: 0.4592
Epoch 22/50
 - 12s - loss: 1.6321 - acc: 0.4893 - val_loss: 1.9431 - val_acc: 0.4638
Epoch 23/50
 - 12s - loss: 1.5940 - acc: 0.5001 - val_loss: 1.9145 - val_acc: 0.4667
Epoch 24/50
 - 12s - loss: 1.5566 - acc: 0.5100 - val_loss: 1.8894 - val_acc: 0.4751
Epoch 25/50
 - 12s - loss: 1.5196 - acc: 0.5229 - val_loss: 1.8680 - val_acc: 0.4798
Epoch 26/50
 - 12s - loss: 1.4829 - acc: 0.5401 - val_loss: 1.8466 - val_acc: 0.4883
Epoch 27/50
 - 12s - loss: 1.4460 - acc: 0.5570 - val_loss: 1.8288 - val_acc: 0.4967
Epoch 28/50
 - 12s - loss: 1.4096 - acc: 0.5746 - val_loss: 1.8021 - val_acc: 0.5080
Epoch 29/50
 - 12s - loss: 1.3729 - acc: 0.5969 - val_loss: 1.7801 - val_acc: 0.5108
Epoch 30/50
 - 12s - loss: 1.3365 - acc: 0.6133 - val_loss: 1.7539 - val_acc: 0.5230
Epoch 31/50
 - 12s - loss: 1.3003 - acc: 0.6303 - val_loss: 1.7274 - val_acc: 0.5352
Epoch 32/50
 - 12s - loss: 1.2649 - acc: 0.6469 - val_loss: 1.6990 - val_acc: 0.5437
Epoch 33/50
 - 12s - loss: 1.2301 - acc: 0.6631 - val_loss: 1.6740 - val_acc: 0.5531
Epoch 34/50
 - 12s - loss: 1.1951 - acc: 0.6761 - val_loss: 1.6592 - val_acc: 0.5559
Epoch 35/50
 - 12s - loss: 1.1614 - acc: 0.6911 - val_loss: 1.6270 - val_acc: 0.5681
Epoch 36/50
 - 12s - loss: 1.1287 - acc: 0.7031 - val_loss: 1.5923 - val_acc: 0.5784
Epoch 37/50
 - 12s - loss: 1.0970 - acc: 0.7108 - val_loss: 1.5604 - val_acc: 0.5915
Epoch 38/50
 - 12s - loss: 1.0671 - acc: 0.7195 - val_loss: 1.5329 - val_acc: 0.6066
Epoch 39/50
 - 12s - loss: 1.0380 - acc: 0.7273 - val_loss: 1.5138 - val_acc: 0.6169
Epoch 40/50
 - 12s - loss: 1.0100 - acc: 0.7336 - val_loss: 1.4935 - val_acc: 0.6169
Epoch 41/50
 - 12s - loss: 0.9831 - acc: 0.7423 - val_loss: 1.4804 - val_acc: 0.6150
Epoch 42/50
 - 12s - loss: 0.9571 - acc: 0.7463 - val_loss: 1.4674 - val_acc: 0.6188
Epoch 43/50
 - 12s - loss: 0.9323 - acc: 0.7482 - val_loss: 1.4576 - val_acc: 0.6178
Epoch 44/50
 - 12s - loss: 0.9083 - acc: 0.7526 - val_loss: 1.4508 - val_acc: 0.6225
Epoch 45/50
 - 12s - loss: 0.8852 - acc: 0.7585 - val_loss: 1.4453 - val_acc: 0.6225
Epoch 46/50
 - 12s - loss: 0.8629 - acc: 0.7663 - val_loss: 1.4436 - val_acc: 0.6225
Epoch 47/50
 - 12s - loss: 0.8412 - acc: 0.7740 - val_loss: 1.4451 - val_acc: 0.6254
Epoch 48/50
 - 12s - loss: 0.8207 - acc: 0.7869 - val_loss: 1.4486 - val_acc: 0.6310
Epoch 49/50
 - 12s - loss: 0.8006 - acc: 0.8010 - val_loss: 1.4473 - val_acc: 0.6376
Epoch 50/50
 - 12s - loss: 0.7815 - acc: 0.8090 - val_loss: 1.4514 - val_acc: 0.6413
LSTM training time: 618.3670010566711s
Model: "sequential_19"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_9 (LSTM)                (None, 8)                 2944
_________________________________________________________________
dense_37 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_38 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1543902795808205, 3.074283210274085, 2.9684177038113444, 2.845505436289498, 2.7201055717042615, 2.604063974178423, 2.500396270160527, 2.4116159367510934, 2.3303622199751444, 2.2554283489888616, 2.185563336523144, 2.119752110688933, 2.0581850001922315, 2.0016900836679334, 1.9484289954575065, 1.8979131458285607, 1.8483758256948972, 1.8005300627674383, 1.7552262210134668, 1.7119258986074992, 1.6712505922565053, 1.632111503214295, 1.594028494476796, 1.556567499621922, 1.5196453710663114, 1.482923687078428, 1.446028959640517, 1.4096178206684502, 1.372926163538986, 1.3364907793450418, 1.3003310463239197, 1.2649222393116537, 1.2300778783863073, 1.1951266864263481, 1.1614079377461914, 1.1286672494460195, 1.0970192181761622, 1.0670692024864947, 1.0380047289858072, 1.0099590776502625, 0.98310231071754, 0.9571406586309652, 0.9323457196225352, 0.9083051030375965, 0.8851607190224103, 0.8628910177535845, 0.8412497221009396, 0.8206812782637076, 0.8006343295351843, 0.7814574009037253]
[0.09866103, 0.13225275, 0.14141414, 0.15550858, 0.22010806, 0.22245713, 0.2280949, 0.23561193, 0.24359877, 0.25369978, 0.2710829, 0.28235847, 0.306319, 0.3779657, 0.40474513, 0.41414142, 0.42612168, 0.44092083, 0.45055205, 0.46229738, 0.47545218, 0.48931172, 0.5001175, 0.50998354, 0.52290344, 0.5400517, 0.556965, 0.57458305, 0.5968992, 0.6133427, 0.63025606, 0.64693445, 0.66314304, 0.67606294, 0.691097, 0.70307726, 0.7108292, 0.7195208, 0.72727275, 0.7336152, 0.74230677, 0.7463002, 0.7481795, 0.7526427, 0.75851536, 0.7662673, 0.77401924, 0.78693914, 0.8010336, 0.80902046]
[3.124002223619273, 3.035296969346597, 2.9264755651984418, 2.814987210824456, 2.7128419589548605, 2.622099150178578, 2.544827913678308, 2.4774888188626285, 2.4149245143496376, 2.3571648819345823, 2.3073174337825866, 2.2600221592495697, 2.219225478060369, 2.1813387022331847, 2.145234044840638, 2.1126401442877003, 2.0816456875890634, 2.0532979794511212, 2.0257200846090004, 1.999808849974977, 1.971006207231065, 1.943101026754424, 1.914457447987767, 1.8893727147523227, 1.868023541891519, 1.8466399791095178, 1.8288017842691269, 1.8021397898174787, 1.780071624427894, 1.7539237450825775, 1.7273570338045487, 1.698955226084436, 1.67400613360002, 1.659184319480484, 1.627026069080326, 1.5922505117358177, 1.56042060253206, 1.5329095204951058, 1.5138290540153432, 1.4935183637578722, 1.4803876162974487, 1.467360444695737, 1.4575860161736538, 1.4507951912185955, 1.4453217374327036, 1.4435758212362657, 1.4450770620449047, 1.4485694894488428, 1.4473277509212494, 1.451444134382015]
[0.12112676352262497, 0.12582159042358398, 0.13708920776844025, 0.20563380420207977, 0.23849765956401825, 0.23943662643432617, 0.24319249391555786, 0.2488262951374054, 0.2535211145877838, 0.26384976506233215, 0.28075116872787476, 0.31361502408981323, 0.4009389579296112, 0.42347419261932373, 0.4262910783290863, 0.42816901206970215, 0.4356807470321655, 0.4422535300254822, 0.44976526498794556, 0.4516431987285614, 0.4591549336910248, 0.4638497531414032, 0.46666666865348816, 0.47511738538742065, 0.4798122048377991, 0.48826292157173157, 0.4967136085033417, 0.5079812407493591, 0.5107980966567993, 0.5230047106742859, 0.5352112650871277, 0.5436619520187378, 0.5530516505241394, 0.5558685660362244, 0.5680751204490662, 0.5784037709236145, 0.591549277305603, 0.6065727472305298, 0.6169013977050781, 0.6169013977050781, 0.6150234937667847, 0.6187793612480164, 0.6178403496742249, 0.622535228729248, 0.622535228729248, 0.622535228729248, 0.6253520846366882, 0.6309859156608582, 0.6375586986541748, 0.6413145661354065]

  32/2898 [..............................] - ETA: 1:06
 384/2898 [==>...........................] - ETA: 5s
 736/2898 [======>.......................] - ETA: 2s
1088/2898 [==========>...................] - ETA: 1s
1440/2898 [=============>................] - ETA: 0s
1792/2898 [=================>............] - ETA: 0s
2144/2898 [=====================>........] - ETA: 0s
2496/2898 [========================>.....] - ETA: 0s
2848/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 404us/step
Accuracies per class for LSTM
[0.03 0.5  0.92 0.84 0.86 0.65 0.7  0.76 0.   0.29 0.16 0.   0.   0.15
 0.3  0.85 0.   0.   0.68 0.43 0.   0.77]
[3.1543902795808205, 3.074283210274085, 2.9684177038113444, 2.845505436289498, 2.7201055717042615, 2.604063974178423, 2.500396270160527, 2.4116159367510934, 2.3303622199751444, 2.2554283489888616, 2.185563336523144, 2.119752110688933, 2.0581850001922315, 2.0016900836679334, 1.9484289954575065, 1.8979131458285607, 1.8483758256948972, 1.8005300627674383, 1.7552262210134668, 1.7119258986074992, 1.6712505922565053, 1.632111503214295, 1.594028494476796, 1.556567499621922, 1.5196453710663114, 1.482923687078428, 1.446028959640517, 1.4096178206684502, 1.372926163538986, 1.3364907793450418, 1.3003310463239197, 1.2649222393116537, 1.2300778783863073, 1.1951266864263481, 1.1614079377461914, 1.1286672494460195, 1.0970192181761622, 1.0670692024864947, 1.0380047289858072, 1.0099590776502625, 0.98310231071754, 0.9571406586309652, 0.9323457196225352, 0.9083051030375965, 0.8851607190224103, 0.8628910177535845, 0.8412497221009396, 0.8206812782637076, 0.8006343295351843, 0.7814574009037253]
[0.09866103, 0.13225275, 0.14141414, 0.15550858, 0.22010806, 0.22245713, 0.2280949, 0.23561193, 0.24359877, 0.25369978, 0.2710829, 0.28235847, 0.306319, 0.3779657, 0.40474513, 0.41414142, 0.42612168, 0.44092083, 0.45055205, 0.46229738, 0.47545218, 0.48931172, 0.5001175, 0.50998354, 0.52290344, 0.5400517, 0.556965, 0.57458305, 0.5968992, 0.6133427, 0.63025606, 0.64693445, 0.66314304, 0.67606294, 0.691097, 0.70307726, 0.7108292, 0.7195208, 0.72727275, 0.7336152, 0.74230677, 0.7463002, 0.7481795, 0.7526427, 0.75851536, 0.7662673, 0.77401924, 0.78693914, 0.8010336, 0.80902046]
[3.124002223619273, 3.035296969346597, 2.9264755651984418, 2.814987210824456, 2.7128419589548605, 2.622099150178578, 2.544827913678308, 2.4774888188626285, 2.4149245143496376, 2.3571648819345823, 2.3073174337825866, 2.2600221592495697, 2.219225478060369, 2.1813387022331847, 2.145234044840638, 2.1126401442877003, 2.0816456875890634, 2.0532979794511212, 2.0257200846090004, 1.999808849974977, 1.971006207231065, 1.943101026754424, 1.914457447987767, 1.8893727147523227, 1.868023541891519, 1.8466399791095178, 1.8288017842691269, 1.8021397898174787, 1.780071624427894, 1.7539237450825775, 1.7273570338045487, 1.698955226084436, 1.67400613360002, 1.659184319480484, 1.627026069080326, 1.5922505117358177, 1.56042060253206, 1.5329095204951058, 1.5138290540153432, 1.4935183637578722, 1.4803876162974487, 1.467360444695737, 1.4575860161736538, 1.4507951912185955, 1.4453217374327036, 1.4435758212362657, 1.4450770620449047, 1.4485694894488428, 1.4473277509212494, 1.451444134382015]
[0.12112676352262497, 0.12582159042358398, 0.13708920776844025, 0.20563380420207977, 0.23849765956401825, 0.23943662643432617, 0.24319249391555786, 0.2488262951374054, 0.2535211145877838, 0.26384976506233215, 0.28075116872787476, 0.31361502408981323, 0.4009389579296112, 0.42347419261932373, 0.4262910783290863, 0.42816901206970215, 0.4356807470321655, 0.4422535300254822, 0.44976526498794556, 0.4516431987285614, 0.4591549336910248, 0.4638497531414032, 0.46666666865348816, 0.47511738538742065, 0.4798122048377991, 0.48826292157173157, 0.4967136085033417, 0.5079812407493591, 0.5107980966567993, 0.5230047106742859, 0.5352112650871277, 0.5436619520187378, 0.5530516505241394, 0.5558685660362244, 0.5680751204490662, 0.5784037709236145, 0.591549277305603, 0.6065727472305298, 0.6169013977050781, 0.6169013977050781, 0.6150234937667847, 0.6187793612480164, 0.6178403496742249, 0.622535228729248, 0.622535228729248, 0.622535228729248, 0.6253520846366882, 0.6309859156608582, 0.6375586986541748, 0.6413145661354065]
recall 0.5275
precision 0.51
f1 0.4713
mcc 0.4727
RMSE: 6.244
classification report:
              precision    recall  f1-score   support

           1       0.11      0.03      0.04        72
           2       0.40      0.50      0.44        68
           3       0.74      0.92      0.82       252
           4       0.42      0.84      0.56       132
           5       0.59      0.86      0.70       191
           6       0.36      0.65      0.46       150
           7       0.36      0.70      0.48       240
           9       0.67      0.76      0.71       100
          10       0.00      0.00      0.00        61
          11       0.53      0.29      0.37        87
          12       0.61      0.16      0.26       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.11      0.15      0.13        72
          16       0.36      0.30      0.33        30
          17       0.44      0.85      0.58       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.78      0.68      0.72       222
          22       0.48      0.43      0.46       168
          23       0.00      0.00      0.00        65
          24       0.61      0.77      0.68        82

   micro avg       0.50      0.50      0.50      2898
   macro avg       0.34      0.40      0.35      2898
weighted avg       0.48      0.50      0.45      2898

>#9: 49.931
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1370 - acc: 0.0392 - val_loss: 3.0533 - val_acc: 0.0930
Epoch 2/50
 - 12s - loss: 2.9913 - acc: 0.1252 - val_loss: 2.8646 - val_acc: 0.1484
Epoch 3/50
 - 12s - loss: 2.8250 - acc: 0.1576 - val_loss: 2.7152 - val_acc: 0.1850
Epoch 4/50
 - 12s - loss: 2.6845 - acc: 0.2192 - val_loss: 2.5994 - val_acc: 0.2019
Epoch 5/50
 - 12s - loss: 2.5716 - acc: 0.2319 - val_loss: 2.5184 - val_acc: 0.2085
Epoch 6/50
 - 12s - loss: 2.4794 - acc: 0.2537 - val_loss: 2.4528 - val_acc: 0.2404
Epoch 7/50
 - 12s - loss: 2.3940 - acc: 0.2866 - val_loss: 2.3873 - val_acc: 0.2160
Epoch 8/50
 - 12s - loss: 2.3084 - acc: 0.2962 - val_loss: 2.3175 - val_acc: 0.2272
Epoch 9/50
 - 12s - loss: 2.2243 - acc: 0.2986 - val_loss: 2.2616 - val_acc: 0.2254
Epoch 10/50
 - 12s - loss: 2.1473 - acc: 0.3037 - val_loss: 2.2023 - val_acc: 0.2310
Epoch 11/50
 - 12s - loss: 2.0807 - acc: 0.3094 - val_loss: 2.1447 - val_acc: 0.2366
Epoch 12/50
 - 12s - loss: 2.0155 - acc: 0.3181 - val_loss: 2.0962 - val_acc: 0.2451
Epoch 13/50
 - 13s - loss: 1.9587 - acc: 0.3263 - val_loss: 2.0586 - val_acc: 0.2507
Epoch 14/50
 - 12s - loss: 1.8997 - acc: 0.3340 - val_loss: 2.0195 - val_acc: 0.2638
Epoch 15/50
 - 12s - loss: 1.8474 - acc: 0.3397 - val_loss: 1.9836 - val_acc: 0.2704
Epoch 16/50
 - 12s - loss: 1.7989 - acc: 0.3470 - val_loss: 1.9424 - val_acc: 0.2770
Epoch 17/50
 - 12s - loss: 1.7519 - acc: 0.3554 - val_loss: 1.9091 - val_acc: 0.2873
Epoch 18/50
 - 12s - loss: 1.7068 - acc: 0.3683 - val_loss: 1.8713 - val_acc: 0.3099
Epoch 19/50
 - 12s - loss: 1.6630 - acc: 0.3928 - val_loss: 1.8310 - val_acc: 0.3286
Epoch 20/50
 - 12s - loss: 1.6209 - acc: 0.4214 - val_loss: 1.7969 - val_acc: 0.3531
Epoch 21/50
 - 12s - loss: 1.5798 - acc: 0.4545 - val_loss: 1.7706 - val_acc: 0.3700
Epoch 22/50
 - 12s - loss: 1.5412 - acc: 0.4802 - val_loss: 1.7236 - val_acc: 0.4019
Epoch 23/50
 - 12s - loss: 1.5042 - acc: 0.5022 - val_loss: 1.7019 - val_acc: 0.4094
Epoch 24/50
 - 12s - loss: 1.4688 - acc: 0.5234 - val_loss: 1.6829 - val_acc: 0.4150
Epoch 25/50
 - 12s - loss: 1.4346 - acc: 0.5480 - val_loss: 1.6481 - val_acc: 0.4254
Epoch 26/50
 - 12s - loss: 1.4020 - acc: 0.5762 - val_loss: 1.6245 - val_acc: 0.4423
Epoch 27/50
 - 12s - loss: 1.3696 - acc: 0.5964 - val_loss: 1.5946 - val_acc: 0.4648
Epoch 28/50
 - 12s - loss: 1.3403 - acc: 0.6054 - val_loss: 1.5689 - val_acc: 0.4817
Epoch 29/50
 - 12s - loss: 1.3102 - acc: 0.6206 - val_loss: 1.5653 - val_acc: 0.4629
Epoch 30/50
 - 12s - loss: 1.2821 - acc: 0.6281 - val_loss: 1.5296 - val_acc: 0.4798
Epoch 31/50
 - 12s - loss: 1.2563 - acc: 0.6364 - val_loss: 1.5158 - val_acc: 0.4770
Epoch 32/50
 - 12s - loss: 1.2296 - acc: 0.6441 - val_loss: 1.4993 - val_acc: 0.4798
Epoch 33/50
 - 12s - loss: 1.2045 - acc: 0.6474 - val_loss: 1.4842 - val_acc: 0.4873
Epoch 34/50
 - 12s - loss: 1.1797 - acc: 0.6537 - val_loss: 1.4557 - val_acc: 0.4873
Epoch 35/50
 - 12s - loss: 1.1578 - acc: 0.6570 - val_loss: 1.4364 - val_acc: 0.4920
Epoch 36/50
 - 12s - loss: 1.1332 - acc: 0.6693 - val_loss: 1.4410 - val_acc: 0.4714
Epoch 37/50
 - 12s - loss: 1.1108 - acc: 0.6808 - val_loss: 1.3991 - val_acc: 0.5005
Epoch 38/50
 - 12s - loss: 1.0895 - acc: 0.6831 - val_loss: 1.3933 - val_acc: 0.4995
Epoch 39/50
 - 12s - loss: 1.0664 - acc: 0.6991 - val_loss: 1.3900 - val_acc: 0.5023
Epoch 40/50
 - 12s - loss: 1.0458 - acc: 0.7033 - val_loss: 1.3623 - val_acc: 0.5249
Epoch 41/50
 - 12s - loss: 1.0276 - acc: 0.7094 - val_loss: 1.3602 - val_acc: 0.5146
Epoch 42/50
 - 12s - loss: 1.0069 - acc: 0.7169 - val_loss: 1.3435 - val_acc: 0.5315
Epoch 43/50
 - 12s - loss: 0.9887 - acc: 0.7181 - val_loss: 1.3304 - val_acc: 0.5352
Epoch 44/50
 - 12s - loss: 0.9707 - acc: 0.7195 - val_loss: 1.3206 - val_acc: 0.5418
Epoch 45/50
 - 12s - loss: 0.9530 - acc: 0.7226 - val_loss: 1.3216 - val_acc: 0.5305
Epoch 46/50
 - 12s - loss: 0.9362 - acc: 0.7259 - val_loss: 1.3033 - val_acc: 0.5568
Epoch 47/50
 - 12s - loss: 0.9218 - acc: 0.7261 - val_loss: 1.3037 - val_acc: 0.5521
Epoch 48/50
 - 12s - loss: 0.9047 - acc: 0.7296 - val_loss: 1.2887 - val_acc: 0.5549
Epoch 49/50
 - 12s - loss: 0.8883 - acc: 0.7320 - val_loss: 1.2881 - val_acc: 0.5587
Epoch 50/50
 - 12s - loss: 0.8723 - acc: 0.7336 - val_loss: 1.2801 - val_acc: 0.5624
LSTM training time: 618.7289307117462s
Model: "sequential_20"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_10 (LSTM)               (None, 8)                 2944
_________________________________________________________________
dense_39 (Dense)             (None, 8)                 72
_________________________________________________________________
dense_40 (Dense)             (None, 24)                216
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.136993249593443, 2.9913115685104286, 2.8249844634423718, 2.6845001625516254, 2.5715504508637923, 2.4794147420103125, 2.39399660140493, 2.308361425236242, 2.2242715985443997, 2.147336504396094, 2.080693247734394, 2.015504752018655, 1.9587003265660443, 1.8997343401408178, 1.847442340089145, 1.7988645588256733, 1.751910806710322, 1.7068349857803016, 1.6629580478307533, 1.620932370025226, 1.5797529561379047, 1.5411579102900825, 1.5042168513300946, 1.4687900726213292, 1.4345761601398999, 1.401987472231858, 1.3695957874758915, 1.3402729509356774, 1.3102105747429675, 1.282074986558188, 1.2562705662599438, 1.2296059216751805, 1.2044757896531544, 1.179742509969557, 1.157779988707414, 1.133247000561484, 1.1108035682539774, 1.0895295800430718, 1.0663682607409257, 1.045764607794553, 1.0275521637451377, 1.0068503995862454, 0.9886940869614079, 0.9706541847631017, 0.953011856046618, 0.936201558640677, 0.9217589993921558, 0.9047486787674599, 0.8883307425732733, 0.8723300904252592]
[0.039229505, 0.12520555, 0.15762274, 0.21916842, 0.23185341, 0.25369978, 0.2865868, 0.296218, 0.29856706, 0.30373502, 0.3093728, 0.31806436, 0.3262861, 0.33403805, 0.3396758, 0.34695795, 0.3554146, 0.3683345, 0.39276487, 0.42142352, 0.45454547, 0.48015034, 0.5022316, 0.52337325, 0.54803854, 0.57622737, 0.5964294, 0.60535586, 0.62062484, 0.6281419, 0.6363636, 0.64411557, 0.64740425, 0.6537468, 0.65703547, 0.66925067, 0.6807611, 0.6831102, 0.69908386, 0.7033122, 0.7094198, 0.7169368, 0.71811134, 0.7195208, 0.7225746, 0.7258633, 0.7260982, 0.7296218, 0.73197085, 0.7336152]
[3.053344555304084, 2.8645964503847936, 2.7151549755687445, 2.599357602070195, 2.518423711749869, 2.4528175448028136, 2.387251279275742, 2.3174584245457894, 2.261577943792925, 2.202278973351062, 2.144690993582139, 2.0961630129478346, 2.058618758653811, 2.0195033299531175, 1.9835577917770602, 1.9424275074766275, 1.909058013880197, 1.871283751138499, 1.8310186581992207, 1.7968734240867723, 1.7705544535542879, 1.7235661239131514, 1.701870012059458, 1.6828558366623283, 1.648137369066337, 1.6244803223811404, 1.5945840404067242, 1.5689365016462657, 1.5652676123968312, 1.5296498194546766, 1.5157788860406114, 1.4992705847176029, 1.484245353293531, 1.4557396465064214, 1.4363743208943398, 1.440971275870229, 1.3991311971010736, 1.3932812654356441, 1.3899853826408655, 1.362346771765203, 1.3601568694125878, 1.3435107445213157, 1.3304125877613193, 1.3206215104866477, 1.3216241189571614, 1.303286522151159, 1.3037432179764403, 1.2886744298005888, 1.288144958746825, 1.2801042982110395]
[0.0929577499628067, 0.14835681021213531, 0.18497653305530548, 0.20187793672084808, 0.20845070481300354, 0.2403755933046341, 0.21596243977546692, 0.227230042219162, 0.22535210847854614, 0.23098590970039368, 0.2366197109222412, 0.2450704276561737, 0.25070422887802124, 0.26384976506233215, 0.2704225480556488, 0.27699530124664307, 0.2873239517211914, 0.30985915660858154, 0.32863849401474, 0.35305163264274597, 0.36995306611061096, 0.4018779397010803, 0.4093896746635437, 0.41502347588539124, 0.4253521263599396, 0.4422535300254822, 0.4647887349128723, 0.4816901385784149, 0.46291080117225647, 0.4798122048377991, 0.4769953191280365, 0.4798122048377991, 0.48732393980026245, 0.48732393980026245, 0.49201878905296326, 0.4713614881038666, 0.5004695057868958, 0.49953052401542664, 0.5023474097251892, 0.5248826146125793, 0.514553964138031, 0.531455397605896, 0.5352112650871277, 0.5417840480804443, 0.5305164456367493, 0.5568075180053711, 0.5521126985549927, 0.5549295544624329, 0.5586854219436646, 0.5624412894248962]

  32/2898 [..............................] - ETA: 1:13
 384/2898 [==>...........................] - ETA: 5s
 736/2898 [======>.......................] - ETA: 2s
1120/2898 [==========>...................] - ETA: 1s
1472/2898 [==============>...............] - ETA: 0s
1824/2898 [=================>............] - ETA: 0s
2208/2898 [=====================>........] - ETA: 0s
2560/2898 [=========================>....] - ETA: 0s
2898/2898 [==============================] - 1s 426us/step
Accuracies per class for LSTM
[0.   0.57 0.89 0.   0.61 0.57 0.92  nan 0.77 0.31 0.06 0.31 0.   0.
 0.03 0.03 0.81 0.   0.   0.88 0.76 0.   1.  ]
[3.136993249593443, 2.9913115685104286, 2.8249844634423718, 2.6845001625516254, 2.5715504508637923, 2.4794147420103125, 2.39399660140493, 2.308361425236242, 2.2242715985443997, 2.147336504396094, 2.080693247734394, 2.015504752018655, 1.9587003265660443, 1.8997343401408178, 1.847442340089145, 1.7988645588256733, 1.751910806710322, 1.7068349857803016, 1.6629580478307533, 1.620932370025226, 1.5797529561379047, 1.5411579102900825, 1.5042168513300946, 1.4687900726213292, 1.4345761601398999, 1.401987472231858, 1.3695957874758915, 1.3402729509356774, 1.3102105747429675, 1.282074986558188, 1.2562705662599438, 1.2296059216751805, 1.2044757896531544, 1.179742509969557, 1.157779988707414, 1.133247000561484, 1.1108035682539774, 1.0895295800430718, 1.0663682607409257, 1.045764607794553, 1.0275521637451377, 1.0068503995862454, 0.9886940869614079, 0.9706541847631017, 0.953011856046618, 0.936201558640677, 0.9217589993921558, 0.9047486787674599, 0.8883307425732733, 0.8723300904252592]
[0.039229505, 0.12520555, 0.15762274, 0.21916842, 0.23185341, 0.25369978, 0.2865868, 0.296218, 0.29856706, 0.30373502, 0.3093728, 0.31806436, 0.3262861, 0.33403805, 0.3396758, 0.34695795, 0.3554146, 0.3683345, 0.39276487, 0.42142352, 0.45454547, 0.48015034, 0.5022316, 0.52337325, 0.54803854, 0.57622737, 0.5964294, 0.60535586, 0.62062484, 0.6281419, 0.6363636, 0.64411557, 0.64740425, 0.6537468, 0.65703547, 0.66925067, 0.6807611, 0.6831102, 0.69908386, 0.7033122, 0.7094198, 0.7169368, 0.71811134, 0.7195208, 0.7225746, 0.7258633, 0.7260982, 0.7296218, 0.73197085, 0.7336152]
[3.053344555304084, 2.8645964503847936, 2.7151549755687445, 2.599357602070195, 2.518423711749869, 2.4528175448028136, 2.387251279275742, 2.3174584245457894, 2.261577943792925, 2.202278973351062, 2.144690993582139, 2.0961630129478346, 2.058618758653811, 2.0195033299531175, 1.9835577917770602, 1.9424275074766275, 1.909058013880197, 1.871283751138499, 1.8310186581992207, 1.7968734240867723, 1.7705544535542879, 1.7235661239131514, 1.701870012059458, 1.6828558366623283, 1.648137369066337, 1.6244803223811404, 1.5945840404067242, 1.5689365016462657, 1.5652676123968312, 1.5296498194546766, 1.5157788860406114, 1.4992705847176029, 1.484245353293531, 1.4557396465064214, 1.4363743208943398, 1.440971275870229, 1.3991311971010736, 1.3932812654356441, 1.3899853826408655, 1.362346771765203, 1.3601568694125878, 1.3435107445213157, 1.3304125877613193, 1.3206215104866477, 1.3216241189571614, 1.303286522151159, 1.3037432179764403, 1.2886744298005888, 1.288144958746825, 1.2801042982110395]
[0.0929577499628067, 0.14835681021213531, 0.18497653305530548, 0.20187793672084808, 0.20845070481300354, 0.2403755933046341, 0.21596243977546692, 0.227230042219162, 0.22535210847854614, 0.23098590970039368, 0.2366197109222412, 0.2450704276561737, 0.25070422887802124, 0.26384976506233215, 0.2704225480556488, 0.27699530124664307, 0.2873239517211914, 0.30985915660858154, 0.32863849401474, 0.35305163264274597, 0.36995306611061096, 0.4018779397010803, 0.4093896746635437, 0.41502347588539124, 0.4253521263599396, 0.4422535300254822, 0.4647887349128723, 0.4816901385784149, 0.46291080117225647, 0.4798122048377991, 0.4769953191280365, 0.4798122048377991, 0.48732393980026245, 0.48732393980026245, 0.49201878905296326, 0.4713614881038666, 0.5004695057868958, 0.49953052401542664, 0.5023474097251892, 0.5248826146125793, 0.514553964138031, 0.531455397605896, 0.5352112650871277, 0.5417840480804443, 0.5305164456367493, 0.5568075180053711, 0.5521126985549927, 0.5549295544624329, 0.5586854219436646, 0.5624412894248962]
recall 0.579
precision 0.5277
f1 0.5103
mcc 0.4911
RMSE: 5.096
classification report:
              precision    recall  f1-score   support

           1       0.00      0.00      0.00        72
           2       0.38      0.57      0.46        68
           3       0.51      0.89      0.65       252
           4       0.00      0.00      0.00       132
           5       0.50      0.61      0.55       191
           6       0.28      0.57      0.38       150
           7       0.84      0.92      0.88       240
           8       0.00      0.00      0.00         0
           9       0.41      0.77      0.53       100
          10       0.37      0.31      0.34        61
          11       0.17      0.06      0.09        87
          12       0.74      0.31      0.44       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.10      0.03      0.04        72
          16       0.33      0.03      0.06        30
          17       0.59      0.81      0.68       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.50      0.88      0.64       222
          22       0.63      0.76      0.69       168
          23       0.00      0.00      0.00        65
          24       0.44      1.00      0.61        82

   micro avg       0.52      0.52      0.52      2898
   macro avg       0.30      0.37      0.31      2898
weighted avg       0.47      0.52      0.46      2898

>#10: 52.070
Output from summarize_results:
Accuracy: 0.574% (+/-0.134)
Loss: 1.647% (+/-0.505)
Recall: 0.607% (+/-0.119)
Precision: 0.633% (+/-0.111)
F1: 0.565% (+/-0.140)
MCC: 0.551% (+/-0.137)
RMSE: 4.977% (+/-1.279)
#################### end ###################
Summary of results:
   accuracy               algorithm        f1       loss       mcc  precision    recall      rmse  trainingtime
0  0.656300     Logistic Regression  0.631100   1.416500  0.644800   0.714400  0.670200  3.987000    122.771472
0  0.335400    Gaussian Naive Bayes  0.396400  22.954300  0.321300   0.557500  0.433300  6.482200      0.541251
0  0.429300           Decision Tree  0.400300  19.712600  0.402700   0.483900  0.429300  7.009500      2.716925
0  0.527300  Support Vector Machine  0.502300   1.154800  0.519800   0.479300  0.613900  5.728600   1162.428663
0  0.349200     K-Nearest Neighbors  0.315400  20.859600  0.284600   0.471600  0.429900  7.694400      1.789515
0  0.461000           Random Forest  0.509000   2.283900  0.445100   0.500000  0.611700  6.673500     51.824933
0  0.476900                 Bagging  0.433600  10.767300  0.458100   0.553100  0.487000  6.483400     18.421640
0  0.563800              Extra Tree  0.513000   9.218800  0.551800   0.621500  0.565000  4.570500      0.503705
0  0.561800       Gradient Boosting  0.516200   2.453400  0.553100   0.644100  0.573600  4.825300   1011.383440
0  0.503100   Multilayer Perceptron  0.486600   2.292000  0.476700   0.587100  0.513700  5.383400    217.519972
0  0.700552                     CNN  0.697970   1.192779  0.681250   0.733570  0.718660  3.836852    122.874800
0  0.574068                    LSTM  0.565315   1.647374  0.551465   0.632635  0.606765  4.976712    369.040501

Process finished with exit code 0
