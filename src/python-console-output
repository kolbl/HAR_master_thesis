Running ML algorithms with the following parameters:
Lags (time steps): 40
Features: 3403
Training samples: 5322
Testing samples: 2898
Logistic Regression training time: 111.6987955570221s
Logistic Regression : training accuracy is:  0.66
Missing classes in prediction:  {19.0, 14.0}
Logistic Regression : testing accuracy is:  0.6563 , recall (weighted) is:  0.6702 precision (weighted) is:  0.7144 F1 (weighted) is:  0.6311 MCC is: 0.6448 RMSE is: 3.987
-------------------------------------------------------------------------------------------------------
Accuracies per class for Logistic Regression
[0.89 0.97 0.81 0.98 0.77 0.83 0.94 0.88 0.77 0.   0.25 0.31 0.   0.76
 0.8  0.81 0.06 0.    nan 0.94 0.76 0.43 0.87]
Naive Bayes training time: 0.6489672660827637s
Naive Bayes : training accuracy is:  0.34
Missing classes in prediction:  {19.0, 12.0, 14.0}
Naive Bayes : testing accuracy is:  0.3354 , recall (weighted) is:  0.4333 precision (weighted) is:  0.5575 F1 (weighted) is:  0.3964 MCC is: 0.3213 RMSE is: 6.4822
-------------------------------------------------------------------------------------------------------
Accuracies per class for Naive Bayes
[0.32 0.87 0.46 0.58 0.4  0.54 0.1   nan 0.   0.46 0.   0.   0.12 0.
 0.69 0.27 0.79 0.36 0.    nan 0.28 0.83 0.29 0.8 ]
Decision tree training time: 3.0114808082580566s
Decision Tree : training accuracy is:  0.43
Missing classes in prediction:  set()
Decision Tree : testing accuracy is:  0.4327 , recall (weighted) is:  0.4327 precision (weighted) is:  0.4892 F1 (weighted) is:  0.403 MCC is: 0.4068 RMSE is: 6.7914
-------------------------------------------------------------------------------------------------------
Accuracies per class for Decision Tree
[0.22 0.41 0.81 0.74 0.6  0.75 0.83  nan 0.   0.62 0.   0.08 0.22 0.
 0.25 0.5  0.45 0.03 0.    nan 0.65 0.38 0.51 0.48]
Feature importances Decision Tree:
             importance
var83(t)       0.104142
var38(t-9)     0.057857
var44(t-3)     0.036583
var44(t-13)    0.035793
var83(t-40)    0.034806
...                 ...
var75(t-26)    0.000000
var77(t-26)    0.000000
var78(t-26)    0.000000
var79(t-26)    0.000000
var1(t-40)     0.000000

[3403 rows x 1 columns]
Support Vector Machine training time: 1012.8342928886414s
Support Vector Machine : training accuracy is:  0.53
Missing classes in prediction:  {1.0, 11.0, 13.0, 14.0, 16.0, 18.0, 19.0, 23.0}
Support Vector Machine : testing accuracy is:  0.5273 , recall (weighted) is:  0.6139 precision (weighted) is:  0.4793 F1 (weighted) is:  0.5023 MCC is: 0.5198 RMSE is: 5.7286
-------------------------------------------------------------------------------------------------------
Accuracies per class for Support Vector Machine
[0.   0.82 0.92 0.95 0.87 1.   0.97 0.95 0.54 0.   0.   0.   0.   0.43
 0.   0.79 0.   0.   0.41 0.75 0.   0.82]
K-Nearest Neighbors training time: 1.266676425933838s
K-Nearest Neighbors : training accuracy is:  0.35
Missing classes in prediction:  {1.0, 2.0, 10.0, 11.0, 13.0, 14.0, 15.0, 19.0, 23.0}
K-Nearest Neighbors : testing accuracy is:  0.3492 , recall (weighted) is:  0.4299 precision (weighted) is:  0.4716 F1 (weighted) is:  0.3154 MCC is: 0.2846 RMSE is: 7.6944
-------------------------------------------------------------------------------------------------------
Accuracies per class for K-Nearest Neighbors
[0.   0.   0.19 0.17 0.   0.   1.   0.89 0.   0.   0.53 0.   0.   0.
 0.27 0.12 0.   0.   1.   0.02 0.   0.56]
Random Forest training time: 26.20348882675171s
Random Forest : training accuracy is:  0.46
Missing classes in prediction:  {19.0, 12.0, 13.0, 14.0}
Random Forest : testing accuracy is:  0.461 , recall (weighted) is:  0.6117 precision (weighted) is:  0.5 F1 (weighted) is:  0.509 MCC is: 0.4451 RMSE is: 6.6735
-------------------------------------------------------------------------------------------------------
Accuracies per class for Random Forest
[0.   0.96 0.98 0.98 0.52 0.91 0.96 0.   0.62 0.   0.   0.   0.   0.17
 0.37 0.42 0.   0.   0.61 0.54 0.15 0.79]
Feature importances Random Forest:
             importance
var83(t)       0.012170
var83(t-1)     0.011087
var83(t-2)     0.009701
var83(t-6)     0.009613
var83(t-4)     0.008937
...                 ...
var19(t-32)    0.000000
var45(t-6)     0.000000
var45(t-15)    0.000000
var5(t-27)     0.000000
var5(t-14)     0.000000

[3403 rows x 1 columns]
Bagging training time: 17.127686262130737s
Bagging : training accuracy is:  0.49
Missing classes in prediction:  {14.0}
Bagging : testing accuracy is:  0.4859 , recall (weighted) is:  0.4869 precision (weighted) is:  0.5408 F1 (weighted) is:  0.4289 MCC is: 0.4682 RMSE is: 6.5699
-------------------------------------------------------------------------------------------------------
Accuracies per class for Bagging
[0.1  0.68 0.97 0.92 0.68 0.81 0.94  nan 0.   0.77 0.   0.03 0.54 0.
 0.43 0.4  0.59 0.   0.02 0.62 0.46 0.34 0.51]
Extra Tree training time: 0.4788234233856201s
Extra Tree : training accuracy is:  0.54
Missing classes in prediction:  {19.0, 14.0}
Extra Tree : testing accuracy is:  0.5404 , recall (weighted) is:  0.5518 precision (weighted) is:  0.5923 F1 (weighted) is:  0.4954 MCC is: 0.5256 RMSE is: 5.1121
-------------------------------------------------------------------------------------------------------
Accuracies per class for Extra Tree
[0.24 0.85 0.95 0.92 0.88 0.91 0.86  nan 0.   0.57 0.   0.07 0.27 0.
 0.33 0.57 0.68 0.   0.   0.85 0.56 0.46 0.83]
Feature importances Extra Forest:
             importance
var83(t-28)    0.020938
var83(t-3)     0.012701
var83(t-15)    0.011492
var83(t-8)     0.010248
var83(t-4)     0.008278
...                 ...
var61(t-21)    0.000000
var60(t-21)    0.000000
var59(t-21)    0.000000
var55(t-21)    0.000000
var20(t-18)    0.000000

[3403 rows x 1 columns]
Gradient Boosting training time: 782.5434889793396s
Gradient Boosting : training accuracy is:  0.56
Missing classes in prediction:  {19.0, 14.0}
Gradient Boosting : testing accuracy is:  0.5631 , recall (weighted) is:  0.5751 precision (weighted) is:  0.6518 F1 (weighted) is:  0.5181 MCC is: 0.5546 RMSE is: 4.8178
-------------------------------------------------------------------------------------------------------
Accuracies per class for Gradient Boosting
[0.83 0.87 0.92 0.95 0.85 0.7  0.95 0.05 0.74 0.   0.01 0.36 0.   0.75
 0.8  0.85 0.58 0.    nan 0.61 0.83 0.46 0.55]
Feature importances Gradient Boosting:
             importance
var83(t)       0.073580
var38(t-8)     0.023356
var83(t-10)    0.020813
var38(t-9)     0.017741
var83(t-36)    0.016542
...                 ...
var49(t-19)    0.000000
var46(t-19)    0.000000
var37(t-36)    0.000000
var39(t-36)    0.000000
var1(t-40)     0.000000

[3403 rows x 1 columns]
Multilayer Perceptron training time: 807.1605463027954s
Multilayer Perceptron : training accuracy is:  0.56
Missing classes in prediction:  {19.0, 14.0}
Multilayer Perceptron : testing accuracy is:  0.5635 , recall (weighted) is:  0.5754 precision (weighted) is:  0.6504 F1 (weighted) is:  0.5179 MCC is: 0.5549 RMSE is: 4.8376
-------------------------------------------------------------------------------------------------------
Accuracies per class for Multilayer Perceptron
[0.83 0.88 0.92 0.95 0.86 0.69 0.95 0.05 0.72 0.   0.01 0.37 0.   0.75
 0.8  0.85 0.61 0.    nan 0.61 0.83 0.43 0.55]
Output from summarize_results:
Accuracy: nan% (+/-nan)
Loss: nan% (+/-nan)
Recall: nan% (+/-nan)
Precision: nan% (+/-nan)
F1: nan% (+/-nan)
MCC: nan% (+/-nan)
RMSE: nan% (+/-nan)
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 4s - loss: 2.8889 - acc: 0.1533 - val_loss: 2.4201 - val_acc: 0.2404
Epoch 2/50
 - 3s - loss: 2.2812 - acc: 0.3375 - val_loss: 1.7762 - val_acc: 0.5109
Epoch 3/50
 - 3s - loss: 1.7798 - acc: 0.5009 - val_loss: 1.3584 - val_acc: 0.5808
Epoch 4/50
 - 3s - loss: 1.4105 - acc: 0.6016 - val_loss: 1.0857 - val_acc: 0.6897
Epoch 5/50
 - 3s - loss: 1.1442 - acc: 0.6723 - val_loss: 0.9351 - val_acc: 0.7333
Epoch 6/50
 - 3s - loss: 0.9569 - acc: 0.7181 - val_loss: 0.8011 - val_acc: 0.7678
Epoch 7/50
 - 3s - loss: 0.8252 - acc: 0.7697 - val_loss: 0.7424 - val_acc: 0.7881
Epoch 8/50
 - 3s - loss: 0.7292 - acc: 0.7910 - val_loss: 0.7060 - val_acc: 0.7814
Epoch 9/50
 - 3s - loss: 0.6376 - acc: 0.8166 - val_loss: 0.6374 - val_acc: 0.8129
Epoch 10/50
 - 3s - loss: 0.5570 - acc: 0.8411 - val_loss: 0.6363 - val_acc: 0.8084
Epoch 11/50
 - 3s - loss: 0.5238 - acc: 0.8534 - val_loss: 0.6056 - val_acc: 0.8287
Epoch 12/50
 - 3s - loss: 0.4577 - acc: 0.8767 - val_loss: 0.6027 - val_acc: 0.8152
Epoch 13/50
 - 3s - loss: 0.4253 - acc: 0.8822 - val_loss: 0.5773 - val_acc: 0.8279
Epoch 14/50
 - 3s - loss: 0.3725 - acc: 0.9013 - val_loss: 0.5798 - val_acc: 0.8249
Epoch 15/50
 - 3s - loss: 0.3548 - acc: 0.8995 - val_loss: 0.5697 - val_acc: 0.8310
Epoch 16/50
 - 3s - loss: 0.3263 - acc: 0.9100 - val_loss: 0.6148 - val_acc: 0.8144
Epoch 17/50
 - 3s - loss: 0.2909 - acc: 0.9151 - val_loss: 0.5841 - val_acc: 0.8144
Epoch 18/50
 - 3s - loss: 0.2741 - acc: 0.9238 - val_loss: 0.5577 - val_acc: 0.8347
Epoch 19/50
 - 3s - loss: 0.2366 - acc: 0.9369 - val_loss: 0.5985 - val_acc: 0.8212
Epoch 20/50
 - 3s - loss: 0.2239 - acc: 0.9401 - val_loss: 0.5886 - val_acc: 0.8122
Epoch 21/50
 - 3s - loss: 0.2205 - acc: 0.9359 - val_loss: 0.6170 - val_acc: 0.8159
Epoch 22/50
 - 3s - loss: 0.1987 - acc: 0.9436 - val_loss: 0.6253 - val_acc: 0.8129
Epoch 23/50
 - 3s - loss: 0.1829 - acc: 0.9494 - val_loss: 0.6098 - val_acc: 0.8144
Epoch 00023: early stopping
CNN training time: 66.58548045158386s
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_1 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                81984     
_________________________________________________________________
dropout_1 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.888871405797326, 2.281152427110628, 1.779800955909023, 1.410534415262455, 1.144213981941376, 0.9569005621011448, 0.8251733242821595, 0.7292418995957548, 0.6375683021306501, 0.5569607934336274, 0.5238197150325626, 0.45766665258175787, 0.42529080674397374, 0.37253449156496976, 0.35484135032665176, 0.326344234658317, 0.2909412805588806, 0.2740599368554798, 0.2366045206425054, 0.22388870858030216, 0.220463853688467, 0.198703138111211, 0.18287803887123896]
[0.15334503, 0.3375094, 0.50087696, 0.6016036, 0.6722626, 0.71811575, 0.7697319, 0.7910298, 0.8165873, 0.8411426, 0.8534202, 0.87672263, 0.88223505, 0.9012779, 0.8995239, 0.9100476, 0.9150589, 0.9238286, 0.93685794, 0.9401153, 0.9358557, 0.9436231, 0.9493861]
[2.4200692745548364, 1.776158356693434, 1.3583778316010935, 1.0857227327376269, 0.9350679671905436, 0.8010974679125443, 0.7424144081891725, 0.7059832731365757, 0.6374394376982622, 0.6362542497112827, 0.6056289001160957, 0.6027200146903883, 0.5773084435011432, 0.5797880966821406, 0.5697490208643012, 0.6148227979686404, 0.584091310236031, 0.5577318960206707, 0.5984673597909245, 0.588593929395705, 0.6169592811778829, 0.6253241239250334, 0.6097845577524398]
[0.24042072892189026, 0.5108940601348877, 0.5807663202285767, 0.6897069811820984, 0.7332832217216492, 0.7678437232971191, 0.7881292104721069, 0.7813674211502075, 0.8129225969314575, 0.8084146976470947, 0.8287002444267273, 0.8151765465736389, 0.8279489278793335, 0.8249436616897583, 0.8309541940689087, 0.8144252300262451, 0.8144252300262451, 0.8347107172012329, 0.8211870789527893, 0.8121712803840637, 0.8159278631210327, 0.8129225969314575, 0.8144252300262451]

  32/2898 [..............................] - ETA: 3s
 608/2898 [=====>........................] - ETA: 0s
1216/2898 [===========>..................] - ETA: 0s
1792/2898 [=================>............] - ETA: 0s
2368/2898 [=======================>......] - ETA: 0s
2816/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 0s 107us/step
Accuracies per class for CNN
[0.83 0.94 0.97 0.95 0.87 0.59 0.89 0.92 0.66 0.   0.31 0.22 0.   0.79
 0.8  0.82 0.03 0.   0.92 0.84 0.31 0.9 ]
[2.888871405797326, 2.281152427110628, 1.779800955909023, 1.410534415262455, 1.144213981941376, 0.9569005621011448, 0.8251733242821595, 0.7292418995957548, 0.6375683021306501, 0.5569607934336274, 0.5238197150325626, 0.45766665258175787, 0.42529080674397374, 0.37253449156496976, 0.35484135032665176, 0.326344234658317, 0.2909412805588806, 0.2740599368554798, 0.2366045206425054, 0.22388870858030216, 0.220463853688467, 0.198703138111211, 0.18287803887123896]
[0.15334503, 0.3375094, 0.50087696, 0.6016036, 0.6722626, 0.71811575, 0.7697319, 0.7910298, 0.8165873, 0.8411426, 0.8534202, 0.87672263, 0.88223505, 0.9012779, 0.8995239, 0.9100476, 0.9150589, 0.9238286, 0.93685794, 0.9401153, 0.9358557, 0.9436231, 0.9493861]
[2.4200692745548364, 1.776158356693434, 1.3583778316010935, 1.0857227327376269, 0.9350679671905436, 0.8010974679125443, 0.7424144081891725, 0.7059832731365757, 0.6374394376982622, 0.6362542497112827, 0.6056289001160957, 0.6027200146903883, 0.5773084435011432, 0.5797880966821406, 0.5697490208643012, 0.6148227979686404, 0.584091310236031, 0.5577318960206707, 0.5984673597909245, 0.588593929395705, 0.6169592811778829, 0.6253241239250334, 0.6097845577524398]
[0.24042072892189026, 0.5108940601348877, 0.5807663202285767, 0.6897069811820984, 0.7332832217216492, 0.7678437232971191, 0.7881292104721069, 0.7813674211502075, 0.8129225969314575, 0.8084146976470947, 0.8287002444267273, 0.8151765465736389, 0.8279489278793335, 0.8249436616897583, 0.8309541940689087, 0.8144252300262451, 0.8144252300262451, 0.8347107172012329, 0.8211870789527893, 0.8121712803840637, 0.8159278631210327, 0.8129225969314575, 0.8144252300262451]
recall 0.6846
precision 0.7386
f1 0.6631
mcc 0.6587
RMSE: 3.606
classification report:
              precision    recall  f1-score   support

           1       0.65      0.83      0.73        72
           2       0.56      0.94      0.70        68
           3       0.94      0.97      0.95       252
           4       0.83      0.95      0.89       132
           5       0.97      0.87      0.92       191
           6       0.85      0.59      0.69       150
           7       0.39      0.89      0.54       240
           9       0.54      0.92      0.68       100
          10       0.62      0.66      0.63        61
          11       0.00      0.00      0.00        87
          12       0.91      0.31      0.47       595
          13       0.54      0.22      0.31        59
          14       0.00      0.00      0.00         6
          15       0.73      0.79      0.76        72
          16       0.77      0.80      0.79        30
          17       0.70      0.82      0.75       156
          18       0.01      0.03      0.02        36
          19       0.00      0.00      0.00        54
          21       0.93      0.92      0.93       222
          22       0.71      0.84      0.77       168
          23       0.62      0.31      0.41        65
          24       0.53      0.90      0.67        82

   micro avg       0.67      0.67      0.67      2898
   macro avg       0.58      0.62      0.57      2898
weighted avg       0.72      0.67      0.65      2898

>#1: 67.046
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 3s - loss: 2.9385 - acc: 0.1546 - val_loss: 2.4933 - val_acc: 0.3238
Epoch 2/50
 - 3s - loss: 2.4222 - acc: 0.3170 - val_loss: 1.9774 - val_acc: 0.5079
Epoch 3/50
 - 3s - loss: 2.0115 - acc: 0.4458 - val_loss: 1.5966 - val_acc: 0.5710
Epoch 4/50
 - 3s - loss: 1.6595 - acc: 0.5232 - val_loss: 1.3043 - val_acc: 0.6071
Epoch 5/50
 - 3s - loss: 1.4255 - acc: 0.5863 - val_loss: 1.1204 - val_acc: 0.6566
Epoch 6/50
 - 3s - loss: 1.1977 - acc: 0.6577 - val_loss: 0.9619 - val_acc: 0.7288
Epoch 7/50
 - 5s - loss: 1.0337 - acc: 0.7058 - val_loss: 0.8555 - val_acc: 0.7491
Epoch 8/50
 - 4s - loss: 0.9007 - acc: 0.7427 - val_loss: 0.7673 - val_acc: 0.7844
Epoch 9/50
 - 3s - loss: 0.8024 - acc: 0.7722 - val_loss: 0.7007 - val_acc: 0.8009
Epoch 10/50
 - 3s - loss: 0.6970 - acc: 0.8063 - val_loss: 0.6702 - val_acc: 0.8137
Epoch 11/50
 - 3s - loss: 0.6372 - acc: 0.8271 - val_loss: 0.6209 - val_acc: 0.8377
Epoch 12/50
 - 3s - loss: 0.5554 - acc: 0.8494 - val_loss: 0.6127 - val_acc: 0.8219
Epoch 13/50
 - 3s - loss: 0.5161 - acc: 0.8612 - val_loss: 0.5894 - val_acc: 0.8310
Epoch 14/50
 - 3s - loss: 0.4718 - acc: 0.8735 - val_loss: 0.5895 - val_acc: 0.8295
Epoch 15/50
 - 3s - loss: 0.4257 - acc: 0.8845 - val_loss: 0.5735 - val_acc: 0.8317
Epoch 16/50
 - 3s - loss: 0.4050 - acc: 0.8867 - val_loss: 0.5649 - val_acc: 0.8340
Epoch 17/50
 - 3s - loss: 0.3531 - acc: 0.8973 - val_loss: 0.5692 - val_acc: 0.8242
Epoch 18/50
 - 3s - loss: 0.3368 - acc: 0.9118 - val_loss: 0.5630 - val_acc: 0.8242
Epoch 19/50
 - 3s - loss: 0.3081 - acc: 0.9171 - val_loss: 0.5810 - val_acc: 0.8174
Epoch 20/50
 - 3s - loss: 0.2809 - acc: 0.9236 - val_loss: 0.5812 - val_acc: 0.8249
Epoch 21/50
 - 3s - loss: 0.2515 - acc: 0.9328 - val_loss: 0.5640 - val_acc: 0.8272
Epoch 22/50
 - 3s - loss: 0.2348 - acc: 0.9376 - val_loss: 0.5685 - val_acc: 0.8234
Epoch 23/50
 - 3s - loss: 0.2288 - acc: 0.9321 - val_loss: 0.5897 - val_acc: 0.8182
Epoch 00023: early stopping
CNN training time: 71.49931073188782s
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_2 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 64)                81984     
_________________________________________________________________
dropout_2 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.938450888161431, 2.4221971339784867, 2.011477956454524, 1.65945539283453, 1.425536536653854, 1.1976914086531492, 1.033679305333448, 0.900705752964289, 0.8023541670994608, 0.6970349356619139, 0.6371529060889636, 0.5554481306913946, 0.516112182925548, 0.47182577029420136, 0.42568681038663153, 0.40498791790859734, 0.3531488406287732, 0.3368481717253307, 0.3081019324921925, 0.28092233880360556, 0.25151850458619907, 0.23482737715938082, 0.22878722587529876]
[0.15459785, 0.31696317, 0.44575295, 0.52317715, 0.5863192, 0.65772986, 0.70583814, 0.742671, 0.77223754, 0.80631423, 0.827111, 0.8494112, 0.8611877, 0.8734653, 0.88449013, 0.88674515, 0.89726883, 0.9118016, 0.9170634, 0.923578, 0.93284893, 0.9376096, 0.9320972]
[2.4933351978917124, 1.9773503098718033, 1.596606314148695, 1.3043292777866102, 1.1203844086466472, 0.9618964345690498, 0.8554768278553891, 0.7673316763260539, 0.7006856098282693, 0.6702433989444855, 0.6208624837694061, 0.612711152612147, 0.5893518706945636, 0.5895161406330521, 0.5735480533324578, 0.5649358147759989, 0.569150637377759, 0.5629833127897585, 0.581023373064573, 0.5811848640432273, 0.5639685954643079, 0.5685092210971696, 0.5897256503940558]
[0.3238166868686676, 0.5078887939453125, 0.570999264717102, 0.6070623397827148, 0.656649112701416, 0.7287753820419312, 0.749060869216919, 0.7843726277351379, 0.8009015917778015, 0.8136739134788513, 0.8377159833908081, 0.8219383955001831, 0.8309541940689087, 0.8294515609741211, 0.8317055106163025, 0.8339594006538391, 0.8241923451423645, 0.8241923451423645, 0.8174304962158203, 0.8249436616897583, 0.8271976113319397, 0.8234410285949707, 0.8181818127632141]

  32/2898 [..............................] - ETA: 18s
 352/2898 [==>...........................] - ETA: 1s 
 864/2898 [=======>......................] - ETA: 0s
1376/2898 [=============>................] - ETA: 0s
1888/2898 [==================>...........] - ETA: 0s
2304/2898 [======================>.......] - ETA: 0s
2560/2898 [=========================>....] - ETA: 0s
2898/2898 [==============================] - 1s 196us/step
Accuracies per class for CNN
[0.39 0.97 0.9  0.87 0.8  0.15 0.81 0.86 0.7  0.   0.51 0.46 0.   0.79
 0.73 0.79 0.03 0.    nan 0.88 0.81 0.38 0.72]
[2.938450888161431, 2.4221971339784867, 2.011477956454524, 1.65945539283453, 1.425536536653854, 1.1976914086531492, 1.033679305333448, 0.900705752964289, 0.8023541670994608, 0.6970349356619139, 0.6371529060889636, 0.5554481306913946, 0.516112182925548, 0.47182577029420136, 0.42568681038663153, 0.40498791790859734, 0.3531488406287732, 0.3368481717253307, 0.3081019324921925, 0.28092233880360556, 0.25151850458619907, 0.23482737715938082, 0.22878722587529876]
[0.15459785, 0.31696317, 0.44575295, 0.52317715, 0.5863192, 0.65772986, 0.70583814, 0.742671, 0.77223754, 0.80631423, 0.827111, 0.8494112, 0.8611877, 0.8734653, 0.88449013, 0.88674515, 0.89726883, 0.9118016, 0.9170634, 0.923578, 0.93284893, 0.9376096, 0.9320972]
[2.4933351978917124, 1.9773503098718033, 1.596606314148695, 1.3043292777866102, 1.1203844086466472, 0.9618964345690498, 0.8554768278553891, 0.7673316763260539, 0.7006856098282693, 0.6702433989444855, 0.6208624837694061, 0.612711152612147, 0.5893518706945636, 0.5895161406330521, 0.5735480533324578, 0.5649358147759989, 0.569150637377759, 0.5629833127897585, 0.581023373064573, 0.5811848640432273, 0.5639685954643079, 0.5685092210971696, 0.5897256503940558]
[0.3238166868686676, 0.5078887939453125, 0.570999264717102, 0.6070623397827148, 0.656649112701416, 0.7287753820419312, 0.749060869216919, 0.7843726277351379, 0.8009015917778015, 0.8136739134788513, 0.8377159833908081, 0.8219383955001831, 0.8309541940689087, 0.8294515609741211, 0.8317055106163025, 0.8339594006538391, 0.8241923451423645, 0.8241923451423645, 0.8174304962158203, 0.8249436616897583, 0.8271976113319397, 0.8234410285949707, 0.8181818127632141]
recall 0.6635
precision 0.6947
f1 0.6451
mcc 0.6296
RMSE: 3.931
classification report:
              precision    recall  f1-score   support

           1       0.74      0.39      0.51        72
           2       0.30      0.97      0.46        68
           3       0.90      0.90      0.90       252
           4       0.82      0.87      0.84       132
           5       0.59      0.80      0.68       191
           6       0.40      0.15      0.22       150
           7       0.49      0.81      0.61       240
           9       0.49      0.86      0.63       100
          10       0.63      0.70      0.67        61
          11       0.00      0.00      0.00        87
          12       0.96      0.51      0.67       595
          13       0.71      0.46      0.56        59
          14       0.00      0.00      0.00         6
          15       0.78      0.79      0.79        72
          16       0.92      0.73      0.81        30
          17       0.68      0.79      0.73       156
          18       0.06      0.03      0.04        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.93      0.88      0.91       222
          22       0.48      0.81      0.61       168
          23       0.66      0.38      0.49        65
          24       0.56      0.72      0.63        82

   micro avg       0.65      0.65      0.65      2898
   macro avg       0.53      0.55      0.51      2898
weighted avg       0.68      0.65      0.63      2898

>#2: 64.976
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 4s - loss: 2.9697 - acc: 0.1619 - val_loss: 2.5343 - val_acc: 0.3095
Epoch 2/50
 - 3s - loss: 2.3366 - acc: 0.3488 - val_loss: 1.8648 - val_acc: 0.4981
Epoch 3/50
 - 3s - loss: 1.8590 - acc: 0.4793 - val_loss: 1.4592 - val_acc: 0.5740
Epoch 4/50
 - 4s - loss: 1.5059 - acc: 0.5733 - val_loss: 1.1805 - val_acc: 0.6206
Epoch 5/50
 - 3s - loss: 1.2544 - acc: 0.6470 - val_loss: 1.0452 - val_acc: 0.6574
Epoch 6/50
 - 3s - loss: 1.0733 - acc: 0.6978 - val_loss: 0.9226 - val_acc: 0.7168
Epoch 7/50
 - 3s - loss: 0.9491 - acc: 0.7439 - val_loss: 0.8363 - val_acc: 0.7273
Epoch 8/50
 - 3s - loss: 0.8171 - acc: 0.7682 - val_loss: 0.7644 - val_acc: 0.7739
Epoch 9/50
 - 3s - loss: 0.7166 - acc: 0.7940 - val_loss: 0.7003 - val_acc: 0.7881
Epoch 10/50
 - 3s - loss: 0.6298 - acc: 0.8234 - val_loss: 0.6815 - val_acc: 0.7956
Epoch 11/50
 - 3s - loss: 0.5619 - acc: 0.8452 - val_loss: 0.6434 - val_acc: 0.8144
Epoch 12/50
 - 3s - loss: 0.5226 - acc: 0.8517 - val_loss: 0.6148 - val_acc: 0.8302
Epoch 13/50
 - 3s - loss: 0.4820 - acc: 0.8642 - val_loss: 0.6047 - val_acc: 0.8422
Epoch 14/50
 - 3s - loss: 0.4092 - acc: 0.8870 - val_loss: 0.6138 - val_acc: 0.8204
Epoch 15/50
 - 3s - loss: 0.3829 - acc: 0.8930 - val_loss: 0.5997 - val_acc: 0.8325
Epoch 16/50
 - 3s - loss: 0.3400 - acc: 0.9005 - val_loss: 0.6211 - val_acc: 0.8174
Epoch 17/50
 - 3s - loss: 0.3228 - acc: 0.9108 - val_loss: 0.5850 - val_acc: 0.8400
Epoch 18/50
 - 3s - loss: 0.2869 - acc: 0.9186 - val_loss: 0.6036 - val_acc: 0.8385
Epoch 19/50
 - 3s - loss: 0.2643 - acc: 0.9253 - val_loss: 0.6014 - val_acc: 0.8332
Epoch 20/50
 - 3s - loss: 0.2541 - acc: 0.9273 - val_loss: 0.6292 - val_acc: 0.8355
Epoch 21/50
 - 5s - loss: 0.2261 - acc: 0.9351 - val_loss: 0.6157 - val_acc: 0.8430
Epoch 22/50
 - 4s - loss: 0.2179 - acc: 0.9391 - val_loss: 0.6319 - val_acc: 0.8385
Epoch 00022: early stopping
CNN training time: 72.06944704055786s
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_3 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 64)                81984     
_________________________________________________________________
dropout_3 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_6 (Dense)              (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9697326533748405, 2.33663061549418, 1.8590464302768268, 1.505863185414722, 1.2543722009106602, 1.0732771425942704, 0.9491104711889305, 0.8171198181490631, 0.7166180100634534, 0.6297686110330927, 0.5619150200769804, 0.5225977648764362, 0.48203073301350463, 0.4091529450608292, 0.3829085964007003, 0.34003254481087614, 0.3228218433425684, 0.28690954182964157, 0.2642796837087072, 0.2541206353199999, 0.22612753807051286, 0.21792648496683442]
[0.16186419, 0.34878477, 0.47932848, 0.57328993, 0.64695567, 0.69782007, 0.74392384, 0.76822853, 0.79403657, 0.8233525, 0.8451516, 0.8516663, 0.86419445, 0.88699573, 0.89300925, 0.90052617, 0.9107993, 0.91856676, 0.925332, 0.9273365, 0.935104, 0.939113]
[2.534269720651797, 1.864787464568169, 1.4592443098930556, 1.1804923928663928, 1.0451633627173105, 0.9225739719812177, 0.8363037619538253, 0.7644154254555602, 0.7002572313368819, 0.6815018562435933, 0.643403997252591, 0.6147705381171138, 0.6046665193913449, 0.6138185678833998, 0.5996643907244381, 0.6210689815959108, 0.5849865106049064, 0.6036109464928187, 0.6013996701386675, 0.6291641797993668, 0.6156628150501215, 0.6318506524056194]
[0.3095417022705078, 0.4981217086315155, 0.5740045309066772, 0.6205860376358032, 0.6574004292488098, 0.7167543172836304, 0.7272727489471436, 0.7738542556762695, 0.7881292104721069, 0.7956423759460449, 0.8144252300262451, 0.8302028775215149, 0.8422238826751709, 0.8204357624053955, 0.8324568271636963, 0.8174304962158203, 0.8399699330329895, 0.8384672999382019, 0.8332081437110901, 0.8354620337486267, 0.8429751992225647, 0.8384672999382019]

  32/2898 [..............................] - ETA: 7s
 576/2898 [====>.........................] - ETA: 0s
1184/2898 [===========>..................] - ETA: 0s
1792/2898 [=================>............] - ETA: 0s
2400/2898 [=======================>......] - ETA: 0s
2898/2898 [==============================] - 0s 114us/step
Accuracies per class for CNN
[0.79 0.94 0.98 0.97 0.84 0.03 0.95  nan 0.9  0.56 0.   0.69 0.32 0.
 0.76 0.67 0.79 0.   0.    nan 0.89 0.89 0.37 0.72]
[2.9697326533748405, 2.33663061549418, 1.8590464302768268, 1.505863185414722, 1.2543722009106602, 1.0732771425942704, 0.9491104711889305, 0.8171198181490631, 0.7166180100634534, 0.6297686110330927, 0.5619150200769804, 0.5225977648764362, 0.48203073301350463, 0.4091529450608292, 0.3829085964007003, 0.34003254481087614, 0.3228218433425684, 0.28690954182964157, 0.2642796837087072, 0.2541206353199999, 0.22612753807051286, 0.21792648496683442]
[0.16186419, 0.34878477, 0.47932848, 0.57328993, 0.64695567, 0.69782007, 0.74392384, 0.76822853, 0.79403657, 0.8233525, 0.8451516, 0.8516663, 0.86419445, 0.88699573, 0.89300925, 0.90052617, 0.9107993, 0.91856676, 0.925332, 0.9273365, 0.935104, 0.939113]
[2.534269720651797, 1.864787464568169, 1.4592443098930556, 1.1804923928663928, 1.0451633627173105, 0.9225739719812177, 0.8363037619538253, 0.7644154254555602, 0.7002572313368819, 0.6815018562435933, 0.643403997252591, 0.6147705381171138, 0.6046665193913449, 0.6138185678833998, 0.5996643907244381, 0.6210689815959108, 0.5849865106049064, 0.6036109464928187, 0.6013996701386675, 0.6291641797993668, 0.6156628150501215, 0.6318506524056194]
[0.3095417022705078, 0.4981217086315155, 0.5740045309066772, 0.6205860376358032, 0.6574004292488098, 0.7167543172836304, 0.7272727489471436, 0.7738542556762695, 0.7881292104721069, 0.7956423759460449, 0.8144252300262451, 0.8302028775215149, 0.8422238826751709, 0.8204357624053955, 0.8324568271636963, 0.8174304962158203, 0.8399699330329895, 0.8384672999382019, 0.8332081437110901, 0.8354620337486267, 0.8429751992225647, 0.8384672999382019]
recall 0.7287
precision 0.7302
f1 0.6974
mcc 0.6947
RMSE: 3.758
classification report:
              precision    recall  f1-score   support

           1       0.66      0.79      0.72        72
           2       0.62      0.94      0.75        68
           3       0.92      0.98      0.95       252
           4       0.89      0.97      0.93       132
           5       0.91      0.84      0.87       191
           6       0.57      0.03      0.05       150
           7       0.49      0.95      0.65       240
           8       0.00      0.00      0.00         0
           9       0.58      0.90      0.70       100
          10       0.59      0.56      0.57        61
          11       0.00      0.00      0.00        87
          12       0.94      0.69      0.79       595
          13       0.34      0.32      0.33        59
          14       0.00      0.00      0.00         6
          15       0.70      0.76      0.73        72
          16       0.83      0.67      0.74        30
          17       0.66      0.79      0.72       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.94      0.89      0.91       222
          22       0.53      0.89      0.67       168
          23       0.89      0.37      0.52        65
          24       0.59      0.72      0.65        82

   micro avg       0.71      0.71      0.71      2898
   macro avg       0.53      0.54      0.51      2898
weighted avg       0.72      0.71      0.68      2898

>#3: 71.360
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 3s - loss: 2.9747 - acc: 0.1243 - val_loss: 2.5997 - val_acc: 0.1886
Epoch 2/50
 - 3s - loss: 2.4520 - acc: 0.2974 - val_loss: 1.9614 - val_acc: 0.4914
Epoch 3/50
 - 4s - loss: 1.9521 - acc: 0.4598 - val_loss: 1.4809 - val_acc: 0.5853
Epoch 4/50
 - 3s - loss: 1.5507 - acc: 0.5748 - val_loss: 1.2004 - val_acc: 0.6574
Epoch 5/50
 - 3s - loss: 1.2890 - acc: 0.6477 - val_loss: 1.0199 - val_acc: 0.7077
Epoch 6/50
 - 3s - loss: 1.0699 - acc: 0.7051 - val_loss: 0.8732 - val_acc: 0.7431
Epoch 7/50
 - 3s - loss: 0.9111 - acc: 0.7497 - val_loss: 0.7822 - val_acc: 0.7739
Epoch 8/50
 - 3s - loss: 0.7934 - acc: 0.7785 - val_loss: 0.7291 - val_acc: 0.7784
Epoch 9/50
 - 5s - loss: 0.6815 - acc: 0.8181 - val_loss: 0.6901 - val_acc: 0.7956
Epoch 10/50
 - 4s - loss: 0.6036 - acc: 0.8361 - val_loss: 0.6536 - val_acc: 0.8084
Epoch 11/50
 - 3s - loss: 0.5399 - acc: 0.8489 - val_loss: 0.6273 - val_acc: 0.8137
Epoch 12/50
 - 3s - loss: 0.4869 - acc: 0.8652 - val_loss: 0.6221 - val_acc: 0.8107
Epoch 13/50
 - 3s - loss: 0.4294 - acc: 0.8810 - val_loss: 0.6114 - val_acc: 0.8257
Epoch 14/50
 - 3s - loss: 0.4021 - acc: 0.8950 - val_loss: 0.6198 - val_acc: 0.8189
Epoch 15/50
 - 3s - loss: 0.3406 - acc: 0.9100 - val_loss: 0.5988 - val_acc: 0.8234
Epoch 16/50
 - 3s - loss: 0.3241 - acc: 0.9073 - val_loss: 0.6044 - val_acc: 0.8257
Epoch 17/50
 - 3s - loss: 0.2885 - acc: 0.9216 - val_loss: 0.6004 - val_acc: 0.8264
Epoch 18/50
 - 3s - loss: 0.2684 - acc: 0.9283 - val_loss: 0.6034 - val_acc: 0.8279
Epoch 19/50
 - 3s - loss: 0.2456 - acc: 0.9318 - val_loss: 0.5962 - val_acc: 0.8370
Epoch 20/50
 - 3s - loss: 0.2216 - acc: 0.9411 - val_loss: 0.6453 - val_acc: 0.8249
Epoch 21/50
 - 3s - loss: 0.2176 - acc: 0.9401 - val_loss: 0.6637 - val_acc: 0.8152
Epoch 22/50
 - 3s - loss: 0.1906 - acc: 0.9464 - val_loss: 0.6398 - val_acc: 0.8310
Epoch 23/50
 - 3s - loss: 0.1816 - acc: 0.9496 - val_loss: 0.6397 - val_acc: 0.8219
Epoch 24/50
 - 3s - loss: 0.1771 - acc: 0.9504 - val_loss: 0.6536 - val_acc: 0.8212
Epoch 00024: early stopping
CNN training time: 75.52279710769653s
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_4 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_4 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 64)                81984     
_________________________________________________________________
dropout_4 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_8 (Dense)              (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.974723910667848, 2.4520445139194247, 1.952126433177233, 1.5506636867711783, 1.2889831764327162, 1.0698550992667026, 0.9110946724202043, 0.7933851572753725, 0.6814713440206582, 0.6036130265554797, 0.5399301235478703, 0.48693534443635644, 0.4293719182063303, 0.4021127017264366, 0.340607138085317, 0.3241488381789004, 0.288478215225143, 0.26840613237144023, 0.24560438464245188, 0.22159445487232238, 0.21763415804896447, 0.19063243275605113, 0.18158314138651785, 0.17705514691287036]
[0.124279626, 0.2974192, 0.4597845, 0.5747933, 0.64770734, 0.70508647, 0.7496868, 0.77850163, 0.8180907, 0.8361313, 0.84891003, 0.8651967, 0.8809822, 0.8950138, 0.9100476, 0.9072914, 0.9215735, 0.92833877, 0.9318467, 0.9411175, 0.9401153, 0.94637936, 0.9496367, 0.9503884]
[2.5996930566521894, 1.9613640589162338, 1.480927944219735, 1.2004215856167404, 1.0199449796587856, 0.8731623300939664, 0.7822459306236241, 0.7290931393292325, 0.6900953465319084, 0.6536278223956828, 0.6272902800934215, 0.6220847436886038, 0.6113702575681391, 0.6198346229330626, 0.5987686699617617, 0.6044008130389409, 0.6004219830228716, 0.6033680126830978, 0.596245153677421, 0.6452773642740357, 0.663688785644641, 0.6398385449171528, 0.6396945860405018, 0.6535930679023907]
[0.18858002126216888, 0.4913598895072937, 0.5852742195129395, 0.6574004292488098, 0.7077385187149048, 0.7430503368377686, 0.7738542556762695, 0.7783621549606323, 0.7956423759460449, 0.8084146976470947, 0.8136739134788513, 0.8106686472892761, 0.8256949782371521, 0.8189331293106079, 0.8234410285949707, 0.8256949782371521, 0.8264462947845459, 0.8279489278793335, 0.8369646668434143, 0.8249436616897583, 0.8151765465736389, 0.8309541940689087, 0.8219383955001831, 0.8211870789527893]

  32/2898 [..............................] - ETA: 9s
 640/2898 [=====>........................] - ETA: 0s
1248/2898 [===========>..................] - ETA: 0s
1824/2898 [=================>............] - ETA: 0s
2432/2898 [========================>.....] - ETA: 0s
2898/2898 [==============================] - 0s 120us/step
Accuracies per class for CNN
[0.88 0.94 0.96 0.86 0.84 0.02 0.76 0.89 0.69 0.   0.66 0.29 0.   0.79
 0.53 0.79 0.17 0.02 0.9  0.86 0.35 0.72]
[2.974723910667848, 2.4520445139194247, 1.952126433177233, 1.5506636867711783, 1.2889831764327162, 1.0698550992667026, 0.9110946724202043, 0.7933851572753725, 0.6814713440206582, 0.6036130265554797, 0.5399301235478703, 0.48693534443635644, 0.4293719182063303, 0.4021127017264366, 0.340607138085317, 0.3241488381789004, 0.288478215225143, 0.26840613237144023, 0.24560438464245188, 0.22159445487232238, 0.21763415804896447, 0.19063243275605113, 0.18158314138651785, 0.17705514691287036]
[0.124279626, 0.2974192, 0.4597845, 0.5747933, 0.64770734, 0.70508647, 0.7496868, 0.77850163, 0.8180907, 0.8361313, 0.84891003, 0.8651967, 0.8809822, 0.8950138, 0.9100476, 0.9072914, 0.9215735, 0.92833877, 0.9318467, 0.9411175, 0.9401153, 0.94637936, 0.9496367, 0.9503884]
[2.5996930566521894, 1.9613640589162338, 1.480927944219735, 1.2004215856167404, 1.0199449796587856, 0.8731623300939664, 0.7822459306236241, 0.7290931393292325, 0.6900953465319084, 0.6536278223956828, 0.6272902800934215, 0.6220847436886038, 0.6113702575681391, 0.6198346229330626, 0.5987686699617617, 0.6044008130389409, 0.6004219830228716, 0.6033680126830978, 0.596245153677421, 0.6452773642740357, 0.663688785644641, 0.6398385449171528, 0.6396945860405018, 0.6535930679023907]
[0.18858002126216888, 0.4913598895072937, 0.5852742195129395, 0.6574004292488098, 0.7077385187149048, 0.7430503368377686, 0.7738542556762695, 0.7783621549606323, 0.7956423759460449, 0.8084146976470947, 0.8136739134788513, 0.8106686472892761, 0.8256949782371521, 0.8189331293106079, 0.8234410285949707, 0.8256949782371521, 0.8264462947845459, 0.8279489278793335, 0.8369646668434143, 0.8249436616897583, 0.8151765465736389, 0.8309541940689087, 0.8219383955001831, 0.8211870789527893]
recall 0.6895
precision 0.6905
f1 0.6585
mcc 0.6661
RMSE: 3.778
classification report:
              precision    recall  f1-score   support

           1       0.62      0.88      0.73        72
           2       0.49      0.94      0.64        68
           3       0.87      0.96      0.91       252
           4       0.67      0.86      0.75       132
           5       0.65      0.84      0.74       191
           6       0.30      0.02      0.04       150
           7       0.55      0.76      0.64       240
           9       0.53      0.89      0.67       100
          10       0.54      0.69      0.60        61
          11       0.00      0.00      0.00        87
          12       0.95      0.66      0.78       595
          13       0.28      0.29      0.29        59
          14       0.00      0.00      0.00         6
          15       0.77      0.79      0.78        72
          16       0.84      0.53      0.65        30
          17       0.69      0.79      0.74       156
          18       0.33      0.17      0.22        36
          19       1.00      0.02      0.04        54
          21       0.88      0.90      0.89       222
          22       0.59      0.86      0.70       168
          23       0.72      0.35      0.47        65
          24       0.56      0.72      0.63        82

   micro avg       0.69      0.69      0.69      2898
   macro avg       0.58      0.59      0.54      2898
weighted avg       0.69      0.69      0.66      2898

>#4: 68.806
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 3s - loss: 2.9350 - acc: 0.1646 - val_loss: 2.5142 - val_acc: 0.2787
Epoch 2/50
 - 3s - loss: 2.3038 - acc: 0.3586 - val_loss: 1.8892 - val_acc: 0.4996
Epoch 3/50
 - 3s - loss: 1.7915 - acc: 0.4966 - val_loss: 1.4659 - val_acc: 0.5845
Epoch 4/50
 - 3s - loss: 1.4587 - acc: 0.5901 - val_loss: 1.1879 - val_acc: 0.6664
Epoch 5/50
 - 3s - loss: 1.2155 - acc: 0.6480 - val_loss: 1.0097 - val_acc: 0.7122
Epoch 6/50
 - 3s - loss: 1.0126 - acc: 0.7174 - val_loss: 0.8786 - val_acc: 0.7438
Epoch 7/50
 - 3s - loss: 0.8889 - acc: 0.7489 - val_loss: 0.7977 - val_acc: 0.7693
Epoch 8/50
 - 3s - loss: 0.7774 - acc: 0.7767 - val_loss: 0.7402 - val_acc: 0.7746
Epoch 9/50
 - 3s - loss: 0.6865 - acc: 0.8036 - val_loss: 0.7069 - val_acc: 0.7934
Epoch 10/50
 - 3s - loss: 0.6064 - acc: 0.8346 - val_loss: 0.6516 - val_acc: 0.8174
Epoch 11/50
 - 3s - loss: 0.5320 - acc: 0.8562 - val_loss: 0.6458 - val_acc: 0.8069
Epoch 12/50
 - 3s - loss: 0.4797 - acc: 0.8647 - val_loss: 0.6322 - val_acc: 0.8069
Epoch 13/50
 - 3s - loss: 0.4384 - acc: 0.8717 - val_loss: 0.6028 - val_acc: 0.8264
Epoch 14/50
 - 3s - loss: 0.3824 - acc: 0.8905 - val_loss: 0.6155 - val_acc: 0.8122
Epoch 15/50
 - 3s - loss: 0.3722 - acc: 0.8895 - val_loss: 0.5933 - val_acc: 0.8219
Epoch 16/50
 - 3s - loss: 0.3201 - acc: 0.9080 - val_loss: 0.6203 - val_acc: 0.8114
Epoch 17/50
 - 3s - loss: 0.3059 - acc: 0.9108 - val_loss: 0.5894 - val_acc: 0.8242
Epoch 18/50
 - 3s - loss: 0.2795 - acc: 0.9238 - val_loss: 0.6011 - val_acc: 0.8114
Epoch 19/50
 - 3s - loss: 0.2635 - acc: 0.9311 - val_loss: 0.6092 - val_acc: 0.8137
Epoch 20/50
 - 3s - loss: 0.2475 - acc: 0.9286 - val_loss: 0.5867 - val_acc: 0.8249
Epoch 21/50
 - 3s - loss: 0.2208 - acc: 0.9356 - val_loss: 0.5946 - val_acc: 0.8332
Epoch 22/50
 - 3s - loss: 0.1971 - acc: 0.9454 - val_loss: 0.6105 - val_acc: 0.8204
Epoch 23/50
 - 3s - loss: 0.1975 - acc: 0.9436 - val_loss: 0.6139 - val_acc: 0.8114
Epoch 24/50
 - 3s - loss: 0.1837 - acc: 0.9446 - val_loss: 0.6116 - val_acc: 0.8249
Epoch 25/50
 - 3s - loss: 0.1629 - acc: 0.9539 - val_loss: 0.6450 - val_acc: 0.8122
Epoch 00025: early stopping
CNN training time: 74.01151776313782s
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_5 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_5 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_5 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_9 (Dense)              (None, 64)                81984     
_________________________________________________________________
dropout_5 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_10 (Dense)             (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.935032612636435, 2.3037573960306372, 1.791493692853731, 1.4586570323385397, 1.2155004008461945, 1.0125523923899287, 0.8888607026932239, 0.7774433113196577, 0.68651089098445, 0.6063791781651526, 0.5319859146655386, 0.479736455680956, 0.4384078714936837, 0.38238067423424954, 0.37217719104080454, 0.3201400155993139, 0.3059287898995925, 0.27945500056991956, 0.2635416043238354, 0.2475168688688573, 0.22079100423825518, 0.19708990784029798, 0.19748730832524966, 0.18373107892798188, 0.16290562075948495]
[0.1646204, 0.35855675, 0.49661738, 0.5900777, 0.6479579, 0.7173641, 0.7489351, 0.7767477, 0.803558, 0.8346279, 0.8561764, 0.86469555, 0.8717114, 0.89050364, 0.8895014, 0.9080431, 0.9107993, 0.9238286, 0.93109494, 0.92858934, 0.9356051, 0.9453771, 0.9436231, 0.9446254, 0.9538963]
[2.5141617332876938, 1.8892055390182965, 1.465862038203149, 1.1879334385142257, 1.0096965460589535, 0.8785968799777972, 0.7976779432769285, 0.7402286517306473, 0.7068520440534772, 0.6516363420569184, 0.6458143789650401, 0.6322230825996036, 0.6027852513963479, 0.6155163032790103, 0.5933498081175739, 0.6203433441162389, 0.5894064539910636, 0.6010641209535187, 0.6091509151197552, 0.5867258926169504, 0.5946264509162099, 0.610499075601196, 0.6139256880725479, 0.6116365701660271, 0.6449819146834884]
[0.27873778343200684, 0.4996243417263031, 0.5845229029655457, 0.6664162278175354, 0.7122464179992676, 0.7438016533851624, 0.7693463563919067, 0.7746055722236633, 0.7933884263038635, 0.8174304962158203, 0.8069121241569519, 0.8069121241569519, 0.8264462947845459, 0.8121712803840637, 0.8219383955001831, 0.8114199638366699, 0.8241923451423645, 0.8114199638366699, 0.8136739134788513, 0.8249436616897583, 0.8332081437110901, 0.8204357624053955, 0.8114199638366699, 0.8249436616897583, 0.8121712803840637]

  32/2898 [..............................] - ETA: 11s
 608/2898 [=====>........................] - ETA: 0s 
1184/2898 [===========>..................] - ETA: 0s
1792/2898 [=================>............] - ETA: 0s
2400/2898 [=======================>......] - ETA: 0s
2898/2898 [==============================] - 0s 130us/step
Accuracies per class for CNN
[0.88 0.93 0.8  0.78 0.86 0.39 0.93 0.92 0.75 0.   0.46 0.32 0.   0.82
 0.63 0.77 0.   0.   0.89 0.79 0.35 0.71]
[2.935032612636435, 2.3037573960306372, 1.791493692853731, 1.4586570323385397, 1.2155004008461945, 1.0125523923899287, 0.8888607026932239, 0.7774433113196577, 0.68651089098445, 0.6063791781651526, 0.5319859146655386, 0.479736455680956, 0.4384078714936837, 0.38238067423424954, 0.37217719104080454, 0.3201400155993139, 0.3059287898995925, 0.27945500056991956, 0.2635416043238354, 0.2475168688688573, 0.22079100423825518, 0.19708990784029798, 0.19748730832524966, 0.18373107892798188, 0.16290562075948495]
[0.1646204, 0.35855675, 0.49661738, 0.5900777, 0.6479579, 0.7173641, 0.7489351, 0.7767477, 0.803558, 0.8346279, 0.8561764, 0.86469555, 0.8717114, 0.89050364, 0.8895014, 0.9080431, 0.9107993, 0.9238286, 0.93109494, 0.92858934, 0.9356051, 0.9453771, 0.9436231, 0.9446254, 0.9538963]
[2.5141617332876938, 1.8892055390182965, 1.465862038203149, 1.1879334385142257, 1.0096965460589535, 0.8785968799777972, 0.7976779432769285, 0.7402286517306473, 0.7068520440534772, 0.6516363420569184, 0.6458143789650401, 0.6322230825996036, 0.6027852513963479, 0.6155163032790103, 0.5933498081175739, 0.6203433441162389, 0.5894064539910636, 0.6010641209535187, 0.6091509151197552, 0.5867258926169504, 0.5946264509162099, 0.610499075601196, 0.6139256880725479, 0.6116365701660271, 0.6449819146834884]
[0.27873778343200684, 0.4996243417263031, 0.5845229029655457, 0.6664162278175354, 0.7122464179992676, 0.7438016533851624, 0.7693463563919067, 0.7746055722236633, 0.7933884263038635, 0.8174304962158203, 0.8069121241569519, 0.8069121241569519, 0.8264462947845459, 0.8121712803840637, 0.8219383955001831, 0.8114199638366699, 0.8241923451423645, 0.8114199638366699, 0.8136739134788513, 0.8249436616897583, 0.8332081437110901, 0.8204357624053955, 0.8114199638366699, 0.8249436616897583, 0.8121712803840637]
recall 0.6751
precision 0.708
f1 0.6549
mcc 0.6437
RMSE: 3.766
classification report:
              precision    recall  f1-score   support

           1       0.68      0.88      0.77        72
           2       0.46      0.93      0.61        68
           3       0.85      0.80      0.82       252
           4       0.67      0.78      0.72       132
           5       0.92      0.86      0.89       191
           6       0.81      0.39      0.52       150
           7       0.41      0.93      0.57       240
           9       0.58      0.92      0.71       100
          10       0.48      0.75      0.59        61
          11       0.00      0.00      0.00        87
          12       0.93      0.46      0.62       595
          13       0.46      0.32      0.38        59
          14       0.00      0.00      0.00         6
          15       0.75      0.82      0.78        72
          16       0.79      0.63      0.70        30
          17       0.69      0.77      0.73       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.86      0.89      0.87       222
          22       0.58      0.79      0.67       168
          23       0.72      0.35      0.47        65
          24       0.53      0.71      0.60        82

   micro avg       0.66      0.66      0.66      2898
   macro avg       0.55      0.59      0.55      2898
weighted avg       0.69      0.66      0.64      2898

>#5: 66.115
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 3s - loss: 2.9317 - acc: 0.1684 - val_loss: 2.5379 - val_acc: 0.2434
Epoch 2/50
 - 3s - loss: 2.3441 - acc: 0.3222 - val_loss: 1.9549 - val_acc: 0.3599
Epoch 3/50
 - 3s - loss: 1.8952 - acc: 0.4605 - val_loss: 1.5723 - val_acc: 0.5485
Epoch 4/50
 - 3s - loss: 1.5558 - acc: 0.5482 - val_loss: 1.2492 - val_acc: 0.6454
Epoch 5/50
 - 3s - loss: 1.2696 - acc: 0.6442 - val_loss: 1.0417 - val_acc: 0.7010
Epoch 6/50
 - 3s - loss: 1.0831 - acc: 0.6908 - val_loss: 0.9226 - val_acc: 0.7333
Epoch 7/50
 - 3s - loss: 0.9395 - acc: 0.7296 - val_loss: 0.8027 - val_acc: 0.7821
Epoch 8/50
 - 3s - loss: 0.8182 - acc: 0.7750 - val_loss: 0.7196 - val_acc: 0.8032
Epoch 9/50
 - 3s - loss: 0.7385 - acc: 0.7845 - val_loss: 0.6557 - val_acc: 0.8174
Epoch 10/50
 - 3s - loss: 0.6456 - acc: 0.8191 - val_loss: 0.6174 - val_acc: 0.8182
Epoch 11/50
 - 3s - loss: 0.5763 - acc: 0.8384 - val_loss: 0.5902 - val_acc: 0.8234
Epoch 12/50
 - 3s - loss: 0.5127 - acc: 0.8589 - val_loss: 0.5593 - val_acc: 0.8242
Epoch 13/50
 - 3s - loss: 0.4785 - acc: 0.8637 - val_loss: 0.5509 - val_acc: 0.8355
Epoch 14/50
 - 3s - loss: 0.4327 - acc: 0.8730 - val_loss: 0.5340 - val_acc: 0.8400
Epoch 15/50
 - 3s - loss: 0.3834 - acc: 0.8930 - val_loss: 0.5295 - val_acc: 0.8355
Epoch 16/50
 - 3s - loss: 0.3543 - acc: 0.9008 - val_loss: 0.5138 - val_acc: 0.8497
Epoch 17/50
 - 3s - loss: 0.3235 - acc: 0.9126 - val_loss: 0.5257 - val_acc: 0.8400
Epoch 18/50
 - 3s - loss: 0.2956 - acc: 0.9223 - val_loss: 0.5332 - val_acc: 0.8415
Epoch 19/50
 - 3s - loss: 0.2711 - acc: 0.9258 - val_loss: 0.4907 - val_acc: 0.8520
Epoch 20/50
 - 3s - loss: 0.2625 - acc: 0.9251 - val_loss: 0.5398 - val_acc: 0.8407
Epoch 21/50
 - 3s - loss: 0.2432 - acc: 0.9306 - val_loss: 0.5223 - val_acc: 0.8445
Epoch 22/50
 - 3s - loss: 0.2243 - acc: 0.9361 - val_loss: 0.5316 - val_acc: 0.8445
Epoch 23/50
 - 3s - loss: 0.2099 - acc: 0.9406 - val_loss: 0.5208 - val_acc: 0.8497
Epoch 24/50
 - 3s - loss: 0.2020 - acc: 0.9389 - val_loss: 0.5300 - val_acc: 0.8497
Epoch 00024: early stopping
CNN training time: 74.4977080821991s
Model: "sequential_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_6 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_6 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_11 (Dense)             (None, 64)                81984     
_________________________________________________________________
dropout_6 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_12 (Dense)             (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.93171206882083, 2.3440709871229926, 1.8952254962705335, 1.5557916347853105, 1.2696290348762185, 1.0831053320167245, 0.9395013254152649, 0.8182186468459332, 0.7384643315819944, 0.6456158992393813, 0.5763120689131144, 0.5127381825362468, 0.4784957845568138, 0.43268632267536916, 0.3833998560306783, 0.3543114929734007, 0.3235462151280066, 0.29561322100348636, 0.2710985547303885, 0.2625460338327353, 0.24316492468039458, 0.2243427727001622, 0.20986159021365786, 0.20195017866711337]
[0.16837886, 0.322225, 0.4605362, 0.5482335, 0.64419943, 0.6908043, 0.7296417, 0.7749937, 0.78451514, 0.8190929, 0.83838636, 0.8589326, 0.8636933, 0.87296414, 0.89300925, 0.90077674, 0.91255325, 0.92232525, 0.9258331, 0.92508143, 0.93059385, 0.93610626, 0.94061637, 0.93886244]
[2.5378829726234224, 1.9548941481547817, 1.5722587784985955, 1.2491907039524288, 1.0417042598436195, 0.9226476631759142, 0.8027481315020755, 0.7196186569517242, 0.6556548909084755, 0.6174116419060792, 0.590248167815854, 0.5593011970224193, 0.5508877279735985, 0.5339532067138067, 0.5295498962157247, 0.5137787825056838, 0.525730507943037, 0.533214820657523, 0.4907032103985998, 0.5398236702641855, 0.5222684370025847, 0.5315501451005011, 0.5207829201990221, 0.5299946367584015]
[0.24342599511146545, 0.3598797917366028, 0.5484598278999329, 0.6453794240951538, 0.7009767293930054, 0.7332832217216492, 0.7821186780929565, 0.8031555414199829, 0.8174304962158203, 0.8181818127632141, 0.8234410285949707, 0.8241923451423645, 0.8354620337486267, 0.8399699330329895, 0.8354620337486267, 0.8497370481491089, 0.8399699330329895, 0.8414725661277771, 0.8519909977912903, 0.8407212495803833, 0.8444778323173523, 0.8444778323173523, 0.8497370481491089, 0.8497370481491089]

  32/2898 [..............................] - ETA: 14s
 608/2898 [=====>........................] - ETA: 0s 
1152/2898 [==========>...................] - ETA: 0s
1728/2898 [================>.............] - ETA: 0s
2304/2898 [======================>.......] - ETA: 0s
2880/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 0s 147us/step
Accuracies per class for CNN
[0.69 0.87 0.98 0.92 0.88 0.48 0.9  0.98 0.72 0.   0.5  0.42 0.   0.75
 0.77 0.76 0.31 0.    nan 0.89 0.84 0.32 0.73]
[2.93171206882083, 2.3440709871229926, 1.8952254962705335, 1.5557916347853105, 1.2696290348762185, 1.0831053320167245, 0.9395013254152649, 0.8182186468459332, 0.7384643315819944, 0.6456158992393813, 0.5763120689131144, 0.5127381825362468, 0.4784957845568138, 0.43268632267536916, 0.3833998560306783, 0.3543114929734007, 0.3235462151280066, 0.29561322100348636, 0.2710985547303885, 0.2625460338327353, 0.24316492468039458, 0.2243427727001622, 0.20986159021365786, 0.20195017866711337]
[0.16837886, 0.322225, 0.4605362, 0.5482335, 0.64419943, 0.6908043, 0.7296417, 0.7749937, 0.78451514, 0.8190929, 0.83838636, 0.8589326, 0.8636933, 0.87296414, 0.89300925, 0.90077674, 0.91255325, 0.92232525, 0.9258331, 0.92508143, 0.93059385, 0.93610626, 0.94061637, 0.93886244]
[2.5378829726234224, 1.9548941481547817, 1.5722587784985955, 1.2491907039524288, 1.0417042598436195, 0.9226476631759142, 0.8027481315020755, 0.7196186569517242, 0.6556548909084755, 0.6174116419060792, 0.590248167815854, 0.5593011970224193, 0.5508877279735985, 0.5339532067138067, 0.5295498962157247, 0.5137787825056838, 0.525730507943037, 0.533214820657523, 0.4907032103985998, 0.5398236702641855, 0.5222684370025847, 0.5315501451005011, 0.5207829201990221, 0.5299946367584015]
[0.24342599511146545, 0.3598797917366028, 0.5484598278999329, 0.6453794240951538, 0.7009767293930054, 0.7332832217216492, 0.7821186780929565, 0.8031555414199829, 0.8174304962158203, 0.8181818127632141, 0.8234410285949707, 0.8241923451423645, 0.8354620337486267, 0.8399699330329895, 0.8354620337486267, 0.8497370481491089, 0.8399699330329895, 0.8414725661277771, 0.8519909977912903, 0.8407212495803833, 0.8444778323173523, 0.8444778323173523, 0.8497370481491089, 0.8497370481491089]
recall 0.7125
precision 0.7307
f1 0.6946
mcc 0.68
RMSE: 3.897
classification report:
              precision    recall  f1-score   support

           1       0.57      0.69      0.62        72
           2       0.49      0.87      0.62        68
           3       0.94      0.98      0.96       252
           4       0.88      0.92      0.90       132
           5       0.93      0.88      0.91       191
           6       0.82      0.48      0.61       150
           7       0.48      0.90      0.63       240
           9       0.59      0.98      0.73       100
          10       0.62      0.72      0.67        61
          11       0.00      0.00      0.00        87
          12       0.88      0.50      0.64       595
          13       0.54      0.42      0.48        59
          14       0.00      0.00      0.00         6
          15       0.84      0.75      0.79        72
          16       0.85      0.77      0.81        30
          17       0.71      0.76      0.74       156
          18       0.27      0.31      0.29        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.84      0.89      0.86       222
          22       0.51      0.84      0.64       168
          23       0.75      0.32      0.45        65
          24       0.60      0.73      0.66        82

   micro avg       0.70      0.70      0.70      2898
   macro avg       0.57      0.60      0.56      2898
weighted avg       0.72      0.70      0.68      2898

>#6: 69.772
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 3s - loss: 2.9102 - acc: 0.1867 - val_loss: 2.3845 - val_acc: 0.3621
Epoch 2/50
 - 3s - loss: 2.2811 - acc: 0.3698 - val_loss: 1.7503 - val_acc: 0.5222
Epoch 3/50
 - 3s - loss: 1.7795 - acc: 0.5147 - val_loss: 1.3469 - val_acc: 0.6566
Epoch 4/50
 - 3s - loss: 1.4136 - acc: 0.6121 - val_loss: 1.1127 - val_acc: 0.7175
Epoch 5/50
 - 3s - loss: 1.2012 - acc: 0.6607 - val_loss: 0.9523 - val_acc: 0.7731
Epoch 6/50
 - 4s - loss: 1.0017 - acc: 0.7176 - val_loss: 0.8514 - val_acc: 0.7919
Epoch 7/50
 - 5s - loss: 0.8538 - acc: 0.7592 - val_loss: 0.7728 - val_acc: 0.8032
Epoch 8/50
 - 3s - loss: 0.7450 - acc: 0.7898 - val_loss: 0.7357 - val_acc: 0.8122
Epoch 9/50
 - 4s - loss: 0.6655 - acc: 0.8151 - val_loss: 0.6999 - val_acc: 0.8174
Epoch 10/50
 - 4s - loss: 0.6006 - acc: 0.8331 - val_loss: 0.6525 - val_acc: 0.8249
Epoch 11/50
 - 3s - loss: 0.5265 - acc: 0.8527 - val_loss: 0.6395 - val_acc: 0.8272
Epoch 12/50
 - 3s - loss: 0.4831 - acc: 0.8682 - val_loss: 0.6325 - val_acc: 0.8257
Epoch 13/50
 - 3s - loss: 0.4397 - acc: 0.8720 - val_loss: 0.6198 - val_acc: 0.8242
Epoch 14/50
 - 3s - loss: 0.4081 - acc: 0.8900 - val_loss: 0.6204 - val_acc: 0.8182
Epoch 15/50
 - 3s - loss: 0.3606 - acc: 0.8983 - val_loss: 0.6164 - val_acc: 0.8257
Epoch 16/50
 - 3s - loss: 0.3219 - acc: 0.9088 - val_loss: 0.6063 - val_acc: 0.8272
Epoch 17/50
 - 3s - loss: 0.3046 - acc: 0.9136 - val_loss: 0.5869 - val_acc: 0.8340
Epoch 18/50
 - 3s - loss: 0.2752 - acc: 0.9201 - val_loss: 0.6046 - val_acc: 0.8272
Epoch 19/50
 - 3s - loss: 0.2715 - acc: 0.9208 - val_loss: 0.6400 - val_acc: 0.8272
Epoch 20/50
 - 3s - loss: 0.2474 - acc: 0.9271 - val_loss: 0.6001 - val_acc: 0.8325
Epoch 21/50
 - 3s - loss: 0.2241 - acc: 0.9336 - val_loss: 0.5851 - val_acc: 0.8460
Epoch 22/50
 - 3s - loss: 0.2157 - acc: 0.9351 - val_loss: 0.6122 - val_acc: 0.8347
Epoch 23/50
 - 3s - loss: 0.1908 - acc: 0.9504 - val_loss: 0.6133 - val_acc: 0.8325
Epoch 24/50
 - 3s - loss: 0.1791 - acc: 0.9499 - val_loss: 0.6047 - val_acc: 0.8385
Epoch 25/50
 - 3s - loss: 0.1703 - acc: 0.9506 - val_loss: 0.6412 - val_acc: 0.8287
Epoch 26/50
 - 3s - loss: 0.1596 - acc: 0.9569 - val_loss: 0.5966 - val_acc: 0.8475
Epoch 00026: early stopping
CNN training time: 87.81168150901794s
Model: "sequential_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_7 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_7 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_7 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 64)                81984     
_________________________________________________________________
dropout_7 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_14 (Dense)             (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9102340185442115, 2.281063034887811, 1.779542514024313, 1.4135936787704333, 1.2011885804487512, 1.0016539697611178, 0.8537777555439985, 0.7449500595717181, 0.6654892927490441, 0.6006445912620348, 0.5264783551120387, 0.4831017941847071, 0.43969740031418747, 0.40809654199104717, 0.3605602382601698, 0.32188027517782847, 0.3045985969852996, 0.275199646524156, 0.27148721718445035, 0.2473795055682485, 0.22410763059632927, 0.21571585319726502, 0.19078172324380635, 0.17913269948630908, 0.17028558695945623, 0.15957936919386437]
[0.18667, 0.36983213, 0.514658, 0.6121273, 0.6607367, 0.71761465, 0.7592082, 0.789777, 0.8150839, 0.8331245, 0.8526685, 0.86820346, 0.8719619, 0.8900025, 0.8982711, 0.90879476, 0.9135555, 0.9200702, 0.92082185, 0.92708594, 0.9336006, 0.935104, 0.9503884, 0.9498873, 0.95063895, 0.95690304]
[2.3845395265352805, 1.7502600403767077, 1.346908350666155, 1.112745261490121, 0.9523331776672763, 0.851388928807214, 0.7727664976844677, 0.735744653401545, 0.6999028108144286, 0.6524784564819736, 0.6395382481866994, 0.6325020469981141, 0.6197594410064307, 0.6204240086324561, 0.6163997774447502, 0.6062621178232319, 0.5868599946672959, 0.6045594831461298, 0.6400278545111844, 0.6000703290409691, 0.5850902080941172, 0.6122435653007994, 0.6133345592807906, 0.6046871842687102, 0.6411578711271811, 0.5965881381475042]
[0.3621337413787842, 0.5221638083457947, 0.656649112701416, 0.7175056338310242, 0.7731029391288757, 0.7918857932090759, 0.8031555414199829, 0.8121712803840637, 0.8174304962158203, 0.8249436616897583, 0.8271976113319397, 0.8256949782371521, 0.8241923451423645, 0.8181818127632141, 0.8256949782371521, 0.8271976113319397, 0.8339594006538391, 0.8271976113319397, 0.8271976113319397, 0.8324568271636963, 0.8459804654121399, 0.8347107172012329, 0.8324568271636963, 0.8384672999382019, 0.8287002444267273, 0.8474830985069275]

  32/2898 [..............................] - ETA: 20s
 544/2898 [====>.........................] - ETA: 1s 
1056/2898 [=========>....................] - ETA: 0s
1568/2898 [===============>..............] - ETA: 0s
2080/2898 [====================>.........] - ETA: 0s
2592/2898 [=========================>....] - ETA: 0s
2898/2898 [==============================] - 1s 178us/step
Accuracies per class for CNN
[0.58 0.91 0.9  0.99 0.88 0.49 0.92 0.82 0.49 0.   0.68 0.22 0.   0.86
 0.73 0.78 0.   0.    nan 0.91 0.86 0.29 0.78]
[2.9102340185442115, 2.281063034887811, 1.779542514024313, 1.4135936787704333, 1.2011885804487512, 1.0016539697611178, 0.8537777555439985, 0.7449500595717181, 0.6654892927490441, 0.6006445912620348, 0.5264783551120387, 0.4831017941847071, 0.43969740031418747, 0.40809654199104717, 0.3605602382601698, 0.32188027517782847, 0.3045985969852996, 0.275199646524156, 0.27148721718445035, 0.2473795055682485, 0.22410763059632927, 0.21571585319726502, 0.19078172324380635, 0.17913269948630908, 0.17028558695945623, 0.15957936919386437]
[0.18667, 0.36983213, 0.514658, 0.6121273, 0.6607367, 0.71761465, 0.7592082, 0.789777, 0.8150839, 0.8331245, 0.8526685, 0.86820346, 0.8719619, 0.8900025, 0.8982711, 0.90879476, 0.9135555, 0.9200702, 0.92082185, 0.92708594, 0.9336006, 0.935104, 0.9503884, 0.9498873, 0.95063895, 0.95690304]
[2.3845395265352805, 1.7502600403767077, 1.346908350666155, 1.112745261490121, 0.9523331776672763, 0.851388928807214, 0.7727664976844677, 0.735744653401545, 0.6999028108144286, 0.6524784564819736, 0.6395382481866994, 0.6325020469981141, 0.6197594410064307, 0.6204240086324561, 0.6163997774447502, 0.6062621178232319, 0.5868599946672959, 0.6045594831461298, 0.6400278545111844, 0.6000703290409691, 0.5850902080941172, 0.6122435653007994, 0.6133345592807906, 0.6046871842687102, 0.6411578711271811, 0.5965881381475042]
[0.3621337413787842, 0.5221638083457947, 0.656649112701416, 0.7175056338310242, 0.7731029391288757, 0.7918857932090759, 0.8031555414199829, 0.8121712803840637, 0.8174304962158203, 0.8249436616897583, 0.8271976113319397, 0.8256949782371521, 0.8241923451423645, 0.8181818127632141, 0.8256949782371521, 0.8271976113319397, 0.8339594006538391, 0.8271976113319397, 0.8271976113319397, 0.8324568271636963, 0.8459804654121399, 0.8347107172012329, 0.8324568271636963, 0.8384672999382019, 0.8287002444267273, 0.8474830985069275]
recall 0.7364
precision 0.7398
f1 0.717
mcc 0.7008
RMSE: 3.658
classification report:
              precision    recall  f1-score   support

           1       0.53      0.58      0.56        72
           2       0.47      0.91      0.62        68
           3       0.92      0.90      0.91       252
           4       0.79      0.99      0.88       132
           5       0.89      0.88      0.89       191
           6       0.95      0.49      0.64       150
           7       0.67      0.92      0.78       240
           9       0.47      0.82      0.60       100
          10       0.60      0.49      0.54        61
          11       0.00      0.00      0.00        87
          12       0.90      0.68      0.78       595
          13       0.50      0.22      0.31        59
          14       0.00      0.00      0.00         6
          15       0.58      0.86      0.69        72
          16       0.85      0.73      0.79        30
          17       0.74      0.78      0.76       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.92      0.91      0.91       222
          22       0.49      0.86      0.62       168
          23       0.83      0.29      0.43        65
          24       0.56      0.78      0.65        82

   micro avg       0.72      0.72      0.72      2898
   macro avg       0.55      0.57      0.54      2898
weighted avg       0.72      0.72      0.70      2898

>#7: 72.119
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 5s - loss: 3.0137 - acc: 0.1318 - val_loss: 2.6497 - val_acc: 0.2352
Epoch 2/50
 - 4s - loss: 2.3959 - acc: 0.3167 - val_loss: 1.9376 - val_acc: 0.5154
Epoch 3/50
 - 3s - loss: 1.8481 - acc: 0.4929 - val_loss: 1.4192 - val_acc: 0.6071
Epoch 4/50
 - 3s - loss: 1.4709 - acc: 0.5936 - val_loss: 1.1390 - val_acc: 0.6732
Epoch 5/50
 - 3s - loss: 1.1947 - acc: 0.6655 - val_loss: 0.9482 - val_acc: 0.7213
Epoch 6/50
 - 3s - loss: 1.0089 - acc: 0.7146 - val_loss: 0.8032 - val_acc: 0.7979
Epoch 7/50
 - 3s - loss: 0.8792 - acc: 0.7494 - val_loss: 0.7143 - val_acc: 0.8212
Epoch 8/50
 - 3s - loss: 0.7333 - acc: 0.7968 - val_loss: 0.6493 - val_acc: 0.8287
Epoch 9/50
 - 3s - loss: 0.6666 - acc: 0.8113 - val_loss: 0.6013 - val_acc: 0.8385
Epoch 10/50
 - 3s - loss: 0.5854 - acc: 0.8339 - val_loss: 0.5874 - val_acc: 0.8325
Epoch 11/50
 - 3s - loss: 0.5422 - acc: 0.8472 - val_loss: 0.5665 - val_acc: 0.8332
Epoch 12/50
 - 3s - loss: 0.4705 - acc: 0.8725 - val_loss: 0.5354 - val_acc: 0.8520
Epoch 13/50
 - 3s - loss: 0.4450 - acc: 0.8700 - val_loss: 0.5573 - val_acc: 0.8272
Epoch 14/50
 - 3s - loss: 0.3985 - acc: 0.8865 - val_loss: 0.5533 - val_acc: 0.8287
Epoch 15/50
 - 3s - loss: 0.3596 - acc: 0.8980 - val_loss: 0.5534 - val_acc: 0.8332
Epoch 16/50
 - 3s - loss: 0.3240 - acc: 0.9053 - val_loss: 0.5514 - val_acc: 0.8287
Epoch 17/50
 - 3s - loss: 0.2948 - acc: 0.9218 - val_loss: 0.5288 - val_acc: 0.8385
Epoch 18/50
 - 3s - loss: 0.2806 - acc: 0.9188 - val_loss: 0.5557 - val_acc: 0.8264
Epoch 19/50
 - 3s - loss: 0.2571 - acc: 0.9298 - val_loss: 0.5499 - val_acc: 0.8377
Epoch 20/50
 - 3s - loss: 0.2286 - acc: 0.9366 - val_loss: 0.5590 - val_acc: 0.8497
Epoch 21/50
 - 3s - loss: 0.2225 - acc: 0.9364 - val_loss: 0.5550 - val_acc: 0.8475
Epoch 22/50
 - 3s - loss: 0.2113 - acc: 0.9394 - val_loss: 0.5890 - val_acc: 0.8287
Epoch 00022: early stopping
CNN training time: 70.93429827690125s
Model: "sequential_8"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_8 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_8 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_8 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_15 (Dense)             (None, 64)                81984     
_________________________________________________________________
dropout_8 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_16 (Dense)             (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[3.0137091809662024, 2.3958716082994096, 1.848140576333965, 1.4708699316152263, 1.194729021970445, 1.0088968017397333, 0.8791766909528193, 0.7333197645320079, 0.6665820537317723, 0.5854255209056896, 0.5421809258522399, 0.4705103384383082, 0.44496999302656354, 0.39851083064671927, 0.3595704966462403, 0.3240226526530618, 0.29481677138259577, 0.28056101954731383, 0.25708580155560196, 0.22858157236947838, 0.22253203793922005, 0.2112974060006476]
[0.13179654, 0.31671262, 0.49285895, 0.59358555, 0.66549736, 0.7146079, 0.74943626, 0.7967928, 0.8113255, 0.8338762, 0.8471561, 0.87246305, 0.8699574, 0.88649464, 0.89802057, 0.9052869, 0.9218241, 0.91881734, 0.9298422, 0.93660736, 0.9363568, 0.93936354]
[2.649682522760487, 1.9375836561489248, 1.419208096739137, 1.1390411504435862, 0.9481682648992825, 0.803162906380478, 0.7143204423629033, 0.6492547947419408, 0.6012658887968911, 0.5873805337598104, 0.566450177918585, 0.5354463051756876, 0.557305976224029, 0.5532862756686959, 0.5533712101808734, 0.5514093452750695, 0.5288428728637894, 0.5557332038578189, 0.5498645076761711, 0.5590275667295941, 0.5550077749641629, 0.5890081736401751]
[0.23516152799129486, 0.5154019594192505, 0.6070623397827148, 0.6731780767440796, 0.7212622165679932, 0.7978963255882263, 0.8211870789527893, 0.8287002444267273, 0.8384672999382019, 0.8324568271636963, 0.8332081437110901, 0.8519909977912903, 0.8271976113319397, 0.8287002444267273, 0.8332081437110901, 0.8287002444267273, 0.8384672999382019, 0.8264462947845459, 0.8377159833908081, 0.8497370481491089, 0.8474830985069275, 0.8287002444267273]

  32/2898 [..............................] - ETA: 18s
 608/2898 [=====>........................] - ETA: 0s 
1152/2898 [==========>...................] - ETA: 0s
1696/2898 [================>.............] - ETA: 0s
2240/2898 [======================>.......] - ETA: 0s
2784/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 0s 165us/step
Accuracies per class for CNN
[0.72 0.97 0.99 0.68 0.84 0.53 0.8  0.96 0.43 0.   0.64 0.36 0.   0.78
 0.7  0.78 0.   0.    nan 0.91 0.83 0.4  0.73]
[3.0137091809662024, 2.3958716082994096, 1.848140576333965, 1.4708699316152263, 1.194729021970445, 1.0088968017397333, 0.8791766909528193, 0.7333197645320079, 0.6665820537317723, 0.5854255209056896, 0.5421809258522399, 0.4705103384383082, 0.44496999302656354, 0.39851083064671927, 0.3595704966462403, 0.3240226526530618, 0.29481677138259577, 0.28056101954731383, 0.25708580155560196, 0.22858157236947838, 0.22253203793922005, 0.2112974060006476]
[0.13179654, 0.31671262, 0.49285895, 0.59358555, 0.66549736, 0.7146079, 0.74943626, 0.7967928, 0.8113255, 0.8338762, 0.8471561, 0.87246305, 0.8699574, 0.88649464, 0.89802057, 0.9052869, 0.9218241, 0.91881734, 0.9298422, 0.93660736, 0.9363568, 0.93936354]
[2.649682522760487, 1.9375836561489248, 1.419208096739137, 1.1390411504435862, 0.9481682648992825, 0.803162906380478, 0.7143204423629033, 0.6492547947419408, 0.6012658887968911, 0.5873805337598104, 0.566450177918585, 0.5354463051756876, 0.557305976224029, 0.5532862756686959, 0.5533712101808734, 0.5514093452750695, 0.5288428728637894, 0.5557332038578189, 0.5498645076761711, 0.5590275667295941, 0.5550077749641629, 0.5890081736401751]
[0.23516152799129486, 0.5154019594192505, 0.6070623397827148, 0.6731780767440796, 0.7212622165679932, 0.7978963255882263, 0.8211870789527893, 0.8287002444267273, 0.8384672999382019, 0.8324568271636963, 0.8332081437110901, 0.8519909977912903, 0.8271976113319397, 0.8287002444267273, 0.8332081437110901, 0.8287002444267273, 0.8384672999382019, 0.8264462947845459, 0.8377159833908081, 0.8497370481491089, 0.8474830985069275, 0.8287002444267273]
recall 0.7412
precision 0.7367
f1 0.7212
mcc 0.6823
RMSE: 3.681
classification report:
              precision    recall  f1-score   support

           1       0.68      0.72      0.70        72
           2       0.47      0.97      0.63        68
           3       0.79      0.99      0.88       252
           4       0.80      0.68      0.73       132
           5       0.85      0.84      0.85       191
           6       0.78      0.53      0.63       150
           7       0.69      0.80      0.74       240
           9       0.48      0.96      0.64       100
          10       0.50      0.43      0.46        61
          11       0.00      0.00      0.00        87
          12       0.91      0.64      0.75       595
          13       0.44      0.36      0.39        59
          14       0.00      0.00      0.00         6
          15       0.77      0.78      0.77        72
          16       0.91      0.70      0.79        30
          17       0.68      0.78      0.73       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.82      0.91      0.86       222
          22       0.52      0.83      0.64       168
          23       0.74      0.40      0.52        65
          24       0.54      0.73      0.62        82

   micro avg       0.70      0.70      0.70      2898
   macro avg       0.54      0.57      0.54      2898
weighted avg       0.70      0.70      0.68      2898

>#8: 70.359
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 3s - loss: 2.9239 - acc: 0.1581 - val_loss: 2.4485 - val_acc: 0.3449
Epoch 2/50
 - 3s - loss: 2.3121 - acc: 0.3433 - val_loss: 1.7519 - val_acc: 0.5319
Epoch 3/50
 - 3s - loss: 1.8157 - acc: 0.4831 - val_loss: 1.3001 - val_acc: 0.6341
Epoch 4/50
 - 3s - loss: 1.4741 - acc: 0.5888 - val_loss: 1.0896 - val_acc: 0.6875
Epoch 5/50
 - 3s - loss: 1.2250 - acc: 0.6480 - val_loss: 0.9404 - val_acc: 0.7513
Epoch 6/50
 - 3s - loss: 1.0439 - acc: 0.7199 - val_loss: 0.8305 - val_acc: 0.7814
Epoch 7/50
 - 3s - loss: 0.8976 - acc: 0.7539 - val_loss: 0.7607 - val_acc: 0.8017
Epoch 8/50
 - 3s - loss: 0.7996 - acc: 0.7845 - val_loss: 0.7194 - val_acc: 0.8129
Epoch 9/50
 - 3s - loss: 0.7028 - acc: 0.8163 - val_loss: 0.6714 - val_acc: 0.8189
Epoch 10/50
 - 3s - loss: 0.6297 - acc: 0.8266 - val_loss: 0.6457 - val_acc: 0.8332
Epoch 11/50
 - 3s - loss: 0.5483 - acc: 0.8494 - val_loss: 0.6370 - val_acc: 0.8167
Epoch 12/50
 - 3s - loss: 0.5007 - acc: 0.8594 - val_loss: 0.6148 - val_acc: 0.8272
Epoch 13/50
 - 3s - loss: 0.4627 - acc: 0.8770 - val_loss: 0.6007 - val_acc: 0.8407
Epoch 14/50
 - 3s - loss: 0.4199 - acc: 0.8860 - val_loss: 0.5902 - val_acc: 0.8392
Epoch 15/50
 - 3s - loss: 0.3892 - acc: 0.8908 - val_loss: 0.6009 - val_acc: 0.8332
Epoch 16/50
 - 3s - loss: 0.3551 - acc: 0.9045 - val_loss: 0.5826 - val_acc: 0.8482
Epoch 17/50
 - 3s - loss: 0.3091 - acc: 0.9128 - val_loss: 0.5927 - val_acc: 0.8340
Epoch 18/50
 - 3s - loss: 0.2818 - acc: 0.9281 - val_loss: 0.5940 - val_acc: 0.8340
Epoch 19/50
 - 3s - loss: 0.2816 - acc: 0.9213 - val_loss: 0.5930 - val_acc: 0.8362
Epoch 20/50
 - 3s - loss: 0.2429 - acc: 0.9306 - val_loss: 0.6101 - val_acc: 0.8310
Epoch 21/50
 - 3s - loss: 0.2319 - acc: 0.9339 - val_loss: 0.6159 - val_acc: 0.8295
Epoch 00021: early stopping
CNN training time: 65.18685936927795s
Model: "sequential_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_9 (Conv1D)            (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_9 (MaxPooling1 (None, 20, 64)            0         
_________________________________________________________________
flatten_9 (Flatten)          (None, 1280)              0         
_________________________________________________________________
dense_17 (Dense)             (None, 64)                81984     
_________________________________________________________________
dropout_9 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_18 (Dense)             (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9239208643163423, 2.3120799943808477, 1.815716312140174, 1.4741118302644867, 1.2249930260324464, 1.0439351542970197, 0.8976132763095576, 0.7996340713894732, 0.7027733977108146, 0.6297436013537293, 0.5483484109865973, 0.5006680363007809, 0.46269830059870587, 0.41992276028131387, 0.38915501527840424, 0.3550587691195973, 0.30908389036048134, 0.2817526747501695, 0.2816071616072544, 0.24286483357977923, 0.23189108070574402]
[0.15810573, 0.34327236, 0.48308694, 0.58882487, 0.6479579, 0.71986973, 0.75394636, 0.78451514, 0.81633675, 0.82660985, 0.8494112, 0.8594337, 0.8769732, 0.8859935, 0.8907542, 0.90453523, 0.9128038, 0.9280882, 0.921323, 0.93059385, 0.9338512]
[2.4485480138496025, 1.7519128881366874, 1.300062242830991, 1.089560990642328, 0.9403910545239003, 0.8305159316648002, 0.7607164501396563, 0.719390664959171, 0.6713608361687581, 0.6456693113675854, 0.6369891577492279, 0.6148028834000479, 0.6007152101132391, 0.5902007477677929, 0.6008508023805916, 0.5825998081535642, 0.5926845995162427, 0.5940426208102717, 0.592981474024942, 0.6100970978135002, 0.615854303221808]
[0.3448534905910492, 0.5319308638572693, 0.6341096758842468, 0.687453031539917, 0.7513148188591003, 0.7813674211502075, 0.8016529083251953, 0.8129225969314575, 0.8189331293106079, 0.8332081437110901, 0.8166791796684265, 0.8271976113319397, 0.8407212495803833, 0.8392186164855957, 0.8332081437110901, 0.8482344150543213, 0.8339594006538391, 0.8339594006538391, 0.8362133502960205, 0.8309541940689087, 0.8294515609741211]

  32/2898 [..............................] - ETA: 21s
 576/2898 [====>.........................] - ETA: 1s 
1152/2898 [==========>...................] - ETA: 0s
1728/2898 [================>.............] - ETA: 0s
2304/2898 [======================>.......] - ETA: 0s
2880/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 0s 170us/step
Accuracies per class for CNN
[0.32 0.99 0.99 0.97 0.85 0.9  0.88 0.87 0.62 0.   0.76 0.34 0.   0.74
 0.6  0.81 0.   0.   0.87 0.89 0.25 0.68]
[2.9239208643163423, 2.3120799943808477, 1.815716312140174, 1.4741118302644867, 1.2249930260324464, 1.0439351542970197, 0.8976132763095576, 0.7996340713894732, 0.7027733977108146, 0.6297436013537293, 0.5483484109865973, 0.5006680363007809, 0.46269830059870587, 0.41992276028131387, 0.38915501527840424, 0.3550587691195973, 0.30908389036048134, 0.2817526747501695, 0.2816071616072544, 0.24286483357977923, 0.23189108070574402]
[0.15810573, 0.34327236, 0.48308694, 0.58882487, 0.6479579, 0.71986973, 0.75394636, 0.78451514, 0.81633675, 0.82660985, 0.8494112, 0.8594337, 0.8769732, 0.8859935, 0.8907542, 0.90453523, 0.9128038, 0.9280882, 0.921323, 0.93059385, 0.9338512]
[2.4485480138496025, 1.7519128881366874, 1.300062242830991, 1.089560990642328, 0.9403910545239003, 0.8305159316648002, 0.7607164501396563, 0.719390664959171, 0.6713608361687581, 0.6456693113675854, 0.6369891577492279, 0.6148028834000479, 0.6007152101132391, 0.5902007477677929, 0.6008508023805916, 0.5825998081535642, 0.5926845995162427, 0.5940426208102717, 0.592981474024942, 0.6100970978135002, 0.615854303221808]
[0.3448534905910492, 0.5319308638572693, 0.6341096758842468, 0.687453031539917, 0.7513148188591003, 0.7813674211502075, 0.8016529083251953, 0.8129225969314575, 0.8189331293106079, 0.8332081437110901, 0.8166791796684265, 0.8271976113319397, 0.8407212495803833, 0.8392186164855957, 0.8332081437110901, 0.8482344150543213, 0.8339594006538391, 0.8339594006538391, 0.8362133502960205, 0.8309541940689087, 0.8294515609741211]
recall 0.7703
precision 0.7765
f1 0.7547
mcc 0.7365
RMSE: 3.730
classification report:
              precision    recall  f1-score   support

           1       0.61      0.32      0.42        72
           2       0.39      0.99      0.56        68
           3       0.93      0.99      0.96       252
           4       0.78      0.97      0.86       132
           5       0.95      0.85      0.90       191
           6       0.94      0.90      0.92       150
           7       0.84      0.88      0.86       240
           9       0.53      0.87      0.66       100
          10       0.61      0.62      0.62        61
          11       0.00      0.00      0.00        87
          12       0.97      0.76      0.85       595
          13       0.57      0.34      0.43        59
          14       0.00      0.00      0.00         6
          15       0.91      0.74      0.82        72
          16       0.90      0.60      0.72        30
          17       0.68      0.81      0.74       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.91      0.87      0.89       222
          22       0.46      0.89      0.61       168
          23       0.84      0.25      0.38        65
          24       0.52      0.68      0.59        82

   micro avg       0.75      0.75      0.75      2898
   macro avg       0.61      0.61      0.58      2898
weighted avg       0.76      0.75      0.74      2898

>#9: 75.431
Train on 3991 samples, validate on 1331 samples
Epoch 1/50
 - 3s - loss: 2.9517 - acc: 0.1616 - val_loss: 2.5699 - val_acc: 0.2600
Epoch 2/50
 - 3s - loss: 2.4974 - acc: 0.2952 - val_loss: 2.0278 - val_acc: 0.4703
Epoch 3/50
 - 3s - loss: 2.0874 - acc: 0.4119 - val_loss: 1.6153 - val_acc: 0.5447
Epoch 4/50
 - 3s - loss: 1.7245 - acc: 0.5132 - val_loss: 1.3041 - val_acc: 0.6386
Epoch 5/50
 - 3s - loss: 1.4187 - acc: 0.6151 - val_loss: 1.0664 - val_acc: 0.6822
Epoch 6/50
 - 3s - loss: 1.1974 - acc: 0.6625 - val_loss: 0.9176 - val_acc: 0.7122
Epoch 7/50
 - 5s - loss: 1.0214 - acc: 0.7101 - val_loss: 0.8020 - val_acc: 0.7693
Epoch 8/50
 - 4s - loss: 0.8706 - acc: 0.7527 - val_loss: 0.7153 - val_acc: 0.7866
Epoch 9/50
 - 3s - loss: 0.7674 - acc: 0.7790 - val_loss: 0.6737 - val_acc: 0.8047
Epoch 10/50
 - 3s - loss: 0.6707 - acc: 0.8031 - val_loss: 0.6279 - val_acc: 0.8325
Epoch 11/50
 - 3s - loss: 0.6146 - acc: 0.8259 - val_loss: 0.6133 - val_acc: 0.8114
Epoch 12/50
 - 3s - loss: 0.5562 - acc: 0.8421 - val_loss: 0.5831 - val_acc: 0.8295
Epoch 13/50
 - 3s - loss: 0.4953 - acc: 0.8649 - val_loss: 0.5659 - val_acc: 0.8310
Epoch 14/50
 - 3s - loss: 0.4463 - acc: 0.8752 - val_loss: 0.5761 - val_acc: 0.8332
Epoch 15/50
 - 3s - loss: 0.4071 - acc: 0.8810 - val_loss: 0.5539 - val_acc: 0.8377
Epoch 16/50
 - 3s - loss: 0.3883 - acc: 0.8877 - val_loss: 0.5305 - val_acc: 0.8467
Epoch 17/50
 - 3s - loss: 0.3466 - acc: 0.8985 - val_loss: 0.5541 - val_acc: 0.8362
Epoch 18/50
 - 3s - loss: 0.3218 - acc: 0.9055 - val_loss: 0.5348 - val_acc: 0.8430
Epoch 19/50
 - 3s - loss: 0.2976 - acc: 0.9163 - val_loss: 0.5446 - val_acc: 0.8437
Epoch 20/50
 - 3s - loss: 0.2725 - acc: 0.9203 - val_loss: 0.5147 - val_acc: 0.8557
Epoch 21/50
 - 3s - loss: 0.2536 - acc: 0.9246 - val_loss: 0.5342 - val_acc: 0.8535
Epoch 22/50
 - 3s - loss: 0.2323 - acc: 0.9336 - val_loss: 0.5383 - val_acc: 0.8497
Epoch 23/50
 - 3s - loss: 0.2197 - acc: 0.9396 - val_loss: 0.5610 - val_acc: 0.8385
Epoch 24/50
 - 3s - loss: 0.2125 - acc: 0.9361 - val_loss: 0.5431 - val_acc: 0.8535
Epoch 25/50
 - 3s - loss: 0.1931 - acc: 0.9419 - val_loss: 0.5378 - val_acc: 0.8550
Epoch 00025: early stopping
CNN training time: 81.589359998703s
Model: "sequential_10"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_10 (Conv1D)           (None, 40, 64)            10688     
_________________________________________________________________
max_pooling1d_10 (MaxPooling (None, 20, 64)            0         
_________________________________________________________________
flatten_10 (Flatten)         (None, 1280)              0         
_________________________________________________________________
dense_19 (Dense)             (None, 64)                81984     
_________________________________________________________________
dropout_10 (Dropout)         (None, 64)                0         
_________________________________________________________________
dense_20 (Dense)             (None, 24)                1560      
=================================================================
Total params: 94,232
Trainable params: 94,232
Non-trainable params: 0
_________________________________________________________________
None
CNN history:
[2.9516872872209463, 2.4974382103359423, 2.0874095899875496, 1.7245028294245794, 1.4186831190958116, 1.1973563294948282, 1.021367264451355, 0.8706085768949956, 0.767373184119385, 0.6707033844199382, 0.6145809240572937, 0.5562393564182614, 0.4952974048153136, 0.44632255637114465, 0.40709925742962366, 0.3882753225376531, 0.3465739109743246, 0.3218469427684218, 0.29755008155554336, 0.27253740563334755, 0.2535583083312752, 0.2322982726055276, 0.21971316590987236, 0.21246266020268778, 0.19309185976863968]
[0.16161363, 0.2951641, 0.41192684, 0.5131546, 0.61513406, 0.6624906, 0.71009773, 0.75269353, 0.7790028, 0.8030569, 0.8258582, 0.84214485, 0.8649461, 0.8752192, 0.8809822, 0.8877474, 0.89852166, 0.9055375, 0.9163117, 0.92032075, 0.9245803, 0.9336006, 0.9396141, 0.93610626, 0.9418692]
[2.569899120293194, 2.0277574531549263, 1.6153480397869016, 1.3040889178834447, 1.0663645563242714, 0.9175631948778228, 0.8019843827807124, 0.7153096913551081, 0.6736978115076598, 0.6278685048608436, 0.6132871268383926, 0.5831001636687285, 0.5658901126699831, 0.5760738573304379, 0.553901737706869, 0.5304851443466544, 0.5540770927193646, 0.5348360894345107, 0.5445715215534123, 0.5147165859754168, 0.5341681297458828, 0.5382967433125985, 0.5610016015433696, 0.543073312256886, 0.5378287687817374]
[0.25995492935180664, 0.4703230559825897, 0.5447032451629639, 0.6386175751686096, 0.6821938157081604, 0.7122464179992676, 0.7693463563919067, 0.7866265773773193, 0.8046581745147705, 0.8324568271636963, 0.8114199638366699, 0.8294515609741211, 0.8309541940689087, 0.8332081437110901, 0.8377159833908081, 0.8467317819595337, 0.8362133502960205, 0.8429751992225647, 0.8437265157699585, 0.8557475805282593, 0.8534936308860779, 0.8497370481491089, 0.8384672999382019, 0.8534936308860779, 0.8549962639808655]

  32/2898 [..............................] - ETA: 24s
 544/2898 [====>.........................] - ETA: 1s 
1120/2898 [==========>...................] - ETA: 0s
1664/2898 [================>.............] - ETA: 0s
2240/2898 [======================>.......] - ETA: 0s
2816/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 186us/step
Accuracies per class for CNN
[0.38 0.91 0.98 0.82 0.86 0.42 0.89 0.96 0.57 0.15 0.53 0.31 0.   0.79
 0.7  0.84 0.08 0.    nan 0.9  0.8  0.38 0.62]
[2.9516872872209463, 2.4974382103359423, 2.0874095899875496, 1.7245028294245794, 1.4186831190958116, 1.1973563294948282, 1.021367264451355, 0.8706085768949956, 0.767373184119385, 0.6707033844199382, 0.6145809240572937, 0.5562393564182614, 0.4952974048153136, 0.44632255637114465, 0.40709925742962366, 0.3882753225376531, 0.3465739109743246, 0.3218469427684218, 0.29755008155554336, 0.27253740563334755, 0.2535583083312752, 0.2322982726055276, 0.21971316590987236, 0.21246266020268778, 0.19309185976863968]
[0.16161363, 0.2951641, 0.41192684, 0.5131546, 0.61513406, 0.6624906, 0.71009773, 0.75269353, 0.7790028, 0.8030569, 0.8258582, 0.84214485, 0.8649461, 0.8752192, 0.8809822, 0.8877474, 0.89852166, 0.9055375, 0.9163117, 0.92032075, 0.9245803, 0.9336006, 0.9396141, 0.93610626, 0.9418692]
[2.569899120293194, 2.0277574531549263, 1.6153480397869016, 1.3040889178834447, 1.0663645563242714, 0.9175631948778228, 0.8019843827807124, 0.7153096913551081, 0.6736978115076598, 0.6278685048608436, 0.6132871268383926, 0.5831001636687285, 0.5658901126699831, 0.5760738573304379, 0.553901737706869, 0.5304851443466544, 0.5540770927193646, 0.5348360894345107, 0.5445715215534123, 0.5147165859754168, 0.5341681297458828, 0.5382967433125985, 0.5610016015433696, 0.543073312256886, 0.5378287687817374]
[0.25995492935180664, 0.4703230559825897, 0.5447032451629639, 0.6386175751686096, 0.6821938157081604, 0.7122464179992676, 0.7693463563919067, 0.7866265773773193, 0.8046581745147705, 0.8324568271636963, 0.8114199638366699, 0.8294515609741211, 0.8309541940689087, 0.8332081437110901, 0.8377159833908081, 0.8467317819595337, 0.8362133502960205, 0.8429751992225647, 0.8437265157699585, 0.8557475805282593, 0.8534936308860779, 0.8497370481491089, 0.8384672999382019, 0.8534936308860779, 0.8549962639808655]
recall 0.6991
precision 0.7191
f1 0.685
mcc 0.6634
RMSE: 3.811
classification report:
              precision    recall  f1-score   support

           1       0.43      0.38      0.40        72
           2       0.39      0.91      0.55        68
           3       0.88      0.98      0.93       252
           4       0.86      0.82      0.84       132
           5       0.95      0.86      0.90       191
           6       0.81      0.42      0.55       150
           7       0.55      0.89      0.68       240
           9       0.56      0.96      0.71       100
          10       0.67      0.57      0.62        61
          11       0.35      0.15      0.21        87
          12       0.83      0.53      0.65       595
          13       0.51      0.31      0.38        59
          14       0.00      0.00      0.00         6
          15       0.56      0.79      0.66        72
          16       0.88      0.70      0.78        30
          17       0.69      0.84      0.76       156
          18       0.20      0.08      0.12        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.94      0.90      0.92       222
          22       0.51      0.80      0.62       168
          23       0.66      0.38      0.49        65
          24       0.52      0.62      0.57        82

   micro avg       0.68      0.68      0.68      2898
   macro avg       0.55      0.56      0.54      2898
weighted avg       0.70      0.68      0.67      2898

>#10: 68.461
Output from summarize_results:
Accuracy: 0.694% (+/-0.029)
Loss: 1.176% (+/-0.084)
Recall: 0.710% (+/-0.032)
Precision: 0.726% (+/-0.024)
F1: 0.689% (+/-0.033)
MCC: 0.676% (+/-0.029)
RMSE: 3.761% (+/-0.096)
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1593 - acc: 0.0329 - val_loss: 3.0799 - val_acc: 0.0892
Epoch 2/50
 - 12s - loss: 3.0783 - acc: 0.0662 - val_loss: 2.9758 - val_acc: 0.1474
Epoch 3/50
 - 13s - loss: 2.9577 - acc: 0.0933 - val_loss: 2.8327 - val_acc: 0.1737
Epoch 4/50
 - 15s - loss: 2.7922 - acc: 0.1626 - val_loss: 2.6652 - val_acc: 0.3042
Epoch 5/50
 - 12s - loss: 2.6145 - acc: 0.2990 - val_loss: 2.4987 - val_acc: 0.3408
Epoch 6/50
 - 12s - loss: 2.4411 - acc: 0.3956 - val_loss: 2.3626 - val_acc: 0.4066
Epoch 7/50
 - 13s - loss: 2.2842 - acc: 0.4534 - val_loss: 2.2464 - val_acc: 0.4432
Epoch 8/50
 - 14s - loss: 2.1467 - acc: 0.4834 - val_loss: 2.1457 - val_acc: 0.4817
Epoch 9/50
 - 13s - loss: 2.0307 - acc: 0.4966 - val_loss: 2.0715 - val_acc: 0.4864
Epoch 10/50
 - 12s - loss: 1.9313 - acc: 0.5147 - val_loss: 1.9981 - val_acc: 0.4911
Epoch 11/50
 - 12s - loss: 1.8482 - acc: 0.5271 - val_loss: 1.9404 - val_acc: 0.5005
Epoch 12/50
 - 12s - loss: 1.7708 - acc: 0.5316 - val_loss: 1.8915 - val_acc: 0.5136
Epoch 13/50
 - 12s - loss: 1.7030 - acc: 0.5379 - val_loss: 1.8382 - val_acc: 0.5146
Epoch 14/50
 - 12s - loss: 1.6384 - acc: 0.5457 - val_loss: 1.7915 - val_acc: 0.5183
Epoch 15/50
 - 12s - loss: 1.5785 - acc: 0.5499 - val_loss: 1.7527 - val_acc: 0.5305
Epoch 16/50
 - 12s - loss: 1.5268 - acc: 0.5537 - val_loss: 1.7139 - val_acc: 0.5362
Epoch 17/50
 - 12s - loss: 1.4783 - acc: 0.5558 - val_loss: 1.6756 - val_acc: 0.5371
Epoch 18/50
 - 12s - loss: 1.4294 - acc: 0.5657 - val_loss: 1.6432 - val_acc: 0.5465
Epoch 19/50
 - 12s - loss: 1.3907 - acc: 0.5725 - val_loss: 1.6268 - val_acc: 0.5577
Epoch 20/50
 - 12s - loss: 1.3480 - acc: 0.5840 - val_loss: 1.5979 - val_acc: 0.5624
Epoch 21/50
 - 12s - loss: 1.3061 - acc: 0.5917 - val_loss: 1.5558 - val_acc: 0.5718
Epoch 22/50
 - 12s - loss: 1.2668 - acc: 0.6103 - val_loss: 1.5394 - val_acc: 0.5822
Epoch 23/50
 - 13s - loss: 1.2312 - acc: 0.6310 - val_loss: 1.5050 - val_acc: 0.6056
Epoch 24/50
 - 14s - loss: 1.2044 - acc: 0.6547 - val_loss: 1.4864 - val_acc: 0.5972
Epoch 25/50
 - 13s - loss: 1.1605 - acc: 0.6739 - val_loss: 1.4593 - val_acc: 0.6000
Epoch 26/50
 - 14s - loss: 1.1289 - acc: 0.6899 - val_loss: 1.4351 - val_acc: 0.6094
Epoch 27/50
 - 12s - loss: 1.1002 - acc: 0.6998 - val_loss: 1.4197 - val_acc: 0.6066
Epoch 28/50
 - 13s - loss: 1.0669 - acc: 0.7113 - val_loss: 1.3996 - val_acc: 0.6141
Epoch 29/50
 - 13s - loss: 1.0357 - acc: 0.7263 - val_loss: 1.3782 - val_acc: 0.6225
Epoch 30/50
 - 13s - loss: 1.0247 - acc: 0.7327 - val_loss: 1.3545 - val_acc: 0.6254
Epoch 31/50
 - 13s - loss: 0.9822 - acc: 0.7437 - val_loss: 1.3356 - val_acc: 0.6263
Epoch 32/50
 - 13s - loss: 0.9534 - acc: 0.7512 - val_loss: 1.3251 - val_acc: 0.6244
Epoch 33/50
 - 13s - loss: 0.9269 - acc: 0.7595 - val_loss: 1.2984 - val_acc: 0.6319
Epoch 34/50
 - 13s - loss: 0.9014 - acc: 0.7630 - val_loss: 1.2795 - val_acc: 0.6329
Epoch 35/50
 - 13s - loss: 0.8778 - acc: 0.7665 - val_loss: 1.2662 - val_acc: 0.6347
Epoch 36/50
 - 13s - loss: 0.8588 - acc: 0.7707 - val_loss: 1.2343 - val_acc: 0.6366
Epoch 37/50
 - 13s - loss: 0.8302 - acc: 0.7804 - val_loss: 1.2093 - val_acc: 0.6441
Epoch 38/50
 - 12s - loss: 0.8089 - acc: 0.7848 - val_loss: 1.1827 - val_acc: 0.6469
Epoch 39/50
 - 12s - loss: 0.7881 - acc: 0.7888 - val_loss: 1.1572 - val_acc: 0.6479
Epoch 40/50
 - 12s - loss: 0.7681 - acc: 0.7912 - val_loss: 1.1319 - val_acc: 0.6535
Epoch 41/50
 - 12s - loss: 0.7486 - acc: 0.7947 - val_loss: 1.1094 - val_acc: 0.6563
Epoch 42/50
 - 12s - loss: 0.7303 - acc: 0.7975 - val_loss: 1.0886 - val_acc: 0.6582
Epoch 43/50
 - 13s - loss: 0.7122 - acc: 0.8008 - val_loss: 1.0701 - val_acc: 0.6592
Epoch 44/50
 - 12s - loss: 0.6945 - acc: 0.8046 - val_loss: 1.0494 - val_acc: 0.6667
Epoch 45/50
 - 12s - loss: 0.6780 - acc: 0.8055 - val_loss: 1.0365 - val_acc: 0.6751
Epoch 46/50
 - 12s - loss: 0.6719 - acc: 0.8083 - val_loss: 1.0344 - val_acc: 0.6751
Epoch 47/50
 - 12s - loss: 0.6494 - acc: 0.8104 - val_loss: 1.0537 - val_acc: 0.6742
Epoch 48/50
 - 12s - loss: 0.6428 - acc: 0.8118 - val_loss: 1.0360 - val_acc: 0.6911
Epoch 49/50
 - 12s - loss: 0.6188 - acc: 0.8184 - val_loss: 1.0036 - val_acc: 0.7005
Epoch 50/50
 - 12s - loss: 0.6049 - acc: 0.8198 - val_loss: 1.0032 - val_acc: 0.7023
LSTM training time: 637.4659833908081s
Model: "sequential_11"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_21 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_22 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.15928381167585, 3.0782748649790728, 2.9576512197947484, 2.7922449467578123, 2.614539342109155, 2.4411169514287545, 2.284150353557828, 2.1467114873746764, 2.0306888927112925, 1.931303615997956, 1.8482056846354302, 1.7708332949970595, 1.7029548663053542, 1.6384300916217878, 1.578548241871356, 1.5267850791789122, 1.4783263961446098, 1.4293908787244134, 1.3906832255750357, 1.348027690492495, 1.3060724858473747, 1.2668404585883346, 1.2312472553104197, 1.2044257667665614, 1.1604772234623886, 1.1288975276506252, 1.1001974934979395, 1.0669058902473105, 1.0357331785554753, 1.0247025506538994, 0.9821998250366853, 0.9534034778289401, 0.9269224942073537, 0.9013946058046154, 0.8777901774348018, 0.8588209017071613, 0.8302481172263748, 0.8088610645419475, 0.7880624150842015, 0.7681437468679965, 0.7486469083607182, 0.7302676436379955, 0.7121525171191193, 0.6944506058190696, 0.6780112798865872, 0.6719265548323756, 0.6494385960367691, 0.6428496805517389, 0.6188439834239003, 0.6048620240629378]
[0.032887008, 0.066243835, 0.093258165, 0.16255578, 0.2990369, 0.39558375, 0.45337093, 0.48343903, 0.49659383, 0.5146817, 0.5271318, 0.531595, 0.5379375, 0.54568946, 0.54991776, 0.5536763, 0.5557905, 0.56565654, 0.5724689, 0.5839793, 0.59173125, 0.6102889, 0.63096076, 0.6546864, 0.67394876, 0.68992245, 0.6997886, 0.71129906, 0.7263331, 0.7326756, 0.74371624, 0.7512333, 0.759455, 0.7629786, 0.76650226, 0.77073056, 0.7803618, 0.78482497, 0.7888184, 0.7911675, 0.7946911, 0.79750997, 0.8007987, 0.8045572, 0.8054968, 0.8083157, 0.8104299, 0.81183934, 0.8184167, 0.8198262]
[3.079906433848708, 2.975785138573445, 2.8327210567366907, 2.665192874048797, 2.498701770540694, 2.3626030299585192, 2.2464105337438447, 2.145719021362878, 2.071540167410049, 1.998091806044601, 1.9403860927187782, 1.8915490367602854, 1.8382212018742807, 1.7914936199994154, 1.7526762439611372, 1.7138669105762607, 1.6756100530355749, 1.6432378912195913, 1.6268426273910093, 1.5979070178779637, 1.5558285943219359, 1.5394234606917476, 1.5049631072321967, 1.486379253360587, 1.459271734701076, 1.4351484554474343, 1.4196510085477516, 1.399606200898757, 1.37816885063346, 1.3544830119385967, 1.3355642951989957, 1.3251496802753127, 1.2983609086331067, 1.2795133337868212, 1.2661750728395624, 1.234342774762794, 1.209312686911771, 1.1827444909464027, 1.1572087540592946, 1.1319212909315672, 1.1094458704543226, 1.088646289160554, 1.0701114290896716, 1.049433646487518, 1.0364885157262775, 1.0343711149664552, 1.053721513616647, 1.036048026389919, 1.0035567460695343, 1.003229149763293]
[0.08920187503099442, 0.1474178433418274, 0.17370891571044922, 0.304225355386734, 0.3408450782299042, 0.40657275915145874, 0.4431924819946289, 0.4816901385784149, 0.4863849878311157, 0.49107980728149414, 0.5004695057868958, 0.5136150121688843, 0.514553964138031, 0.5183098316192627, 0.5305164456367493, 0.5361502170562744, 0.5370892286300659, 0.5464788675308228, 0.5577464699745178, 0.5624412894248962, 0.5718309879302979, 0.5821596384048462, 0.6056337952613831, 0.597183108329773, 0.6000000238418579, 0.6093896627426147, 0.6065727472305298, 0.6140844821929932, 0.622535228729248, 0.6253520846366882, 0.6262910962104797, 0.6244131326675415, 0.6319248676300049, 0.6328638792037964, 0.6347417831420898, 0.6366197466850281, 0.6441314816474915, 0.6469483375549316, 0.6478873491287231, 0.6535211205482483, 0.6563380360603333, 0.6582159399986267, 0.6591549515724182, 0.6666666865348816, 0.6751173734664917, 0.6751173734664917, 0.674178421497345, 0.6910797953605652, 0.7004694938659668, 0.7023473978042603]

  32/2898 [..............................] - ETA: 30s
 416/2898 [===>..........................] - ETA: 2s 
 800/2898 [=======>......................] - ETA: 1s
1184/2898 [===========>..................] - ETA: 0s
1536/2898 [==============>...............] - ETA: 0s
1920/2898 [==================>...........] - ETA: 0s
2304/2898 [======================>.......] - ETA: 0s
2688/2898 [==========================>...] - ETA: 0s
2898/2898 [==============================] - 1s 258us/step
Accuracies per class for LSTM
[0.06 0.72 0.89 0.11 0.66 0.08 0.72 0.72 0.89 0.   0.6  0.2  0.   0.57
 0.1  0.72 0.   0.   0.41 0.79 0.   0.87]
[3.15928381167585, 3.0782748649790728, 2.9576512197947484, 2.7922449467578123, 2.614539342109155, 2.4411169514287545, 2.284150353557828, 2.1467114873746764, 2.0306888927112925, 1.931303615997956, 1.8482056846354302, 1.7708332949970595, 1.7029548663053542, 1.6384300916217878, 1.578548241871356, 1.5267850791789122, 1.4783263961446098, 1.4293908787244134, 1.3906832255750357, 1.348027690492495, 1.3060724858473747, 1.2668404585883346, 1.2312472553104197, 1.2044257667665614, 1.1604772234623886, 1.1288975276506252, 1.1001974934979395, 1.0669058902473105, 1.0357331785554753, 1.0247025506538994, 0.9821998250366853, 0.9534034778289401, 0.9269224942073537, 0.9013946058046154, 0.8777901774348018, 0.8588209017071613, 0.8302481172263748, 0.8088610645419475, 0.7880624150842015, 0.7681437468679965, 0.7486469083607182, 0.7302676436379955, 0.7121525171191193, 0.6944506058190696, 0.6780112798865872, 0.6719265548323756, 0.6494385960367691, 0.6428496805517389, 0.6188439834239003, 0.6048620240629378]
[0.032887008, 0.066243835, 0.093258165, 0.16255578, 0.2990369, 0.39558375, 0.45337093, 0.48343903, 0.49659383, 0.5146817, 0.5271318, 0.531595, 0.5379375, 0.54568946, 0.54991776, 0.5536763, 0.5557905, 0.56565654, 0.5724689, 0.5839793, 0.59173125, 0.6102889, 0.63096076, 0.6546864, 0.67394876, 0.68992245, 0.6997886, 0.71129906, 0.7263331, 0.7326756, 0.74371624, 0.7512333, 0.759455, 0.7629786, 0.76650226, 0.77073056, 0.7803618, 0.78482497, 0.7888184, 0.7911675, 0.7946911, 0.79750997, 0.8007987, 0.8045572, 0.8054968, 0.8083157, 0.8104299, 0.81183934, 0.8184167, 0.8198262]
[3.079906433848708, 2.975785138573445, 2.8327210567366907, 2.665192874048797, 2.498701770540694, 2.3626030299585192, 2.2464105337438447, 2.145719021362878, 2.071540167410049, 1.998091806044601, 1.9403860927187782, 1.8915490367602854, 1.8382212018742807, 1.7914936199994154, 1.7526762439611372, 1.7138669105762607, 1.6756100530355749, 1.6432378912195913, 1.6268426273910093, 1.5979070178779637, 1.5558285943219359, 1.5394234606917476, 1.5049631072321967, 1.486379253360587, 1.459271734701076, 1.4351484554474343, 1.4196510085477516, 1.399606200898757, 1.37816885063346, 1.3544830119385967, 1.3355642951989957, 1.3251496802753127, 1.2983609086331067, 1.2795133337868212, 1.2661750728395624, 1.234342774762794, 1.209312686911771, 1.1827444909464027, 1.1572087540592946, 1.1319212909315672, 1.1094458704543226, 1.088646289160554, 1.0701114290896716, 1.049433646487518, 1.0364885157262775, 1.0343711149664552, 1.053721513616647, 1.036048026389919, 1.0035567460695343, 1.003229149763293]
[0.08920187503099442, 0.1474178433418274, 0.17370891571044922, 0.304225355386734, 0.3408450782299042, 0.40657275915145874, 0.4431924819946289, 0.4816901385784149, 0.4863849878311157, 0.49107980728149414, 0.5004695057868958, 0.5136150121688843, 0.514553964138031, 0.5183098316192627, 0.5305164456367493, 0.5361502170562744, 0.5370892286300659, 0.5464788675308228, 0.5577464699745178, 0.5624412894248962, 0.5718309879302979, 0.5821596384048462, 0.6056337952613831, 0.597183108329773, 0.6000000238418579, 0.6093896627426147, 0.6065727472305298, 0.6140844821929932, 0.622535228729248, 0.6253520846366882, 0.6262910962104797, 0.6244131326675415, 0.6319248676300049, 0.6328638792037964, 0.6347417831420898, 0.6366197466850281, 0.6441314816474915, 0.6469483375549316, 0.6478873491287231, 0.6535211205482483, 0.6563380360603333, 0.6582159399986267, 0.6591549515724182, 0.6666666865348816, 0.6751173734664917, 0.6751173734664917, 0.674178421497345, 0.6910797953605652, 0.7004694938659668, 0.7023473978042603]
recall 0.5849
precision 0.549
f1 0.5371
mcc 0.4966
RMSE: 5.405
classification report:
              precision    recall  f1-score   support

           1       0.11      0.06      0.07        72
           2       0.43      0.72      0.54        68
           3       0.59      0.89      0.71       252
           4       0.20      0.11      0.14       132
           5       0.77      0.66      0.72       191
           6       0.46      0.08      0.14       150
           7       0.46      0.72      0.56       240
           9       0.35      0.72      0.47       100
          10       0.40      0.89      0.55        61
          11       0.00      0.00      0.00        87
          12       0.71      0.60      0.65       595
          13       0.21      0.20      0.21        59
          14       0.00      0.00      0.00         6
          15       0.32      0.57      0.41        72
          16       1.00      0.10      0.18        30
          17       0.56      0.72      0.63       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.60      0.41      0.49       222
          22       0.59      0.79      0.68       168
          23       0.00      0.00      0.00        65
          24       0.59      0.87      0.70        82

   micro avg       0.53      0.53      0.53      2898
   macro avg       0.38      0.41      0.36      2898
weighted avg       0.50      0.53      0.49      2898

>#1: 53.485
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1438 - acc: 0.1031 - val_loss: 3.0886 - val_acc: 0.1427
Epoch 2/50
 - 13s - loss: 3.0132 - acc: 0.1698 - val_loss: 2.9079 - val_acc: 0.1408
Epoch 3/50
 - 13s - loss: 2.8097 - acc: 0.2194 - val_loss: 2.7310 - val_acc: 0.2883
Epoch 4/50
 - 12s - loss: 2.6198 - acc: 0.3235 - val_loss: 2.5911 - val_acc: 0.3239
Epoch 5/50
 - 12s - loss: 2.4653 - acc: 0.3542 - val_loss: 2.4927 - val_acc: 0.3136
Epoch 6/50
 - 12s - loss: 2.3361 - acc: 0.3514 - val_loss: 2.4133 - val_acc: 0.3108
Epoch 7/50
 - 12s - loss: 2.2294 - acc: 0.3646 - val_loss: 2.3478 - val_acc: 0.3164
Epoch 8/50
 - 12s - loss: 2.1384 - acc: 0.3740 - val_loss: 2.3024 - val_acc: 0.3211
Epoch 9/50
 - 13s - loss: 2.0552 - acc: 0.3928 - val_loss: 2.2593 - val_acc: 0.3324
Epoch 10/50
 - 13s - loss: 1.9864 - acc: 0.4099 - val_loss: 2.2249 - val_acc: 0.3399
Epoch 11/50
 - 12s - loss: 1.9206 - acc: 0.4343 - val_loss: 2.2043 - val_acc: 0.3399
Epoch 12/50
 - 12s - loss: 1.8595 - acc: 0.4578 - val_loss: 2.1891 - val_acc: 0.3305
Epoch 13/50
 - 12s - loss: 1.8091 - acc: 0.4675 - val_loss: 2.1608 - val_acc: 0.3390
Epoch 14/50
 - 12s - loss: 1.7522 - acc: 0.4853 - val_loss: 2.1531 - val_acc: 0.3371
Epoch 15/50
 - 14s - loss: 1.6916 - acc: 0.4999 - val_loss: 2.1527 - val_acc: 0.3146
Epoch 16/50
 - 12s - loss: 1.6458 - acc: 0.5128 - val_loss: 2.1608 - val_acc: 0.3042
Epoch 17/50
 - 13s - loss: 1.5934 - acc: 0.5304 - val_loss: 2.1555 - val_acc: 0.2995
Epoch 18/50
 - 12s - loss: 1.5478 - acc: 0.5422 - val_loss: 2.1274 - val_acc: 0.3033
Epoch 19/50
 - 12s - loss: 1.5064 - acc: 0.5485 - val_loss: 2.1202 - val_acc: 0.2911
Epoch 20/50
 - 13s - loss: 1.4657 - acc: 0.5520 - val_loss: 2.0569 - val_acc: 0.3108
Epoch 21/50
 - 15s - loss: 1.4324 - acc: 0.5556 - val_loss: 2.1451 - val_acc: 0.2685
Epoch 22/50
 - 14s - loss: 1.3927 - acc: 0.5605 - val_loss: 2.1388 - val_acc: 0.2629
Epoch 23/50
 - 16s - loss: 1.3602 - acc: 0.5732 - val_loss: 2.1402 - val_acc: 0.2629
Epoch 24/50
 - 13s - loss: 1.3320 - acc: 0.5807 - val_loss: 2.1427 - val_acc: 0.2601
Epoch 25/50
 - 12s - loss: 1.3009 - acc: 0.5929 - val_loss: 2.1459 - val_acc: 0.2563
Epoch 00025: early stopping
LSTM training time: 324.3383665084839s
Model: "sequential_12"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_2 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_23 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_24 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1437595231672506, 3.0132206851169765, 2.809738186398722, 2.61982539133032, 2.4652995572057663, 2.336109123628745, 2.2293799057907417, 2.138426612065884, 2.0551936709883627, 1.9864417795343021, 1.9205956528213122, 1.8594739462586112, 1.8091280550247688, 1.7522174197517988, 1.6916285459077327, 1.6458348436397034, 1.5934226292524367, 1.5478318507105133, 1.506375860084187, 1.4657399749178994, 1.4324158712791002, 1.3926567912787822, 1.360162495684674, 1.331958141745831, 1.3009365376435957]
[0.10312427, 0.1698379, 0.21940334, 0.32346722, 0.3542401, 0.35142118, 0.36457598, 0.37397227, 0.39276487, 0.4099131, 0.43434343, 0.45783415, 0.46746534, 0.4853183, 0.49988255, 0.5128024, 0.5304205, 0.5421658, 0.54850835, 0.55203193, 0.5555556, 0.5604886, 0.5731736, 0.5806906, 0.5929058]
[3.088601950972293, 2.9078516608672524, 2.7309658650501234, 2.591085088308988, 2.4926619413313174, 2.413299258326141, 2.3478037677460435, 2.3023731952541873, 2.259339375786938, 2.224861875722106, 2.204312381386197, 2.1890946435256744, 2.1608105460243046, 2.1531037899250154, 2.152656601516294, 2.160815843617972, 2.15554330427322, 2.1274333362847986, 2.1202255352002353, 2.0568901734732687, 2.1451063251271494, 2.138806895470955, 2.1401821949112585, 2.142691860624322, 2.1458781865840786]
[0.14272300899028778, 0.14084507524967194, 0.28826290369033813, 0.3239436745643616, 0.31361502408981323, 0.31079810857772827, 0.3164319396018982, 0.3211267590522766, 0.3323943614959717, 0.33990609645843506, 0.33990609645843506, 0.33051642775535583, 0.33896714448928833, 0.3370892107486725, 0.31455397605895996, 0.304225355386734, 0.2995305061340332, 0.3032863736152649, 0.2910798192024231, 0.31079810857772827, 0.26854461431503296, 0.26291078329086304, 0.26291078329086304, 0.26009389758110046, 0.2563380300998688]

  32/2898 [..............................] - ETA: 42s
 384/2898 [==>...........................] - ETA: 3s 
 736/2898 [======>.......................] - ETA: 1s
1120/2898 [==========>...................] - ETA: 1s
1504/2898 [==============>...............] - ETA: 0s
1888/2898 [==================>...........] - ETA: 0s
2272/2898 [======================>.......] - ETA: 0s
2656/2898 [==========================>...] - ETA: 0s
2898/2898 [==============================] - 1s 303us/step
Accuracies per class for LSTM
[0.22 0.22 0.15 0.82 0.59 0.17 0.25 0.91 0.2  0.   0.   0.   0.   0.03
 0.   0.23 0.   0.   0.12 0.95 0.   0.  ]
[3.1437595231672506, 3.0132206851169765, 2.809738186398722, 2.61982539133032, 2.4652995572057663, 2.336109123628745, 2.2293799057907417, 2.138426612065884, 2.0551936709883627, 1.9864417795343021, 1.9205956528213122, 1.8594739462586112, 1.8091280550247688, 1.7522174197517988, 1.6916285459077327, 1.6458348436397034, 1.5934226292524367, 1.5478318507105133, 1.506375860084187, 1.4657399749178994, 1.4324158712791002, 1.3926567912787822, 1.360162495684674, 1.331958141745831, 1.3009365376435957]
[0.10312427, 0.1698379, 0.21940334, 0.32346722, 0.3542401, 0.35142118, 0.36457598, 0.37397227, 0.39276487, 0.4099131, 0.43434343, 0.45783415, 0.46746534, 0.4853183, 0.49988255, 0.5128024, 0.5304205, 0.5421658, 0.54850835, 0.55203193, 0.5555556, 0.5604886, 0.5731736, 0.5806906, 0.5929058]
[3.088601950972293, 2.9078516608672524, 2.7309658650501234, 2.591085088308988, 2.4926619413313174, 2.413299258326141, 2.3478037677460435, 2.3023731952541873, 2.259339375786938, 2.224861875722106, 2.204312381386197, 2.1890946435256744, 2.1608105460243046, 2.1531037899250154, 2.152656601516294, 2.160815843617972, 2.15554330427322, 2.1274333362847986, 2.1202255352002353, 2.0568901734732687, 2.1451063251271494, 2.138806895470955, 2.1401821949112585, 2.142691860624322, 2.1458781865840786]
[0.14272300899028778, 0.14084507524967194, 0.28826290369033813, 0.3239436745643616, 0.31361502408981323, 0.31079810857772827, 0.3164319396018982, 0.3211267590522766, 0.3323943614959717, 0.33990609645843506, 0.33990609645843506, 0.33051642775535583, 0.33896714448928833, 0.3370892107486725, 0.31455397605895996, 0.304225355386734, 0.2995305061340332, 0.3032863736152649, 0.2910798192024231, 0.31079810857772827, 0.26854461431503296, 0.26291078329086304, 0.26291078329086304, 0.26009389758110046, 0.2563380300998688]
recall 0.2795
precision 0.2957
f1 0.2082
mcc 0.2136
RMSE: 6.126
classification report:
              precision    recall  f1-score   support

           1       0.53      0.22      0.31        72
           2       0.34      0.22      0.27        68
           3       0.40      0.15      0.22       252
           4       0.30      0.82      0.44       132
           5       0.32      0.59      0.41       191
           6       0.09      0.17      0.12       150
           7       0.51      0.25      0.34       240
           9       0.13      0.91      0.23       100
          10       0.60      0.20      0.30        61
          11       0.00      0.00      0.00        87
          12       0.20      0.00      0.01       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.67      0.03      0.05        72
          16       0.00      0.00      0.00        30
          17       0.34      0.23      0.27       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.13      0.12      0.12       222
          22       0.28      0.95      0.43       168
          23       0.00      0.00      0.00        65
          24       0.00      0.00      0.00        82

   micro avg       0.24      0.24      0.24      2898
   macro avg       0.22      0.22      0.16      2898
weighted avg       0.26      0.24      0.18      2898

>#2: 24.258
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 14s - loss: 3.1665 - acc: 0.0531 - val_loss: 3.1319 - val_acc: 0.1202
Epoch 2/50
 - 13s - loss: 3.0825 - acc: 0.0864 - val_loss: 3.0259 - val_acc: 0.1296
Epoch 3/50
 - 12s - loss: 2.9634 - acc: 0.0872 - val_loss: 2.9135 - val_acc: 0.1249
Epoch 4/50
 - 12s - loss: 2.8316 - acc: 0.0904 - val_loss: 2.7985 - val_acc: 0.1258
Epoch 5/50
 - 13s - loss: 2.6936 - acc: 0.1231 - val_loss: 2.7149 - val_acc: 0.1315
Epoch 6/50
 - 13s - loss: 2.5744 - acc: 0.2128 - val_loss: 2.6568 - val_acc: 0.1803
Epoch 7/50
 - 13s - loss: 2.4646 - acc: 0.3284 - val_loss: 2.5916 - val_acc: 0.1944
Epoch 8/50
 - 13s - loss: 2.3610 - acc: 0.3524 - val_loss: 2.5264 - val_acc: 0.2178
Epoch 9/50
 - 12s - loss: 2.2667 - acc: 0.3744 - val_loss: 2.4621 - val_acc: 0.2629
Epoch 10/50
 - 12s - loss: 2.1804 - acc: 0.3892 - val_loss: 2.4089 - val_acc: 0.2770
Epoch 11/50
 - 12s - loss: 2.1050 - acc: 0.3961 - val_loss: 2.3688 - val_acc: 0.2920
Epoch 12/50
 - 12s - loss: 2.0366 - acc: 0.4167 - val_loss: 2.3336 - val_acc: 0.2977
Epoch 13/50
 - 12s - loss: 1.9737 - acc: 0.4350 - val_loss: 2.3012 - val_acc: 0.3005
Epoch 14/50
 - 12s - loss: 1.9152 - acc: 0.4583 - val_loss: 2.2680 - val_acc: 0.3089
Epoch 15/50
 - 12s - loss: 1.8594 - acc: 0.4823 - val_loss: 2.2384 - val_acc: 0.3174
Epoch 16/50
 - 12s - loss: 1.8069 - acc: 0.5008 - val_loss: 2.2142 - val_acc: 0.3324
Epoch 17/50
 - 12s - loss: 1.7559 - acc: 0.5163 - val_loss: 2.1875 - val_acc: 0.3455
Epoch 18/50
 - 12s - loss: 1.7060 - acc: 0.5323 - val_loss: 2.1584 - val_acc: 0.3596
Epoch 19/50
 - 12s - loss: 1.6545 - acc: 0.5459 - val_loss: 2.1342 - val_acc: 0.3653
Epoch 20/50
 - 12s - loss: 1.6059 - acc: 0.5621 - val_loss: 2.1100 - val_acc: 0.3662
Epoch 21/50
 - 12s - loss: 1.5590 - acc: 0.5736 - val_loss: 2.0854 - val_acc: 0.3728
Epoch 22/50
 - 12s - loss: 1.5131 - acc: 0.5854 - val_loss: 2.0646 - val_acc: 0.3840
Epoch 23/50
 - 14s - loss: 1.4692 - acc: 0.5992 - val_loss: 2.0431 - val_acc: 0.3878
Epoch 24/50
 - 14s - loss: 1.4235 - acc: 0.6068 - val_loss: 2.0232 - val_acc: 0.3925
Epoch 25/50
 - 12s - loss: 1.3800 - acc: 0.6218 - val_loss: 2.0087 - val_acc: 0.4019
Epoch 26/50
 - 12s - loss: 1.3392 - acc: 0.6314 - val_loss: 2.0082 - val_acc: 0.3953
Epoch 27/50
 - 16s - loss: 1.3020 - acc: 0.6415 - val_loss: 1.9945 - val_acc: 0.4000
Epoch 28/50
 - 13s - loss: 1.2657 - acc: 0.6488 - val_loss: 1.9809 - val_acc: 0.4000
Epoch 29/50
 - 14s - loss: 1.2305 - acc: 0.6554 - val_loss: 1.9672 - val_acc: 0.4150
Epoch 30/50
 - 13s - loss: 1.1976 - acc: 0.6646 - val_loss: 1.9531 - val_acc: 0.4225
Epoch 31/50
 - 12s - loss: 1.1667 - acc: 0.6735 - val_loss: 1.9344 - val_acc: 0.4347
Epoch 32/50
 - 12s - loss: 1.1376 - acc: 0.6796 - val_loss: 1.9177 - val_acc: 0.4366
Epoch 33/50
 - 14s - loss: 1.1097 - acc: 0.6864 - val_loss: 1.8977 - val_acc: 0.4460
Epoch 34/50
 - 14s - loss: 1.0834 - acc: 0.6939 - val_loss: 1.8814 - val_acc: 0.4563
Epoch 35/50
 - 14s - loss: 1.0574 - acc: 0.7031 - val_loss: 1.8615 - val_acc: 0.4648
Epoch 36/50
 - 17s - loss: 1.0331 - acc: 0.7111 - val_loss: 1.8491 - val_acc: 0.4695
Epoch 37/50
 - 15s - loss: 1.0129 - acc: 0.7165 - val_loss: 1.8612 - val_acc: 0.4695
Epoch 38/50
 - 13s - loss: 0.9878 - acc: 0.7280 - val_loss: 1.8255 - val_acc: 0.4817
Epoch 39/50
 - 13s - loss: 0.9662 - acc: 0.7334 - val_loss: 1.8129 - val_acc: 0.4892
Epoch 40/50
 - 13s - loss: 0.9433 - acc: 0.7411 - val_loss: 1.8092 - val_acc: 0.4854
Epoch 41/50
 - 13s - loss: 0.9224 - acc: 0.7496 - val_loss: 1.8036 - val_acc: 0.4911
Epoch 42/50
 - 14s - loss: 0.9004 - acc: 0.7571 - val_loss: 1.8271 - val_acc: 0.4892
Epoch 43/50
 - 13s - loss: 0.8803 - acc: 0.7623 - val_loss: 1.8243 - val_acc: 0.4901
Epoch 44/50
 - 13s - loss: 0.8580 - acc: 0.7703 - val_loss: 1.7978 - val_acc: 0.4920
Epoch 45/50
 - 13s - loss: 0.8392 - acc: 0.7754 - val_loss: 1.8008 - val_acc: 0.4930
Epoch 46/50
 - 13s - loss: 0.8197 - acc: 0.7822 - val_loss: 1.7996 - val_acc: 0.4948
Epoch 47/50
 - 13s - loss: 0.8013 - acc: 0.7858 - val_loss: 1.8433 - val_acc: 0.4873
Epoch 48/50
 - 13s - loss: 0.7829 - acc: 0.7919 - val_loss: 1.7772 - val_acc: 0.5042
Epoch 49/50
 - 13s - loss: 0.7662 - acc: 0.7970 - val_loss: 1.8247 - val_acc: 0.5005
Epoch 50/50
 - 13s - loss: 0.7492 - acc: 0.8020 - val_loss: 1.7778 - val_acc: 0.5117
LSTM training time: 650.2352459430695s
Model: "sequential_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_3 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_25 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_26 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.166482479829365, 3.082468559015573, 2.963449654457069, 2.831566255418017, 2.693620937767235, 2.5743944653328916, 2.464558900340723, 2.3610406858360427, 2.2666611948118596, 2.18039802169755, 2.105034269467749, 2.0366448232593855, 1.9736507265450727, 1.9151511766945837, 1.8594203067548347, 1.8069008908306514, 1.7558933971408166, 1.7060410008902054, 1.654474104041641, 1.6059481152520372, 1.5589900986812808, 1.5130689841692286, 1.4691714458071292, 1.423505097648642, 1.3800029959061462, 1.3392203507749356, 1.3019843837143474, 1.2656604984934017, 1.2305480203411236, 1.197555843725825, 1.1666586650826434, 1.1375858755718802, 1.1097266020728915, 1.0833927191057087, 1.0573897843129034, 1.0331452685901192, 1.012901153689347, 0.9878077938967197, 0.9661825386492725, 0.9433375362613993, 0.9224215883981294, 0.9004408437124238, 0.8803038284312062, 0.8579600466258206, 0.8392138686263004, 0.8197483317880584, 0.8013032939978992, 0.7829144521025052, 0.7661909516537613, 0.7492143354954673]
[0.05308903, 0.08644585, 0.087150574, 0.090439275, 0.12309138, 0.21282594, 0.32840028, 0.3523608, 0.3744421, 0.38924125, 0.39605355, 0.4167254, 0.43504816, 0.45830396, 0.48226452, 0.5008222, 0.51632607, 0.53229976, 0.54592437, 0.56213295, 0.5736434, 0.5853888, 0.5992483, 0.60676533, 0.6217994, 0.63143057, 0.6415316, 0.6488137, 0.6553911, 0.6645525, 0.67347896, 0.6795866, 0.68639886, 0.6939159, 0.70307726, 0.7110641, 0.716467, 0.72797745, 0.7333803, 0.74113226, 0.7495889, 0.75710595, 0.7622739, 0.77026075, 0.7754287, 0.782241, 0.78576463, 0.7918722, 0.79704016, 0.8019732]
[3.1318613209075212, 3.025935725091209, 2.9134934380580564, 2.798512169117099, 2.71486141894345, 2.656772997009922, 2.5916290280964454, 2.5264109510770987, 2.462112839121214, 2.4088803242070016, 2.36876258984418, 2.3335667970594667, 2.301196750900555, 2.2680295641993133, 2.2384029368279688, 2.2142194152438024, 2.187472890464353, 2.1584451188503855, 2.1341971359342478, 2.109996787707011, 2.0854338559746184, 2.064582339922587, 2.0430536617135777, 2.023198950234713, 2.008743553654129, 2.0082301173411623, 1.99451168311034, 1.98088036512545, 1.9671949589196505, 1.9530549998574414, 1.9344182696140988, 1.917672289593119, 1.8977406371927037, 1.8813526284526771, 1.8615038207998857, 1.849144217218032, 1.8612482356353546, 1.8255034787554136, 1.81286119642392, 1.8092105455241851, 1.8036209124914357, 1.8270613007702179, 1.8243343861450052, 1.7977680057427132, 1.800822191786878, 1.799589168465753, 1.8432683866908293, 1.7771635318026295, 1.824658824496426, 1.7777553420951109]
[0.12018779665231705, 0.12957745790481567, 0.12488263100385666, 0.12582159042358398, 0.13145539164543152, 0.18028168380260468, 0.1943662017583847, 0.21784037351608276, 0.26291078329086304, 0.27699530124664307, 0.2920187711715698, 0.29765257239341736, 0.3004694879055023, 0.3089201748371124, 0.3173708915710449, 0.3323943614959717, 0.3455398976802826, 0.3596244156360626, 0.36525821685791016, 0.3661971688270569, 0.37276995182037354, 0.3840375542640686, 0.3877934217453003, 0.3924882709980011, 0.4018779397010803, 0.39530515670776367, 0.4000000059604645, 0.4000000059604645, 0.41502347588539124, 0.4225352108478546, 0.4347417950630188, 0.43661972880363464, 0.44600939750671387, 0.4563380181789398, 0.4647887349128723, 0.46948355436325073, 0.46948355436325073, 0.4816901385784149, 0.4892018735408783, 0.4854460060596466, 0.49107980728149414, 0.4892018735408783, 0.4901408553123474, 0.49201878905296326, 0.49295774102211, 0.49483567476272583, 0.48732393980026245, 0.5042253732681274, 0.5004695057868958, 0.5117371082305908]

  32/2898 [..............................] - ETA: 39s
 384/2898 [==>...........................] - ETA: 3s 
 736/2898 [======>.......................] - ETA: 1s
1088/2898 [==========>...................] - ETA: 0s
1440/2898 [=============>................] - ETA: 0s
1792/2898 [=================>............] - ETA: 0s
2144/2898 [=====================>........] - ETA: 0s
2496/2898 [========================>.....] - ETA: 0s
2848/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 297us/step
Accuracies per class for LSTM
[0.   0.57 0.83 0.29 0.71 0.17 0.33 0.96 0.48 0.   0.29 0.15 0.   0.21
 0.   0.69 0.   0.   0.49 0.45 0.22 0.89]
[3.166482479829365, 3.082468559015573, 2.963449654457069, 2.831566255418017, 2.693620937767235, 2.5743944653328916, 2.464558900340723, 2.3610406858360427, 2.2666611948118596, 2.18039802169755, 2.105034269467749, 2.0366448232593855, 1.9736507265450727, 1.9151511766945837, 1.8594203067548347, 1.8069008908306514, 1.7558933971408166, 1.7060410008902054, 1.654474104041641, 1.6059481152520372, 1.5589900986812808, 1.5130689841692286, 1.4691714458071292, 1.423505097648642, 1.3800029959061462, 1.3392203507749356, 1.3019843837143474, 1.2656604984934017, 1.2305480203411236, 1.197555843725825, 1.1666586650826434, 1.1375858755718802, 1.1097266020728915, 1.0833927191057087, 1.0573897843129034, 1.0331452685901192, 1.012901153689347, 0.9878077938967197, 0.9661825386492725, 0.9433375362613993, 0.9224215883981294, 0.9004408437124238, 0.8803038284312062, 0.8579600466258206, 0.8392138686263004, 0.8197483317880584, 0.8013032939978992, 0.7829144521025052, 0.7661909516537613, 0.7492143354954673]
[0.05308903, 0.08644585, 0.087150574, 0.090439275, 0.12309138, 0.21282594, 0.32840028, 0.3523608, 0.3744421, 0.38924125, 0.39605355, 0.4167254, 0.43504816, 0.45830396, 0.48226452, 0.5008222, 0.51632607, 0.53229976, 0.54592437, 0.56213295, 0.5736434, 0.5853888, 0.5992483, 0.60676533, 0.6217994, 0.63143057, 0.6415316, 0.6488137, 0.6553911, 0.6645525, 0.67347896, 0.6795866, 0.68639886, 0.6939159, 0.70307726, 0.7110641, 0.716467, 0.72797745, 0.7333803, 0.74113226, 0.7495889, 0.75710595, 0.7622739, 0.77026075, 0.7754287, 0.782241, 0.78576463, 0.7918722, 0.79704016, 0.8019732]
[3.1318613209075212, 3.025935725091209, 2.9134934380580564, 2.798512169117099, 2.71486141894345, 2.656772997009922, 2.5916290280964454, 2.5264109510770987, 2.462112839121214, 2.4088803242070016, 2.36876258984418, 2.3335667970594667, 2.301196750900555, 2.2680295641993133, 2.2384029368279688, 2.2142194152438024, 2.187472890464353, 2.1584451188503855, 2.1341971359342478, 2.109996787707011, 2.0854338559746184, 2.064582339922587, 2.0430536617135777, 2.023198950234713, 2.008743553654129, 2.0082301173411623, 1.99451168311034, 1.98088036512545, 1.9671949589196505, 1.9530549998574414, 1.9344182696140988, 1.917672289593119, 1.8977406371927037, 1.8813526284526771, 1.8615038207998857, 1.849144217218032, 1.8612482356353546, 1.8255034787554136, 1.81286119642392, 1.8092105455241851, 1.8036209124914357, 1.8270613007702179, 1.8243343861450052, 1.7977680057427132, 1.800822191786878, 1.799589168465753, 1.8432683866908293, 1.7771635318026295, 1.824658824496426, 1.7777553420951109]
[0.12018779665231705, 0.12957745790481567, 0.12488263100385666, 0.12582159042358398, 0.13145539164543152, 0.18028168380260468, 0.1943662017583847, 0.21784037351608276, 0.26291078329086304, 0.27699530124664307, 0.2920187711715698, 0.29765257239341736, 0.3004694879055023, 0.3089201748371124, 0.3173708915710449, 0.3323943614959717, 0.3455398976802826, 0.3596244156360626, 0.36525821685791016, 0.3661971688270569, 0.37276995182037354, 0.3840375542640686, 0.3877934217453003, 0.3924882709980011, 0.4018779397010803, 0.39530515670776367, 0.4000000059604645, 0.4000000059604645, 0.41502347588539124, 0.4225352108478546, 0.4347417950630188, 0.43661972880363464, 0.44600939750671387, 0.4563380181789398, 0.4647887349128723, 0.46948355436325073, 0.46948355436325073, 0.4816901385784149, 0.4892018735408783, 0.4854460060596466, 0.49107980728149414, 0.4892018735408783, 0.4901408553123474, 0.49201878905296326, 0.49295774102211, 0.49483567476272583, 0.48732393980026245, 0.5042253732681274, 0.5004695057868958, 0.5117371082305908]
recall 0.4688
precision 0.53
f1 0.4434
mcc 0.3874
RMSE: 6.567
classification report:
              precision    recall  f1-score   support

           1       0.00      0.00      0.00        72
           2       0.31      0.57      0.40        68
           3       0.60      0.83      0.70       252
           4       0.46      0.29      0.36       132
           5       0.33      0.71      0.45       191
           6       0.09      0.17      0.12       150
           7       0.22      0.33      0.26       240
           9       0.53      0.96      0.68       100
          10       0.66      0.48      0.55        61
          11       0.00      0.00      0.00        87
          12       0.82      0.29      0.42       595
          13       0.82      0.15      0.26        59
          14       0.00      0.00      0.00         6
          15       0.38      0.21      0.27        72
          16       0.00      0.00      0.00        30
          17       0.60      0.69      0.64       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.56      0.49      0.52       222
          22       0.53      0.45      0.49       168
          23       0.56      0.22      0.31        65
          24       0.28      0.89      0.43        82

   micro avg       0.42      0.42      0.42      2898
   macro avg       0.35      0.35      0.31      2898
weighted avg       0.48      0.42      0.40      2898

>#3: 42.271
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1851 - acc: 0.0237 - val_loss: 3.1657 - val_acc: 0.0394
Epoch 2/50
 - 12s - loss: 3.1284 - acc: 0.0397 - val_loss: 3.1240 - val_acc: 0.0338
Epoch 3/50
 - 13s - loss: 3.0520 - acc: 0.0564 - val_loss: 3.0368 - val_acc: 0.0582
Epoch 4/50
 - 12s - loss: 2.9315 - acc: 0.1085 - val_loss: 2.9171 - val_acc: 0.1230
Epoch 5/50
 - 12s - loss: 2.7943 - acc: 0.1628 - val_loss: 2.7812 - val_acc: 0.1502
Epoch 6/50
 - 13s - loss: 2.6478 - acc: 0.1806 - val_loss: 2.6501 - val_acc: 0.1775
Epoch 7/50
 - 12s - loss: 2.5110 - acc: 0.2253 - val_loss: 2.5159 - val_acc: 0.2131
Epoch 8/50
 - 12s - loss: 2.3876 - acc: 0.2864 - val_loss: 2.4064 - val_acc: 0.2667
Epoch 9/50
 - 13s - loss: 2.2661 - acc: 0.3199 - val_loss: 2.3055 - val_acc: 0.3249
Epoch 10/50
 - 13s - loss: 2.1490 - acc: 0.3815 - val_loss: 2.2070 - val_acc: 0.3859
Epoch 11/50
 - 13s - loss: 2.0392 - acc: 0.4583 - val_loss: 2.1066 - val_acc: 0.4648
Epoch 12/50
 - 13s - loss: 1.9382 - acc: 0.5191 - val_loss: 2.0185 - val_acc: 0.4892
Epoch 13/50
 - 13s - loss: 1.8472 - acc: 0.5408 - val_loss: 1.9436 - val_acc: 0.4939
Epoch 14/50
 - 13s - loss: 1.7622 - acc: 0.5527 - val_loss: 1.8719 - val_acc: 0.4892
Epoch 15/50
 - 12s - loss: 1.6846 - acc: 0.5584 - val_loss: 1.8088 - val_acc: 0.4920
Epoch 16/50
 - 13s - loss: 1.6158 - acc: 0.5638 - val_loss: 1.7501 - val_acc: 0.5014
Epoch 17/50
 - 13s - loss: 1.5537 - acc: 0.5699 - val_loss: 1.7030 - val_acc: 0.5099
Epoch 18/50
 - 13s - loss: 1.4992 - acc: 0.5743 - val_loss: 1.6718 - val_acc: 0.5070
Epoch 19/50
 - 12s - loss: 1.4496 - acc: 0.5765 - val_loss: 1.6348 - val_acc: 0.5192
Epoch 20/50
 - 13s - loss: 1.4045 - acc: 0.5840 - val_loss: 1.6186 - val_acc: 0.5211
Epoch 21/50
 - 12s - loss: 1.3635 - acc: 0.5910 - val_loss: 1.6036 - val_acc: 0.5202
Epoch 22/50
 - 13s - loss: 1.3253 - acc: 0.5962 - val_loss: 1.5771 - val_acc: 0.5305
Epoch 23/50
 - 12s - loss: 1.2899 - acc: 0.6030 - val_loss: 1.5702 - val_acc: 0.5333
Epoch 24/50
 - 12s - loss: 1.2571 - acc: 0.6070 - val_loss: 1.5634 - val_acc: 0.5371
Epoch 25/50
 - 12s - loss: 1.2250 - acc: 0.6138 - val_loss: 1.5523 - val_acc: 0.5493
Epoch 26/50
 - 12s - loss: 1.1952 - acc: 0.6180 - val_loss: 1.5386 - val_acc: 0.5455
Epoch 27/50
 - 12s - loss: 1.1643 - acc: 0.6288 - val_loss: 1.5332 - val_acc: 0.5474
Epoch 28/50
 - 12s - loss: 1.1365 - acc: 0.6364 - val_loss: 1.5208 - val_acc: 0.5531
Epoch 29/50
 - 12s - loss: 1.1096 - acc: 0.6568 - val_loss: 1.5120 - val_acc: 0.5596
Epoch 30/50
 - 12s - loss: 1.0840 - acc: 0.6671 - val_loss: 1.5017 - val_acc: 0.5559
Epoch 31/50
 - 12s - loss: 1.0593 - acc: 0.6747 - val_loss: 1.5082 - val_acc: 0.5474
Epoch 32/50
 - 12s - loss: 1.0351 - acc: 0.6796 - val_loss: 1.5042 - val_acc: 0.5474
Epoch 33/50
 - 12s - loss: 1.0120 - acc: 0.6869 - val_loss: 1.5022 - val_acc: 0.5446
Epoch 34/50
 - 12s - loss: 0.9895 - acc: 0.6932 - val_loss: 1.5044 - val_acc: 0.5408
Epoch 35/50
 - 12s - loss: 0.9669 - acc: 0.7050 - val_loss: 1.4921 - val_acc: 0.5371
Epoch 36/50
 - 13s - loss: 0.9464 - acc: 0.7136 - val_loss: 1.5001 - val_acc: 0.5324
Epoch 37/50
 - 17s - loss: 0.9253 - acc: 0.7226 - val_loss: 1.4966 - val_acc: 0.5343
Epoch 38/50
 - 16s - loss: 0.9069 - acc: 0.7303 - val_loss: 1.5024 - val_acc: 0.5343
Epoch 39/50
 - 16s - loss: 0.8862 - acc: 0.7355 - val_loss: 1.5108 - val_acc: 0.5277
Epoch 40/50
 - 18s - loss: 0.8678 - acc: 0.7458 - val_loss: 1.5209 - val_acc: 0.5286
Epoch 00040: early stopping
LSTM training time: 520.844179391861s
Model: "sequential_14"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_4 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_27 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_28 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.185051341165476, 3.1283581994468683, 3.0519785399388786, 2.931463552467807, 2.794297894430127, 2.6477858913821293, 2.5109726277656725, 2.3876478931336496, 2.2661442849063134, 2.1489812562744017, 2.0391859695724905, 1.9381928872636487, 1.8472238085430666, 1.7622315072386037, 1.684592377503825, 1.6158421591242798, 1.5537082815439118, 1.4991629147490397, 1.4496376005758107, 1.4044721349181215, 1.3635421976926658, 1.3253405570059784, 1.289868511502833, 1.257116090181654, 1.2249897675654797, 1.1952136408170961, 1.1643213492297275, 1.1365198704007369, 1.1095583551865889, 1.0840068434378, 1.0592901227422127, 1.0351477094888968, 1.012043132086385, 0.9894671418562108, 0.9668710119838526, 0.9464352715245037, 0.9253066283367821, 0.9068848175014832, 0.8861568823565277, 0.8678453670670679]
[0.023725629, 0.03969932, 0.05637773, 0.10852713, 0.1627907, 0.18064365, 0.22527602, 0.2863519, 0.31994364, 0.3814893, 0.45830396, 0.51914495, 0.5407564, 0.55273664, 0.55837446, 0.5637773, 0.5698849, 0.57434815, 0.57646227, 0.5839793, 0.59102654, 0.5961945, 0.60300684, 0.60700023, 0.61381257, 0.61804086, 0.6288466, 0.6363636, 0.65680057, 0.6671365, 0.67465353, 0.6795866, 0.68686867, 0.6932112, 0.70495653, 0.7136481, 0.7225746, 0.73032653, 0.7354945, 0.7458304]
[3.165664802918412, 3.123960755800417, 3.0367700657374423, 2.9171327205890782, 2.7812438291003447, 2.6500622178467226, 2.515857477367204, 2.406378849235499, 2.3054542924316834, 2.2069660692707473, 2.1066059866981326, 2.0184991219793686, 1.9436017329704034, 1.8718799163478081, 1.8088113525104075, 1.7501355833850556, 1.702989101409912, 1.6718307575709384, 1.6348195576331985, 1.6186000802707223, 1.6035987934036433, 1.5771387129882133, 1.5702275408825404, 1.5634328237721618, 1.5523213323293157, 1.5385572114460906, 1.5331610053357945, 1.520782420277036, 1.5119655383024977, 1.5016982050009178, 1.508205069734457, 1.5041907306847997, 1.5021875466259433, 1.5044192456583463, 1.4920790281094296, 1.500146974868058, 1.496599492332745, 1.5024429425107482, 1.5108442964128486, 1.5208792773210946]
[0.039436619728803635, 0.0338028185069561, 0.05821596086025238, 0.12300469726324081, 0.15023474395275116, 0.1774647831916809, 0.21314553916454315, 0.2666666805744171, 0.3248826265335083, 0.38591548800468445, 0.4647887349128723, 0.4892018735408783, 0.4938967227935791, 0.4892018735408783, 0.49201878905296326, 0.5014084577560425, 0.5098591446876526, 0.5070422291755676, 0.5192488431930542, 0.5211267471313477, 0.5201877951622009, 0.5305164456367493, 0.5333333611488342, 0.5370892286300659, 0.5492957830429077, 0.545539915561676, 0.5474178194999695, 0.5530516505241394, 0.559624433517456, 0.5558685660362244, 0.5474178194999695, 0.5474178194999695, 0.5446009635925293, 0.5408450961112976, 0.5370892286300659, 0.5323943495750427, 0.534272313117981, 0.534272313117981, 0.5276995301246643, 0.528638482093811]

  32/2898 [..............................] - ETA: 56s
 352/2898 [==>...........................] - ETA: 4s 
 704/2898 [======>.......................] - ETA: 2s
1056/2898 [=========>....................] - ETA: 1s
1440/2898 [=============>................] - ETA: 0s
1824/2898 [=================>............] - ETA: 0s
2176/2898 [=====================>........] - ETA: 0s
2368/2898 [=======================>......] - ETA: 0s
2688/2898 [==========================>...] - ETA: 0s
2898/2898 [==============================] - 1s 377us/step
Accuracies per class for LSTM
[0.08 0.32 0.82 0.98 0.2  0.37 0.67 0.49 0.   0.   0.42 0.   0.   0.43
 0.17 0.63 0.   0.   0.3  0.7  0.   0.94]
[3.185051341165476, 3.1283581994468683, 3.0519785399388786, 2.931463552467807, 2.794297894430127, 2.6477858913821293, 2.5109726277656725, 2.3876478931336496, 2.2661442849063134, 2.1489812562744017, 2.0391859695724905, 1.9381928872636487, 1.8472238085430666, 1.7622315072386037, 1.684592377503825, 1.6158421591242798, 1.5537082815439118, 1.4991629147490397, 1.4496376005758107, 1.4044721349181215, 1.3635421976926658, 1.3253405570059784, 1.289868511502833, 1.257116090181654, 1.2249897675654797, 1.1952136408170961, 1.1643213492297275, 1.1365198704007369, 1.1095583551865889, 1.0840068434378, 1.0592901227422127, 1.0351477094888968, 1.012043132086385, 0.9894671418562108, 0.9668710119838526, 0.9464352715245037, 0.9253066283367821, 0.9068848175014832, 0.8861568823565277, 0.8678453670670679]
[0.023725629, 0.03969932, 0.05637773, 0.10852713, 0.1627907, 0.18064365, 0.22527602, 0.2863519, 0.31994364, 0.3814893, 0.45830396, 0.51914495, 0.5407564, 0.55273664, 0.55837446, 0.5637773, 0.5698849, 0.57434815, 0.57646227, 0.5839793, 0.59102654, 0.5961945, 0.60300684, 0.60700023, 0.61381257, 0.61804086, 0.6288466, 0.6363636, 0.65680057, 0.6671365, 0.67465353, 0.6795866, 0.68686867, 0.6932112, 0.70495653, 0.7136481, 0.7225746, 0.73032653, 0.7354945, 0.7458304]
[3.165664802918412, 3.123960755800417, 3.0367700657374423, 2.9171327205890782, 2.7812438291003447, 2.6500622178467226, 2.515857477367204, 2.406378849235499, 2.3054542924316834, 2.2069660692707473, 2.1066059866981326, 2.0184991219793686, 1.9436017329704034, 1.8718799163478081, 1.8088113525104075, 1.7501355833850556, 1.702989101409912, 1.6718307575709384, 1.6348195576331985, 1.6186000802707223, 1.6035987934036433, 1.5771387129882133, 1.5702275408825404, 1.5634328237721618, 1.5523213323293157, 1.5385572114460906, 1.5331610053357945, 1.520782420277036, 1.5119655383024977, 1.5016982050009178, 1.508205069734457, 1.5041907306847997, 1.5021875466259433, 1.5044192456583463, 1.4920790281094296, 1.500146974868058, 1.496599492332745, 1.5024429425107482, 1.5108442964128486, 1.5208792773210946]
[0.039436619728803635, 0.0338028185069561, 0.05821596086025238, 0.12300469726324081, 0.15023474395275116, 0.1774647831916809, 0.21314553916454315, 0.2666666805744171, 0.3248826265335083, 0.38591548800468445, 0.4647887349128723, 0.4892018735408783, 0.4938967227935791, 0.4892018735408783, 0.49201878905296326, 0.5014084577560425, 0.5098591446876526, 0.5070422291755676, 0.5192488431930542, 0.5211267471313477, 0.5201877951622009, 0.5305164456367493, 0.5333333611488342, 0.5370892286300659, 0.5492957830429077, 0.545539915561676, 0.5474178194999695, 0.5530516505241394, 0.559624433517456, 0.5558685660362244, 0.5474178194999695, 0.5474178194999695, 0.5446009635925293, 0.5408450961112976, 0.5370892286300659, 0.5323943495750427, 0.534272313117981, 0.534272313117981, 0.5276995301246643, 0.528638482093811]
recall 0.4953
precision 0.48
f1 0.4676
mcc 0.4132
RMSE: 5.667
classification report:
              precision    recall  f1-score   support

           1       0.12      0.08      0.10        72
           2       0.28      0.32      0.30        68
           3       0.84      0.82      0.83       252
           4       0.44      0.98      0.61       132
           5       0.31      0.20      0.25       191
           6       0.21      0.37      0.27       150
           7       0.57      0.67      0.62       240
           9       0.17      0.49      0.26       100
          10       0.00      0.00      0.00        61
          11       0.00      0.00      0.00        87
          12       0.62      0.42      0.50       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.24      0.43      0.31        72
          16       0.42      0.17      0.24        30
          17       0.54      0.63      0.58       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.44      0.30      0.36       222
          22       0.68      0.70      0.69       168
          23       0.00      0.00      0.00        65
          24       0.44      0.94      0.60        82

   micro avg       0.45      0.45      0.45      2898
   macro avg       0.29      0.34      0.30      2898
weighted avg       0.44      0.45      0.43      2898

>#4: 45.342
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1665 - acc: 0.0543 - val_loss: 3.1315 - val_acc: 0.1239
Epoch 2/50
 - 14s - loss: 3.0815 - acc: 0.1250 - val_loss: 3.0280 - val_acc: 0.1531
Epoch 3/50
 - 16s - loss: 2.9458 - acc: 0.1555 - val_loss: 2.8860 - val_acc: 0.1568
Epoch 4/50
 - 14s - loss: 2.7775 - acc: 0.1950 - val_loss: 2.7693 - val_acc: 0.1690
Epoch 5/50
 - 16s - loss: 2.6278 - acc: 0.2279 - val_loss: 2.6676 - val_acc: 0.1859
Epoch 6/50
 - 13s - loss: 2.4845 - acc: 0.2612 - val_loss: 2.5703 - val_acc: 0.2272
Epoch 7/50
 - 16s - loss: 2.3470 - acc: 0.3120 - val_loss: 2.4749 - val_acc: 0.2216
Epoch 8/50
 - 16s - loss: 2.2217 - acc: 0.3625 - val_loss: 2.3802 - val_acc: 0.2272
Epoch 9/50
 - 16s - loss: 2.1100 - acc: 0.3998 - val_loss: 2.2998 - val_acc: 0.2291
Epoch 10/50
 - 16s - loss: 2.0092 - acc: 0.4268 - val_loss: 2.2256 - val_acc: 0.2526
Epoch 11/50
 - 13s - loss: 1.9189 - acc: 0.4409 - val_loss: 2.1609 - val_acc: 0.2657
Epoch 12/50
 - 12s - loss: 1.8368 - acc: 0.4487 - val_loss: 2.1027 - val_acc: 0.2808
Epoch 13/50
 - 15s - loss: 1.7632 - acc: 0.4576 - val_loss: 2.0440 - val_acc: 0.2836
Epoch 14/50
 - 14s - loss: 1.6969 - acc: 0.4689 - val_loss: 1.9959 - val_acc: 0.3005
Epoch 15/50
 - 15s - loss: 1.6365 - acc: 0.4806 - val_loss: 1.9630 - val_acc: 0.3127
Epoch 16/50
 - 13s - loss: 1.5784 - acc: 0.4954 - val_loss: 1.9257 - val_acc: 0.3493
Epoch 17/50
 - 12s - loss: 1.5239 - acc: 0.5222 - val_loss: 1.8941 - val_acc: 0.3775
Epoch 18/50
 - 13s - loss: 1.4731 - acc: 0.5410 - val_loss: 1.8564 - val_acc: 0.3859
Epoch 19/50
 - 13s - loss: 1.4256 - acc: 0.5720 - val_loss: 1.8320 - val_acc: 0.3981
Epoch 20/50
 - 12s - loss: 1.3805 - acc: 0.5995 - val_loss: 1.7935 - val_acc: 0.4169
Epoch 21/50
 - 12s - loss: 1.3363 - acc: 0.6281 - val_loss: 1.7678 - val_acc: 0.4357
Epoch 22/50
 - 12s - loss: 1.2922 - acc: 0.6490 - val_loss: 1.7487 - val_acc: 0.4479
Epoch 23/50
 - 13s - loss: 1.2506 - acc: 0.6650 - val_loss: 1.6883 - val_acc: 0.5033
Epoch 24/50
 - 12s - loss: 1.2054 - acc: 0.6794 - val_loss: 1.6897 - val_acc: 0.5061
Epoch 25/50
 - 12s - loss: 1.1643 - acc: 0.6883 - val_loss: 1.6698 - val_acc: 0.5108
Epoch 26/50
 - 13s - loss: 1.1235 - acc: 0.6993 - val_loss: 1.6360 - val_acc: 0.5296
Epoch 27/50
 - 13s - loss: 1.0835 - acc: 0.7115 - val_loss: 1.6253 - val_acc: 0.5315
Epoch 28/50
 - 13s - loss: 1.0434 - acc: 0.7221 - val_loss: 1.6060 - val_acc: 0.5418
Epoch 29/50
 - 12s - loss: 1.0064 - acc: 0.7329 - val_loss: 1.6037 - val_acc: 0.5455
Epoch 30/50
 - 13s - loss: 0.9710 - acc: 0.7456 - val_loss: 1.5846 - val_acc: 0.5531
Epoch 31/50
 - 13s - loss: 0.9380 - acc: 0.7524 - val_loss: 1.5780 - val_acc: 0.5493
Epoch 32/50
 - 13s - loss: 0.9070 - acc: 0.7604 - val_loss: 1.5862 - val_acc: 0.5455
Epoch 33/50
 - 13s - loss: 0.8784 - acc: 0.7672 - val_loss: 1.5583 - val_acc: 0.5549
Epoch 34/50
 - 13s - loss: 0.8496 - acc: 0.7724 - val_loss: 1.5710 - val_acc: 0.5446
Epoch 35/50
 - 12s - loss: 0.8247 - acc: 0.7804 - val_loss: 1.5647 - val_acc: 0.5418
Epoch 36/50
 - 12s - loss: 0.7989 - acc: 0.7848 - val_loss: 1.5642 - val_acc: 0.5446
Epoch 37/50
 - 12s - loss: 0.7759 - acc: 0.7914 - val_loss: 1.5678 - val_acc: 0.5455
Epoch 38/50
 - 13s - loss: 0.7527 - acc: 0.7942 - val_loss: 1.5617 - val_acc: 0.5493
Epoch 00038: early stopping
LSTM training time: 510.4670522212982s
Model: "sequential_15"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_5 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_29 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_30 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1665188664137713, 3.0814740344731493, 2.945764430845627, 2.7774925408913322, 2.6277791310118372, 2.4844799374425133, 2.3469599850278016, 2.2216641440312013, 2.1100252059975055, 2.0092001665862256, 1.9189041005296552, 1.836819738184647, 1.7631900831374423, 1.696895743189268, 1.6364594471687046, 1.5783881836719347, 1.5238860809441768, 1.4730713780088158, 1.4255953671536257, 1.3804957934576272, 1.3363097527370449, 1.2922171180954498, 1.2505887909649067, 1.205380788726204, 1.1642592957609552, 1.1234840392884387, 1.083490762151629, 1.0434075384975068, 1.0063806471968582, 0.9710351766863983, 0.9379844659646688, 0.9069629630727165, 0.878442457669045, 0.8496199292365951, 0.8247187726159373, 0.7989016240867394, 0.7758709562407263, 0.7527072606111653]
[0.054263566, 0.12497064, 0.15550858, 0.19497299, 0.22785999, 0.26121682, 0.31195676, 0.36246184, 0.39981207, 0.42682642, 0.44092083, 0.44867277, 0.45759925, 0.46887478, 0.48062015, 0.49541932, 0.52219874, 0.5409913, 0.5719991, 0.5994832, 0.6281419, 0.6490486, 0.6650223, 0.6793516, 0.68827814, 0.69931877, 0.71153396, 0.7221048, 0.7329105, 0.7455955, 0.7524078, 0.76039463, 0.76720697, 0.7723749, 0.7803618, 0.78482497, 0.7914024, 0.7942213]
[3.1315349332603493, 3.028026092332294, 2.885994902910761, 2.769310624386783, 2.6675774086249264, 2.5702671973358298, 2.4749341472213815, 2.380226207115281, 2.2997852455282435, 2.2256288978415477, 2.1609136686638486, 2.1026615644284816, 2.044011746885631, 1.9958903199630165, 1.9630154489911218, 1.9257168682528214, 1.8941189673025283, 1.856361278066053, 1.831973001598752, 1.7934858914831995, 1.7678099098339888, 1.748707955991718, 1.6883431479964457, 1.6896677689932882, 1.6698295101873193, 1.636027360969866, 1.6253078016316946, 1.6059805968557725, 1.6036794759298154, 1.5845925599756374, 1.5780171628849047, 1.5861509386922272, 1.5583449035743033, 1.5709670285104027, 1.5646930987846124, 1.564179423717266, 1.5677857021770567, 1.5617252469062806]
[0.12394366413354874, 0.15305164456367493, 0.15680751204490662, 0.1690140813589096, 0.1859154999256134, 0.227230042219162, 0.22159624099731445, 0.227230042219162, 0.22910797595977783, 0.2525821626186371, 0.265727698802948, 0.28075116872787476, 0.2835680842399597, 0.3004694879055023, 0.3126760423183441, 0.3492957651615143, 0.37746480107307434, 0.38591548800468445, 0.39812207221984863, 0.4169014096260071, 0.4356807470321655, 0.4478873312473297, 0.5032863616943359, 0.5061032772064209, 0.5107980966567993, 0.5295774936676025, 0.531455397605896, 0.5417840480804443, 0.545539915561676, 0.5530516505241394, 0.5492957830429077, 0.545539915561676, 0.5549295544624329, 0.5446009635925293, 0.5417840480804443, 0.5446009635925293, 0.545539915561676, 0.5492957830429077]

  32/2898 [..............................] - ETA: 49s
 384/2898 [==>...........................] - ETA: 3s 
 736/2898 [======>.......................] - ETA: 1s
1088/2898 [==========>...................] - ETA: 1s
1440/2898 [=============>................] - ETA: 0s
1792/2898 [=================>............] - ETA: 0s
2144/2898 [=====================>........] - ETA: 0s
2496/2898 [========================>.....] - ETA: 0s
2848/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 334us/step
Accuracies per class for LSTM
[0.   0.63 0.74 0.9  0.59 0.31 0.66 1.   0.49 0.   0.43 0.   0.   0.62
 0.33 0.65 0.   0.   0.53 0.37 0.   0.63]
[3.1665188664137713, 3.0814740344731493, 2.945764430845627, 2.7774925408913322, 2.6277791310118372, 2.4844799374425133, 2.3469599850278016, 2.2216641440312013, 2.1100252059975055, 2.0092001665862256, 1.9189041005296552, 1.836819738184647, 1.7631900831374423, 1.696895743189268, 1.6364594471687046, 1.5783881836719347, 1.5238860809441768, 1.4730713780088158, 1.4255953671536257, 1.3804957934576272, 1.3363097527370449, 1.2922171180954498, 1.2505887909649067, 1.205380788726204, 1.1642592957609552, 1.1234840392884387, 1.083490762151629, 1.0434075384975068, 1.0063806471968582, 0.9710351766863983, 0.9379844659646688, 0.9069629630727165, 0.878442457669045, 0.8496199292365951, 0.8247187726159373, 0.7989016240867394, 0.7758709562407263, 0.7527072606111653]
[0.054263566, 0.12497064, 0.15550858, 0.19497299, 0.22785999, 0.26121682, 0.31195676, 0.36246184, 0.39981207, 0.42682642, 0.44092083, 0.44867277, 0.45759925, 0.46887478, 0.48062015, 0.49541932, 0.52219874, 0.5409913, 0.5719991, 0.5994832, 0.6281419, 0.6490486, 0.6650223, 0.6793516, 0.68827814, 0.69931877, 0.71153396, 0.7221048, 0.7329105, 0.7455955, 0.7524078, 0.76039463, 0.76720697, 0.7723749, 0.7803618, 0.78482497, 0.7914024, 0.7942213]
[3.1315349332603493, 3.028026092332294, 2.885994902910761, 2.769310624386783, 2.6675774086249264, 2.5702671973358298, 2.4749341472213815, 2.380226207115281, 2.2997852455282435, 2.2256288978415477, 2.1609136686638486, 2.1026615644284816, 2.044011746885631, 1.9958903199630165, 1.9630154489911218, 1.9257168682528214, 1.8941189673025283, 1.856361278066053, 1.831973001598752, 1.7934858914831995, 1.7678099098339888, 1.748707955991718, 1.6883431479964457, 1.6896677689932882, 1.6698295101873193, 1.636027360969866, 1.6253078016316946, 1.6059805968557725, 1.6036794759298154, 1.5845925599756374, 1.5780171628849047, 1.5861509386922272, 1.5583449035743033, 1.5709670285104027, 1.5646930987846124, 1.564179423717266, 1.5677857021770567, 1.5617252469062806]
[0.12394366413354874, 0.15305164456367493, 0.15680751204490662, 0.1690140813589096, 0.1859154999256134, 0.227230042219162, 0.22159624099731445, 0.227230042219162, 0.22910797595977783, 0.2525821626186371, 0.265727698802948, 0.28075116872787476, 0.2835680842399597, 0.3004694879055023, 0.3126760423183441, 0.3492957651615143, 0.37746480107307434, 0.38591548800468445, 0.39812207221984863, 0.4169014096260071, 0.4356807470321655, 0.4478873312473297, 0.5032863616943359, 0.5061032772064209, 0.5107980966567993, 0.5295774936676025, 0.531455397605896, 0.5417840480804443, 0.545539915561676, 0.5530516505241394, 0.5492957830429077, 0.545539915561676, 0.5549295544624329, 0.5446009635925293, 0.5417840480804443, 0.5446009635925293, 0.545539915561676, 0.5492957830429077]
recall 0.5705
precision 0.5774
f1 0.5435
mcc 0.4585
RMSE: 5.043
classification report:
              precision    recall  f1-score   support

           1       0.00      0.00      0.00        72
           2       0.38      0.63      0.48        68
           3       0.99      0.74      0.85       252
           4       0.43      0.90      0.58       132
           5       0.69      0.59      0.64       191
           6       0.38      0.31      0.34       150
           7       0.39      0.66      0.49       240
           9       0.27      1.00      0.42       100
          10       0.83      0.49      0.62        61
          11       0.00      0.00      0.00        87
          12       0.56      0.43      0.48       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.38      0.62      0.47        72
          16       1.00      0.33      0.50        30
          17       0.40      0.65      0.49       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.74      0.53      0.62       222
          22       0.36      0.37      0.36       168
          23       0.00      0.00      0.00        65
          24       1.00      0.63      0.78        82

   micro avg       0.50      0.50      0.50      2898
   macro avg       0.40      0.40      0.37      2898
weighted avg       0.50      0.50      0.47      2898

>#5: 49.586
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1646 - acc: 0.0627 - val_loss: 3.1250 - val_acc: 0.0995
Epoch 2/50
 - 13s - loss: 3.0778 - acc: 0.1132 - val_loss: 3.0308 - val_acc: 0.1850
Epoch 3/50
 - 12s - loss: 2.9690 - acc: 0.2276 - val_loss: 2.9312 - val_acc: 0.1944
Epoch 4/50
 - 13s - loss: 2.8486 - acc: 0.2495 - val_loss: 2.8084 - val_acc: 0.2113
Epoch 5/50
 - 13s - loss: 2.6892 - acc: 0.2530 - val_loss: 2.6531 - val_acc: 0.2150
Epoch 6/50
 - 14s - loss: 2.5134 - acc: 0.2701 - val_loss: 2.5125 - val_acc: 0.2178
Epoch 7/50
 - 16s - loss: 2.3592 - acc: 0.2932 - val_loss: 2.3906 - val_acc: 0.2385
Epoch 8/50
 - 14s - loss: 2.2263 - acc: 0.3324 - val_loss: 2.2866 - val_acc: 0.2901
Epoch 9/50
 - 13s - loss: 2.1139 - acc: 0.3759 - val_loss: 2.1961 - val_acc: 0.3305
Epoch 10/50
 - 14s - loss: 2.0142 - acc: 0.4191 - val_loss: 2.1211 - val_acc: 0.3577
Epoch 11/50
 - 15s - loss: 1.9272 - acc: 0.4480 - val_loss: 2.0630 - val_acc: 0.3953
Epoch 12/50
 - 13s - loss: 1.8499 - acc: 0.4691 - val_loss: 2.0192 - val_acc: 0.4169
Epoch 13/50
 - 13s - loss: 1.7808 - acc: 0.4846 - val_loss: 1.9811 - val_acc: 0.4310
Epoch 14/50
 - 13s - loss: 1.7185 - acc: 0.4935 - val_loss: 1.9414 - val_acc: 0.4535
Epoch 15/50
 - 13s - loss: 1.6617 - acc: 0.5032 - val_loss: 1.9001 - val_acc: 0.4742
Epoch 16/50
 - 13s - loss: 1.6091 - acc: 0.5126 - val_loss: 1.8571 - val_acc: 0.4873
Epoch 17/50
 - 14s - loss: 1.5596 - acc: 0.5236 - val_loss: 1.8167 - val_acc: 0.5099
Epoch 18/50
 - 14s - loss: 1.5131 - acc: 0.5344 - val_loss: 1.7779 - val_acc: 0.5343
Epoch 19/50
 - 13s - loss: 1.4693 - acc: 0.5462 - val_loss: 1.7403 - val_acc: 0.5502
Epoch 20/50
 - 13s - loss: 1.4279 - acc: 0.5591 - val_loss: 1.7074 - val_acc: 0.5606
Epoch 21/50
 - 13s - loss: 1.3883 - acc: 0.5736 - val_loss: 1.6762 - val_acc: 0.5728
Epoch 22/50
 - 13s - loss: 1.3502 - acc: 0.5870 - val_loss: 1.6488 - val_acc: 0.5784
Epoch 23/50
 - 13s - loss: 1.3145 - acc: 0.6009 - val_loss: 1.6229 - val_acc: 0.5869
Epoch 24/50
 - 15s - loss: 1.2768 - acc: 0.6190 - val_loss: 1.5951 - val_acc: 0.5934
Epoch 25/50
 - 13s - loss: 1.2429 - acc: 0.6328 - val_loss: 1.5704 - val_acc: 0.5981
Epoch 26/50
 - 13s - loss: 1.2114 - acc: 0.6439 - val_loss: 1.5495 - val_acc: 0.6028
Epoch 27/50
 - 13s - loss: 1.1793 - acc: 0.6648 - val_loss: 1.5273 - val_acc: 0.6047
Epoch 28/50
 - 13s - loss: 1.1491 - acc: 0.6758 - val_loss: 1.5068 - val_acc: 0.6047
Epoch 29/50
 - 12s - loss: 1.1194 - acc: 0.6974 - val_loss: 1.4818 - val_acc: 0.6056
Epoch 30/50
 - 14s - loss: 1.0903 - acc: 0.7127 - val_loss: 1.4620 - val_acc: 0.6085
Epoch 31/50
 - 13s - loss: 1.0620 - acc: 0.7263 - val_loss: 1.4416 - val_acc: 0.6122
Epoch 32/50
 - 13s - loss: 1.0344 - acc: 0.7367 - val_loss: 1.4245 - val_acc: 0.6131
Epoch 33/50
 - 13s - loss: 1.0077 - acc: 0.7437 - val_loss: 1.4113 - val_acc: 0.6150
Epoch 34/50
 - 13s - loss: 0.9815 - acc: 0.7508 - val_loss: 1.4050 - val_acc: 0.6122
Epoch 35/50
 - 13s - loss: 0.9567 - acc: 0.7595 - val_loss: 1.3937 - val_acc: 0.6113
Epoch 36/50
 - 13s - loss: 0.9329 - acc: 0.7672 - val_loss: 1.3797 - val_acc: 0.6066
Epoch 37/50
 - 13s - loss: 0.9096 - acc: 0.7752 - val_loss: 1.3700 - val_acc: 0.6066
Epoch 38/50
 - 13s - loss: 0.8869 - acc: 0.7801 - val_loss: 1.3615 - val_acc: 0.6150
Epoch 39/50
 - 13s - loss: 0.8651 - acc: 0.7834 - val_loss: 1.3569 - val_acc: 0.6254
Epoch 40/50
 - 13s - loss: 0.8443 - acc: 0.7862 - val_loss: 1.3575 - val_acc: 0.6272
Epoch 41/50
 - 14s - loss: 0.8239 - acc: 0.7883 - val_loss: 1.3592 - val_acc: 0.6272
Epoch 42/50
 - 13s - loss: 0.8037 - acc: 0.7923 - val_loss: 1.3692 - val_acc: 0.6272
Epoch 43/50
 - 13s - loss: 0.7836 - acc: 0.7966 - val_loss: 1.3695 - val_acc: 0.6404
Epoch 44/50
 - 14s - loss: 0.7636 - acc: 0.8020 - val_loss: 1.3687 - val_acc: 0.6366
Epoch 00044: early stopping
LSTM training time: 582.85404920578s
Model: "sequential_16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_6 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_31 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_32 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1646377453368286, 3.0777958931989, 2.968995856992342, 2.8486420969910426, 2.6891615893713654, 2.513378248210241, 2.359163180298163, 2.2263021310618543, 2.113906838005071, 2.014239405728798, 1.9271952612231475, 1.8499450335234915, 1.7808150846352733, 1.71854338627453, 1.6616948584253475, 1.609096399559391, 1.5595755469555415, 1.5130848826055996, 1.46930685770296, 1.42787910364647, 1.388341047715099, 1.3501648495027596, 1.3145370290876974, 1.2768217439463307, 1.2429294378748517, 1.2113864128511782, 1.17930952488802, 1.1491403277068175, 1.1194399206507282, 1.0902879931135918, 1.0619789794881997, 1.0343722738736891, 1.0076513107684903, 0.9815130675303871, 0.9567409464317167, 0.9328652706355144, 0.9096149978277576, 0.8869172811914224, 0.8650848199734812, 0.8442707594351951, 0.8238756705582184, 0.8036884078018705, 0.7835581393092905, 0.7636265180845734]
[0.062720224, 0.113225274, 0.22762509, 0.24947146, 0.25299507, 0.2701433, 0.2931642, 0.3323937, 0.37585154, 0.41907448, 0.44796807, 0.4691097, 0.48461357, 0.49354005, 0.50317127, 0.5125675, 0.52360815, 0.53441393, 0.54615927, 0.5590792, 0.5736434, 0.5870331, 0.60089266, 0.6189805, 0.63284004, 0.64388067, 0.6647874, 0.67582804, 0.6974395, 0.7127085, 0.7263331, 0.736669, 0.74371624, 0.7507635, 0.759455, 0.76720697, 0.7751938, 0.78012687, 0.78341556, 0.78623444, 0.7883486, 0.792342, 0.79657036, 0.8019732]
[3.1249854775101924, 3.030848771529578, 2.9311910855378343, 2.8084341402904527, 2.653103012658061, 2.51249178228244, 2.3905954300517767, 2.286635018514356, 2.1960946517371234, 2.1211459601988816, 2.063036574892035, 2.0191876237940902, 1.9810700386342868, 1.9413685050928537, 1.9000930220868106, 1.8570988118928362, 1.8166620831534337, 1.7779088811135628, 1.7403372104179131, 1.707397916283406, 1.6762236974049063, 1.6487919543270775, 1.6228774449635, 1.5950989421544501, 1.5704075437196543, 1.5495027760384787, 1.527291975726544, 1.5067941899870483, 1.4818009394435256, 1.4620314849094607, 1.4415699583543857, 1.4244559582410283, 1.4112732648009985, 1.4050191516887414, 1.3936571305346601, 1.379689966680858, 1.369995947063249, 1.3615254271757995, 1.3569281434229281, 1.3575307481445618, 1.359182111282304, 1.3692158325457238, 1.3695014630125162, 1.3687056291411182]
[0.09953051805496216, 0.18497653305530548, 0.1943662017583847, 0.2112676054239273, 0.215023472905159, 0.21784037351608276, 0.23849765956401825, 0.290140837430954, 0.33051642775535583, 0.3577464818954468, 0.39530515670776367, 0.4169014096260071, 0.4309859275817871, 0.45352113246917725, 0.47417840361595154, 0.48732393980026245, 0.5098591446876526, 0.534272313117981, 0.5502347350120544, 0.5605633854866028, 0.5727699398994446, 0.5784037709236145, 0.5868544578552246, 0.5934272408485413, 0.5981220602989197, 0.6028168797492981, 0.6046948432922363, 0.6046948432922363, 0.6056337952613831, 0.608450710773468, 0.6122065782546997, 0.6131455302238464, 0.6150234937667847, 0.6122065782546997, 0.611267626285553, 0.6065727472305298, 0.6065727472305298, 0.6150234937667847, 0.6253520846366882, 0.6272300481796265, 0.6272300481796265, 0.6272300481796265, 0.6403756141662598, 0.6366197466850281]

  32/2898 [..............................] - ETA: 52s
 384/2898 [==>...........................] - ETA: 4s 
 736/2898 [======>.......................] - ETA: 2s
1088/2898 [==========>...................] - ETA: 1s
1408/2898 [=============>................] - ETA: 0s
1760/2898 [=================>............] - ETA: 0s
2112/2898 [====================>.........] - ETA: 0s
2432/2898 [========================>.....] - ETA: 0s
2784/2898 [===========================>..] - ETA: 0s
2898/2898 [==============================] - 1s 354us/step
Accuracies per class for LSTM
[0.   0.53 0.69 0.82 0.64 0.44 0.65 0.65 0.43 0.13 0.27 0.17 0.   0.74
 0.   0.38 0.   0.    nan 0.78 0.73 0.09 0.27]
[3.1646377453368286, 3.0777958931989, 2.968995856992342, 2.8486420969910426, 2.6891615893713654, 2.513378248210241, 2.359163180298163, 2.2263021310618543, 2.113906838005071, 2.014239405728798, 1.9271952612231475, 1.8499450335234915, 1.7808150846352733, 1.71854338627453, 1.6616948584253475, 1.609096399559391, 1.5595755469555415, 1.5130848826055996, 1.46930685770296, 1.42787910364647, 1.388341047715099, 1.3501648495027596, 1.3145370290876974, 1.2768217439463307, 1.2429294378748517, 1.2113864128511782, 1.17930952488802, 1.1491403277068175, 1.1194399206507282, 1.0902879931135918, 1.0619789794881997, 1.0343722738736891, 1.0076513107684903, 0.9815130675303871, 0.9567409464317167, 0.9328652706355144, 0.9096149978277576, 0.8869172811914224, 0.8650848199734812, 0.8442707594351951, 0.8238756705582184, 0.8036884078018705, 0.7835581393092905, 0.7636265180845734]
[0.062720224, 0.113225274, 0.22762509, 0.24947146, 0.25299507, 0.2701433, 0.2931642, 0.3323937, 0.37585154, 0.41907448, 0.44796807, 0.4691097, 0.48461357, 0.49354005, 0.50317127, 0.5125675, 0.52360815, 0.53441393, 0.54615927, 0.5590792, 0.5736434, 0.5870331, 0.60089266, 0.6189805, 0.63284004, 0.64388067, 0.6647874, 0.67582804, 0.6974395, 0.7127085, 0.7263331, 0.736669, 0.74371624, 0.7507635, 0.759455, 0.76720697, 0.7751938, 0.78012687, 0.78341556, 0.78623444, 0.7883486, 0.792342, 0.79657036, 0.8019732]
[3.1249854775101924, 3.030848771529578, 2.9311910855378343, 2.8084341402904527, 2.653103012658061, 2.51249178228244, 2.3905954300517767, 2.286635018514356, 2.1960946517371234, 2.1211459601988816, 2.063036574892035, 2.0191876237940902, 1.9810700386342868, 1.9413685050928537, 1.9000930220868106, 1.8570988118928362, 1.8166620831534337, 1.7779088811135628, 1.7403372104179131, 1.707397916283406, 1.6762236974049063, 1.6487919543270775, 1.6228774449635, 1.5950989421544501, 1.5704075437196543, 1.5495027760384787, 1.527291975726544, 1.5067941899870483, 1.4818009394435256, 1.4620314849094607, 1.4415699583543857, 1.4244559582410283, 1.4112732648009985, 1.4050191516887414, 1.3936571305346601, 1.379689966680858, 1.369995947063249, 1.3615254271757995, 1.3569281434229281, 1.3575307481445618, 1.359182111282304, 1.3692158325457238, 1.3695014630125162, 1.3687056291411182]
[0.09953051805496216, 0.18497653305530548, 0.1943662017583847, 0.2112676054239273, 0.215023472905159, 0.21784037351608276, 0.23849765956401825, 0.290140837430954, 0.33051642775535583, 0.3577464818954468, 0.39530515670776367, 0.4169014096260071, 0.4309859275817871, 0.45352113246917725, 0.47417840361595154, 0.48732393980026245, 0.5098591446876526, 0.534272313117981, 0.5502347350120544, 0.5605633854866028, 0.5727699398994446, 0.5784037709236145, 0.5868544578552246, 0.5934272408485413, 0.5981220602989197, 0.6028168797492981, 0.6046948432922363, 0.6046948432922363, 0.6056337952613831, 0.608450710773468, 0.6122065782546997, 0.6131455302238464, 0.6150234937667847, 0.6122065782546997, 0.611267626285553, 0.6065727472305298, 0.6065727472305298, 0.6150234937667847, 0.6253520846366882, 0.6272300481796265, 0.6272300481796265, 0.6272300481796265, 0.6403756141662598, 0.6366197466850281]
recall 0.5089
precision 0.5494
f1 0.4864
mcc 0.4418
RMSE: 6.440
classification report:
              precision    recall  f1-score   support

           1       0.00      0.00      0.00        72
           2       0.24      0.53      0.33        68
           3       0.71      0.69      0.70       252
           4       0.45      0.82      0.58       132
           5       0.58      0.64      0.61       191
           6       0.33      0.44      0.37       150
           7       0.33      0.65      0.44       240
           9       0.26      0.65      0.37       100
          10       0.29      0.43      0.34        61
          11       0.13      0.13      0.13        87
          12       0.65      0.27      0.39       595
          13       0.77      0.17      0.28        59
          14       0.00      0.00      0.00         6
          15       0.42      0.74      0.54        72
          16       0.00      0.00      0.00        30
          17       0.59      0.38      0.47       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.77      0.78      0.78       222
          22       0.77      0.73      0.75       168
          23       0.75      0.09      0.16        65
          24       0.32      0.27      0.29        82

   micro avg       0.47      0.47      0.47      2898
   macro avg       0.36      0.37      0.33      2898
weighted avg       0.51      0.47      0.45      2898

>#6: 47.412
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 14s - loss: 3.1632 - acc: 0.1123 - val_loss: 3.1280 - val_acc: 0.2066
Epoch 2/50
 - 13s - loss: 3.0835 - acc: 0.1536 - val_loss: 3.0195 - val_acc: 0.1606
Epoch 3/50
 - 13s - loss: 2.9606 - acc: 0.1593 - val_loss: 2.8788 - val_acc: 0.1606
Epoch 4/50
 - 13s - loss: 2.8295 - acc: 0.1964 - val_loss: 2.7543 - val_acc: 0.2056
Epoch 5/50
 - 13s - loss: 2.7092 - acc: 0.2443 - val_loss: 2.6459 - val_acc: 0.2535
Epoch 6/50
 - 13s - loss: 2.5976 - acc: 0.3105 - val_loss: 2.5481 - val_acc: 0.3305
Epoch 7/50
 - 13s - loss: 2.4991 - acc: 0.3338 - val_loss: 2.4596 - val_acc: 0.3502
Epoch 8/50
 - 13s - loss: 2.4106 - acc: 0.3392 - val_loss: 2.3812 - val_acc: 0.3540
Epoch 9/50
 - 13s - loss: 2.3289 - acc: 0.3430 - val_loss: 2.3094 - val_acc: 0.3502
Epoch 10/50
 - 13s - loss: 2.2520 - acc: 0.3453 - val_loss: 2.2423 - val_acc: 0.3502
Epoch 11/50
 - 14s - loss: 2.1786 - acc: 0.3507 - val_loss: 2.1786 - val_acc: 0.3540
Epoch 12/50
 - 14s - loss: 2.1036 - acc: 0.3568 - val_loss: 2.1151 - val_acc: 0.3577
Epoch 13/50
 - 13s - loss: 2.0295 - acc: 0.3611 - val_loss: 2.0544 - val_acc: 0.3615
Epoch 14/50
 - 13s - loss: 1.9601 - acc: 0.3695 - val_loss: 1.9976 - val_acc: 0.3718
Epoch 15/50
 - 13s - loss: 1.8964 - acc: 0.3749 - val_loss: 1.9437 - val_acc: 0.3775
Epoch 16/50
 - 13s - loss: 1.8370 - acc: 0.3817 - val_loss: 1.8918 - val_acc: 0.3850
Epoch 17/50
 - 13s - loss: 1.7823 - acc: 0.3899 - val_loss: 1.8453 - val_acc: 0.3925
Epoch 18/50
 - 13s - loss: 1.7325 - acc: 0.3972 - val_loss: 1.8041 - val_acc: 0.4047
Epoch 19/50
 - 13s - loss: 1.6865 - acc: 0.4137 - val_loss: 1.7671 - val_acc: 0.4066
Epoch 20/50
 - 13s - loss: 1.6430 - acc: 0.4266 - val_loss: 1.7325 - val_acc: 0.4019
Epoch 21/50
 - 17s - loss: 1.6007 - acc: 0.4463 - val_loss: 1.7026 - val_acc: 0.4160
Epoch 22/50
 - 13s - loss: 1.5602 - acc: 0.4726 - val_loss: 1.6748 - val_acc: 0.4263
Epoch 23/50
 - 13s - loss: 1.5210 - acc: 0.5032 - val_loss: 1.6490 - val_acc: 0.4451
Epoch 24/50
 - 13s - loss: 1.4819 - acc: 0.5276 - val_loss: 1.6242 - val_acc: 0.4798
Epoch 25/50
 - 12s - loss: 1.4428 - acc: 0.5546 - val_loss: 1.5980 - val_acc: 0.5343
Epoch 26/50
 - 12s - loss: 1.4025 - acc: 0.5899 - val_loss: 1.5717 - val_acc: 0.5577
Epoch 27/50
 - 12s - loss: 1.3617 - acc: 0.6249 - val_loss: 1.5478 - val_acc: 0.5606
Epoch 28/50
 - 13s - loss: 1.3218 - acc: 0.6434 - val_loss: 1.5258 - val_acc: 0.5615
Epoch 29/50
 - 12s - loss: 1.2834 - acc: 0.6530 - val_loss: 1.5040 - val_acc: 0.5587
Epoch 30/50
 - 12s - loss: 1.2463 - acc: 0.6610 - val_loss: 1.4819 - val_acc: 0.5615
Epoch 31/50
 - 12s - loss: 1.2095 - acc: 0.6732 - val_loss: 1.4596 - val_acc: 0.5634
Epoch 32/50
 - 12s - loss: 1.1738 - acc: 0.6920 - val_loss: 1.4382 - val_acc: 0.5662
Epoch 33/50
 - 13s - loss: 1.1389 - acc: 0.7033 - val_loss: 1.4182 - val_acc: 0.5690
Epoch 34/50
 - 13s - loss: 1.1046 - acc: 0.7085 - val_loss: 1.3982 - val_acc: 0.5700
Epoch 35/50
 - 12s - loss: 1.0711 - acc: 0.7144 - val_loss: 1.3784 - val_acc: 0.5746
Epoch 36/50
 - 12s - loss: 1.0383 - acc: 0.7181 - val_loss: 1.3595 - val_acc: 0.5775
Epoch 37/50
 - 12s - loss: 1.0061 - acc: 0.7242 - val_loss: 1.3394 - val_acc: 0.5840
Epoch 38/50
 - 12s - loss: 0.9738 - acc: 0.7299 - val_loss: 1.3219 - val_acc: 0.5869
Epoch 39/50
 - 13s - loss: 0.9425 - acc: 0.7371 - val_loss: 1.3047 - val_acc: 0.5906
Epoch 40/50
 - 15s - loss: 0.9115 - acc: 0.7496 - val_loss: 1.2892 - val_acc: 0.5897
Epoch 41/50
 - 16s - loss: 0.8805 - acc: 0.7665 - val_loss: 1.2777 - val_acc: 0.5915
Epoch 42/50
 - 13s - loss: 0.8520 - acc: 0.7841 - val_loss: 1.2617 - val_acc: 0.6019
Epoch 43/50
 - 12s - loss: 0.8241 - acc: 0.7963 - val_loss: 1.2517 - val_acc: 0.6038
Epoch 44/50
 - 13s - loss: 0.7986 - acc: 0.8010 - val_loss: 1.2381 - val_acc: 0.6085
Epoch 45/50
 - 12s - loss: 0.7733 - acc: 0.8062 - val_loss: 1.2309 - val_acc: 0.6113
Epoch 46/50
 - 12s - loss: 0.7497 - acc: 0.8130 - val_loss: 1.2156 - val_acc: 0.6169
Epoch 47/50
 - 12s - loss: 0.7265 - acc: 0.8170 - val_loss: 1.2129 - val_acc: 0.6131
Epoch 48/50
 - 12s - loss: 0.7050 - acc: 0.8245 - val_loss: 1.1981 - val_acc: 0.6188
Epoch 49/50
 - 12s - loss: 0.6842 - acc: 0.8264 - val_loss: 1.1983 - val_acc: 0.6169
Epoch 50/50
 - 12s - loss: 0.6648 - acc: 0.8332 - val_loss: 1.1829 - val_acc: 0.6207
LSTM training time: 651.3965797424316s
Model: "sequential_17"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_7 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_33 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_34 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1631763295049757, 3.0834840877475607, 2.960648218118615, 2.829546902474871, 2.7092036012475247, 2.597637447863922, 2.499138120812991, 2.4105695649886987, 2.3288973371887924, 2.252019602843005, 2.178587997298524, 2.1036299460926107, 2.029471732485258, 1.9601305824561608, 1.896358486461841, 1.8369798217548965, 1.7823172507780047, 1.7325429839373252, 1.6864860161722393, 1.643045426339142, 1.6007328103035023, 1.5601507753784847, 1.5209572657694244, 1.481879826075935, 1.442819592018544, 1.4025111357623041, 1.3616964763981203, 1.3217698116943202, 1.28342891125884, 1.2462843174886222, 1.20951904413536, 1.1737675228280195, 1.1389032393406893, 1.1046027314615328, 1.0710994114275407, 1.0382551486037945, 1.0060624970722176, 0.9738174620888898, 0.942519622045642, 0.9115109942190127, 0.8805209446645046, 0.8520260348912554, 0.8240901862886105, 0.7986356872528798, 0.7733311275655125, 0.749733261884813, 0.7264717440454507, 0.7050255525201762, 0.6841609442852908, 0.664804062657522]
[0.112285644, 0.15362932, 0.15926708, 0.19638243, 0.2443035, 0.31054732, 0.33380315, 0.339206, 0.34296453, 0.3453136, 0.35071647, 0.35682404, 0.3610524, 0.36950904, 0.3749119, 0.3817242, 0.38994598, 0.3972281, 0.4136716, 0.4265915, 0.4463237, 0.4726333, 0.50317127, 0.5276016, 0.5546159, 0.58985204, 0.6248532, 0.64341086, 0.6530421, 0.6610289, 0.67324406, 0.6920366, 0.7033122, 0.7084802, 0.71435285, 0.71811134, 0.7242189, 0.7298567, 0.7371388, 0.7495889, 0.76650226, 0.78412026, 0.79633546, 0.8010336, 0.8062016, 0.81301385, 0.8170073, 0.8245243, 0.82640356, 0.8332159]
[3.12804448705324, 3.0194976623069514, 2.8787808716017316, 2.7543279665736526, 2.6458896690691023, 2.54806374943872, 2.459563300531235, 2.381174841733046, 2.3094025529046576, 2.2423197723890134, 2.1786061154844614, 2.115113801351735, 2.054380855202115, 1.9975930112068643, 1.9437023711316463, 1.8917883086092595, 1.8452967924691142, 1.8041056647546974, 1.767056517533853, 1.7325294032343117, 1.7025991355869132, 1.6747821910280576, 1.649027592587359, 1.6241997724407715, 1.5980150241807034, 1.5716748015421658, 1.5477900296869411, 1.5258333774239805, 1.5040401239350367, 1.4818878777709925, 1.4595556720321727, 1.4381991118892257, 1.4182238976160686, 1.3982330736419963, 1.3783966259217597, 1.359475499475506, 1.3394131893842993, 1.3218943910699494, 1.3047082002733794, 1.2891859826627472, 1.2776563880589087, 1.261742215649063, 1.2516771066636547, 1.2381160810519831, 1.2309273489484205, 1.215606286967864, 1.212924821751778, 1.198059573559694, 1.1982548559915291, 1.182887582921646]
[0.2065727710723877, 0.1605633795261383, 0.1605633795261383, 0.20563380420207977, 0.2535211145877838, 0.33051642775535583, 0.3502347469329834, 0.3539906144142151, 0.3502347469329834, 0.3502347469329834, 0.3539906144142151, 0.3577464818954468, 0.36150234937667847, 0.3718309998512268, 0.37746480107307434, 0.3849765360355377, 0.3924882709980011, 0.4046948254108429, 0.40657275915145874, 0.4018779397010803, 0.41596242785453796, 0.4262910783290863, 0.44507041573524475, 0.4798122048377991, 0.534272313117981, 0.5577464699745178, 0.5605633854866028, 0.5615023374557495, 0.5586854219436646, 0.5615023374557495, 0.5633803009986877, 0.5661971569061279, 0.5690140724182129, 0.5699530243873596, 0.5746479034423828, 0.577464759349823, 0.5840375423431396, 0.5868544578552246, 0.5906103253364563, 0.5896713733673096, 0.591549277305603, 0.6018779277801514, 0.6037558913230896, 0.608450710773468, 0.611267626285553, 0.6169013977050781, 0.6131455302238464, 0.6187793612480164, 0.6169013977050781, 0.6206572651863098]

  32/2898 [..............................] - ETA: 57s
 384/2898 [==>...........................] - ETA: 4s 
 736/2898 [======>.......................] - ETA: 2s
1088/2898 [==========>...................] - ETA: 1s
1440/2898 [=============>................] - ETA: 0s
1824/2898 [=================>............] - ETA: 0s
2176/2898 [=====================>........] - ETA: 0s
2528/2898 [=========================>....] - ETA: 0s
2880/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 366us/step
Accuracies per class for LSTM
[0.01 0.91 0.73 0.57 0.89 0.36 0.6   nan 0.96 0.18 0.   0.56 0.07 0.
 0.12 0.57 0.73 0.   0.    nan 0.76 0.79 0.   0.89]
[3.1631763295049757, 3.0834840877475607, 2.960648218118615, 2.829546902474871, 2.7092036012475247, 2.597637447863922, 2.499138120812991, 2.4105695649886987, 2.3288973371887924, 2.252019602843005, 2.178587997298524, 2.1036299460926107, 2.029471732485258, 1.9601305824561608, 1.896358486461841, 1.8369798217548965, 1.7823172507780047, 1.7325429839373252, 1.6864860161722393, 1.643045426339142, 1.6007328103035023, 1.5601507753784847, 1.5209572657694244, 1.481879826075935, 1.442819592018544, 1.4025111357623041, 1.3616964763981203, 1.3217698116943202, 1.28342891125884, 1.2462843174886222, 1.20951904413536, 1.1737675228280195, 1.1389032393406893, 1.1046027314615328, 1.0710994114275407, 1.0382551486037945, 1.0060624970722176, 0.9738174620888898, 0.942519622045642, 0.9115109942190127, 0.8805209446645046, 0.8520260348912554, 0.8240901862886105, 0.7986356872528798, 0.7733311275655125, 0.749733261884813, 0.7264717440454507, 0.7050255525201762, 0.6841609442852908, 0.664804062657522]
[0.112285644, 0.15362932, 0.15926708, 0.19638243, 0.2443035, 0.31054732, 0.33380315, 0.339206, 0.34296453, 0.3453136, 0.35071647, 0.35682404, 0.3610524, 0.36950904, 0.3749119, 0.3817242, 0.38994598, 0.3972281, 0.4136716, 0.4265915, 0.4463237, 0.4726333, 0.50317127, 0.5276016, 0.5546159, 0.58985204, 0.6248532, 0.64341086, 0.6530421, 0.6610289, 0.67324406, 0.6920366, 0.7033122, 0.7084802, 0.71435285, 0.71811134, 0.7242189, 0.7298567, 0.7371388, 0.7495889, 0.76650226, 0.78412026, 0.79633546, 0.8010336, 0.8062016, 0.81301385, 0.8170073, 0.8245243, 0.82640356, 0.8332159]
[3.12804448705324, 3.0194976623069514, 2.8787808716017316, 2.7543279665736526, 2.6458896690691023, 2.54806374943872, 2.459563300531235, 2.381174841733046, 2.3094025529046576, 2.2423197723890134, 2.1786061154844614, 2.115113801351735, 2.054380855202115, 1.9975930112068643, 1.9437023711316463, 1.8917883086092595, 1.8452967924691142, 1.8041056647546974, 1.767056517533853, 1.7325294032343117, 1.7025991355869132, 1.6747821910280576, 1.649027592587359, 1.6241997724407715, 1.5980150241807034, 1.5716748015421658, 1.5477900296869411, 1.5258333774239805, 1.5040401239350367, 1.4818878777709925, 1.4595556720321727, 1.4381991118892257, 1.4182238976160686, 1.3982330736419963, 1.3783966259217597, 1.359475499475506, 1.3394131893842993, 1.3218943910699494, 1.3047082002733794, 1.2891859826627472, 1.2776563880589087, 1.261742215649063, 1.2516771066636547, 1.2381160810519831, 1.2309273489484205, 1.215606286967864, 1.212924821751778, 1.198059573559694, 1.1982548559915291, 1.182887582921646]
[0.2065727710723877, 0.1605633795261383, 0.1605633795261383, 0.20563380420207977, 0.2535211145877838, 0.33051642775535583, 0.3502347469329834, 0.3539906144142151, 0.3502347469329834, 0.3502347469329834, 0.3539906144142151, 0.3577464818954468, 0.36150234937667847, 0.3718309998512268, 0.37746480107307434, 0.3849765360355377, 0.3924882709980011, 0.4046948254108429, 0.40657275915145874, 0.4018779397010803, 0.41596242785453796, 0.4262910783290863, 0.44507041573524475, 0.4798122048377991, 0.534272313117981, 0.5577464699745178, 0.5605633854866028, 0.5615023374557495, 0.5586854219436646, 0.5615023374557495, 0.5633803009986877, 0.5661971569061279, 0.5690140724182129, 0.5699530243873596, 0.5746479034423828, 0.577464759349823, 0.5840375423431396, 0.5868544578552246, 0.5906103253364563, 0.5896713733673096, 0.591549277305603, 0.6018779277801514, 0.6037558913230896, 0.608450710773468, 0.611267626285553, 0.6169013977050781, 0.6131455302238464, 0.6187793612480164, 0.6169013977050781, 0.6206572651863098]
recall 0.6018
precision 0.6323
f1 0.5755
mcc 0.5381
RMSE: 5.355
classification report:
              precision    recall  f1-score   support

           1       1.00      0.01      0.03        72
           2       0.33      0.91      0.49        68
           3       0.70      0.73      0.72       252
           4       0.82      0.57      0.67       132
           5       0.60      0.89      0.72       191
           6       0.29      0.36      0.32       150
           7       0.67      0.60      0.63       240
           8       0.00      0.00      0.00         0
           9       0.38      0.96      0.54       100
          10       0.48      0.18      0.26        61
          11       0.00      0.00      0.00        87
          12       0.85      0.56      0.67       595
          13       0.57      0.07      0.12        59
          14       0.00      0.00      0.00         6
          15       0.11      0.12      0.11        72
          16       0.81      0.57      0.67        30
          17       0.45      0.73      0.55       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          20       0.00      0.00      0.00         0
          21       0.87      0.76      0.81       222
          22       0.53      0.79      0.63       168
          23       0.00      0.00      0.00        65
          24       0.53      0.89      0.67        82

   micro avg       0.57      0.57      0.57      2898
   macro avg       0.42      0.40      0.36      2898
weighted avg       0.60      0.57      0.54      2898

>#7: 56.832
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1458 - acc: 0.0904 - val_loss: 3.0908 - val_acc: 0.0967
Epoch 2/50
 - 12s - loss: 3.0585 - acc: 0.1158 - val_loss: 2.9896 - val_acc: 0.1146
Epoch 3/50
 - 12s - loss: 2.9436 - acc: 0.1344 - val_loss: 2.8574 - val_acc: 0.1371
Epoch 4/50
 - 12s - loss: 2.8195 - acc: 0.1442 - val_loss: 2.7207 - val_acc: 0.1437
Epoch 5/50
 - 12s - loss: 2.6954 - acc: 0.1478 - val_loss: 2.6095 - val_acc: 0.1446
Epoch 6/50
 - 12s - loss: 2.5854 - acc: 0.1802 - val_loss: 2.5211 - val_acc: 0.1962
Epoch 7/50
 - 12s - loss: 2.4923 - acc: 0.2138 - val_loss: 2.4609 - val_acc: 0.1803
Epoch 8/50
 - 12s - loss: 2.4148 - acc: 0.2145 - val_loss: 2.4196 - val_acc: 0.1812
Epoch 9/50
 - 12s - loss: 2.3498 - acc: 0.2140 - val_loss: 2.3815 - val_acc: 0.1831
Epoch 10/50
 - 12s - loss: 2.2954 - acc: 0.2140 - val_loss: 2.3563 - val_acc: 0.1803
Epoch 11/50
 - 12s - loss: 2.2449 - acc: 0.2154 - val_loss: 2.3267 - val_acc: 0.1840
Epoch 12/50
 - 12s - loss: 2.1892 - acc: 0.2203 - val_loss: 2.2893 - val_acc: 0.1915
Epoch 13/50
 - 12s - loss: 2.1343 - acc: 0.2373 - val_loss: 2.2603 - val_acc: 0.2009
Epoch 14/50
 - 12s - loss: 2.0755 - acc: 0.2504 - val_loss: 2.2346 - val_acc: 0.2113
Epoch 15/50
 - 12s - loss: 2.0226 - acc: 0.2701 - val_loss: 2.2056 - val_acc: 0.2225
Epoch 16/50
 - 12s - loss: 1.9672 - acc: 0.3056 - val_loss: 2.1869 - val_acc: 0.2432
Epoch 17/50
 - 12s - loss: 1.9167 - acc: 0.3467 - val_loss: 2.1608 - val_acc: 0.2667
Epoch 18/50
 - 12s - loss: 1.8683 - acc: 0.3824 - val_loss: 2.1415 - val_acc: 0.2920
Epoch 19/50
 - 12s - loss: 1.8221 - acc: 0.4040 - val_loss: 2.1184 - val_acc: 0.3033
Epoch 20/50
 - 13s - loss: 1.7759 - acc: 0.4264 - val_loss: 2.1095 - val_acc: 0.3023
Epoch 21/50
 - 13s - loss: 1.7300 - acc: 0.4475 - val_loss: 2.1047 - val_acc: 0.3033
Epoch 22/50
 - 13s - loss: 1.6849 - acc: 0.4632 - val_loss: 2.1017 - val_acc: 0.3136
Epoch 23/50
 - 13s - loss: 1.6377 - acc: 0.4825 - val_loss: 2.1010 - val_acc: 0.3174
Epoch 24/50
 - 13s - loss: 1.5913 - acc: 0.5121 - val_loss: 2.0903 - val_acc: 0.3268
Epoch 25/50
 - 13s - loss: 1.5470 - acc: 0.5492 - val_loss: 2.0876 - val_acc: 0.3437
Epoch 26/50
 - 13s - loss: 1.5010 - acc: 0.5748 - val_loss: 2.0843 - val_acc: 0.3549
Epoch 27/50
 - 13s - loss: 1.4572 - acc: 0.5978 - val_loss: 2.0789 - val_acc: 0.3643
Epoch 28/50
 - 12s - loss: 1.4164 - acc: 0.6126 - val_loss: 2.0731 - val_acc: 0.3756
Epoch 29/50
 - 13s - loss: 1.3775 - acc: 0.6234 - val_loss: 2.0677 - val_acc: 0.3859
Epoch 30/50
 - 13s - loss: 1.3415 - acc: 0.6368 - val_loss: 2.0626 - val_acc: 0.3915
Epoch 31/50
 - 13s - loss: 1.3056 - acc: 0.6502 - val_loss: 2.0452 - val_acc: 0.3944
Epoch 32/50
 - 12s - loss: 1.2727 - acc: 0.6638 - val_loss: 2.0447 - val_acc: 0.4000
Epoch 33/50
 - 13s - loss: 1.2385 - acc: 0.6808 - val_loss: 2.0188 - val_acc: 0.4131
Epoch 34/50
 - 12s - loss: 1.2070 - acc: 0.6930 - val_loss: 1.9888 - val_acc: 0.4366
Epoch 35/50
 - 12s - loss: 1.1694 - acc: 0.7052 - val_loss: 1.9551 - val_acc: 0.4469
Epoch 36/50
 - 12s - loss: 1.1332 - acc: 0.7153 - val_loss: 1.9497 - val_acc: 0.4460
Epoch 37/50
 - 20s - loss: 1.1005 - acc: 0.7221 - val_loss: 1.9366 - val_acc: 0.4507
Epoch 38/50
 - 15s - loss: 1.0694 - acc: 0.7348 - val_loss: 1.9231 - val_acc: 0.4563
Epoch 39/50
 - 13s - loss: 1.0384 - acc: 0.7456 - val_loss: 1.9160 - val_acc: 0.4516
Epoch 40/50
 - 12s - loss: 1.0092 - acc: 0.7515 - val_loss: 1.9110 - val_acc: 0.4451
Epoch 41/50
 - 12s - loss: 0.9805 - acc: 0.7597 - val_loss: 1.9056 - val_acc: 0.4460
Epoch 42/50
 - 13s - loss: 0.9519 - acc: 0.7679 - val_loss: 1.8935 - val_acc: 0.4516
Epoch 43/50
 - 25s - loss: 0.9250 - acc: 0.7764 - val_loss: 1.8788 - val_acc: 0.4563
Epoch 44/50
 - 12s - loss: 0.8960 - acc: 0.7794 - val_loss: 1.8813 - val_acc: 0.4601
Epoch 45/50
 - 12s - loss: 0.8679 - acc: 0.7869 - val_loss: 1.8698 - val_acc: 0.4638
Epoch 46/50
 - 35s - loss: 0.8408 - acc: 0.7942 - val_loss: 1.8667 - val_acc: 0.4573
Epoch 47/50
 - 39s - loss: 0.8139 - acc: 0.8015 - val_loss: 1.8612 - val_acc: 0.4573
Epoch 48/50
 - 38s - loss: 0.7892 - acc: 0.8076 - val_loss: 1.8608 - val_acc: 0.4545
Epoch 49/50
 - 38s - loss: 0.7655 - acc: 0.8116 - val_loss: 1.8590 - val_acc: 0.4554
Epoch 50/50
 - 39s - loss: 0.7434 - acc: 0.8196 - val_loss: 1.8466 - val_acc: 0.4545
LSTM training time: 776.579140663147s
Model: "sequential_18"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_8 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_35 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_36 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1458274420365666, 3.0584558935996538, 2.9436354713314485, 2.819527155977195, 2.695362286414231, 2.585408123272642, 2.4922953287088343, 2.414802093474496, 2.349837989574143, 2.2953722357105866, 2.2449223816548822, 2.1892452922706345, 2.1343466975696215, 2.0754917149591927, 2.0225956725603766, 1.9671976706050276, 1.916730470015517, 1.86831361583233, 1.8221375647917415, 1.7758620196394665, 1.7299563183536937, 1.6849169844272749, 1.6376765827799953, 1.591310972573193, 1.54701277955978, 1.5010094887106413, 1.457246091560716, 1.4164369840114532, 1.3775214392981507, 1.3415447993403957, 1.3055697362524314, 1.2727263085657872, 1.2385406478108838, 1.2070032913751492, 1.169357754281353, 1.133186629436261, 1.100544780219752, 1.0693536383153017, 1.0383566588142374, 1.0091576009029677, 0.9804949086681901, 0.9518677585976825, 0.9249531048996952, 0.8959697227971443, 0.867906879337007, 0.8408399182031601, 0.8138672232571862, 0.7891531529590673, 0.7654646455170488, 0.7433596360332002]
[0.090439275, 0.115809254, 0.13436693, 0.14423303, 0.14775664, 0.18017383, 0.21376556, 0.21447028, 0.21400046, 0.21400046, 0.21540992, 0.22034296, 0.23725629, 0.2504111, 0.2701433, 0.3056143, 0.34672305, 0.38242894, 0.4040404, 0.42635658, 0.44749823, 0.46323702, 0.48249942, 0.5120977, 0.54921305, 0.57481796, 0.5978389, 0.612638, 0.6234437, 0.6368334, 0.65022314, 0.6638478, 0.6807611, 0.6929763, 0.70519143, 0.71529245, 0.7221048, 0.7347897, 0.7455955, 0.7514682, 0.7596899, 0.7679117, 0.7763683, 0.7794221, 0.78693914, 0.7942213, 0.8015034, 0.807611, 0.81160444, 0.8195913]
[3.0907898426055906, 2.989591878120888, 2.8574189927096656, 2.720692659655647, 2.6094639625907505, 2.5211060447871967, 2.460907270315108, 2.419635521078334, 2.3815176321307256, 2.356293963938252, 2.3266822931352356, 2.2893308858916233, 2.2603273763343203, 2.234601351240991, 2.2056289666135545, 2.18688381736827, 2.1607777604474707, 2.1414504841459747, 2.1184226196136833, 2.1094690368768756, 2.1047091634060857, 2.1017254527186005, 2.101007527700612, 2.0903383525884207, 2.087610604617517, 2.0842579105650314, 2.078915338784876, 2.0730826594460177, 2.067679786626162, 2.062615368008054, 2.0452403827452326, 2.0446690047850633, 2.0187909899183283, 1.9887976772348646, 1.9551010648409526, 1.9497458975919535, 1.9366009221950047, 1.9230980252995737, 1.9160334555475924, 1.9110117744392072, 1.9056191062703378, 1.8934517470883652, 1.8788467637250121, 1.881343242260212, 1.869836855886128, 1.8666973536283198, 1.8611686293927716, 1.8608393261690095, 1.8589630269668471, 1.8466405076040349]
[0.09671361744403839, 0.11455398797988892, 0.13708920776844025, 0.1436619758605957, 0.14460094273090363, 0.19624413549900055, 0.18028168380260468, 0.1812206506729126, 0.18309858441352844, 0.18028168380260468, 0.18403755128383636, 0.19154930114746094, 0.20093896985054016, 0.2112676054239273, 0.22253520786762238, 0.24319249391555786, 0.2666666805744171, 0.2920187711715698, 0.3032863736152649, 0.30234742164611816, 0.3032863736152649, 0.31361502408981323, 0.3173708915710449, 0.32676056027412415, 0.34366196393966675, 0.3549295663833618, 0.36431923508644104, 0.3755868673324585, 0.38591548800468445, 0.391549289226532, 0.39436620473861694, 0.4000000059604645, 0.4131455421447754, 0.43661972880363464, 0.4469483494758606, 0.44600939750671387, 0.4507042169570923, 0.4563380181789398, 0.4516431987285614, 0.44507041573524475, 0.44600939750671387, 0.4516431987285614, 0.4563380181789398, 0.4600938856601715, 0.4638497531414032, 0.45727699995040894, 0.45727699995040894, 0.454460084438324, 0.4553990662097931, 0.454460084438324]

  32/2898 [..............................] - ETA: 2:57
 160/2898 [>.............................] - ETA: 35s 
 288/2898 [=>............................] - ETA: 19s
 448/2898 [===>..........................] - ETA: 11s
 576/2898 [====>.........................] - ETA: 9s 
 704/2898 [======>.......................] - ETA: 7s
 864/2898 [=======>......................] - ETA: 5s
 992/2898 [=========>....................] - ETA: 4s
1120/2898 [==========>...................] - ETA: 3s
1248/2898 [===========>..................] - ETA: 3s
1376/2898 [=============>................] - ETA: 2s
1536/2898 [==============>...............] - ETA: 2s
1728/2898 [================>.............] - ETA: 1s
1888/2898 [==================>...........] - ETA: 1s
2016/2898 [===================>..........] - ETA: 1s
2208/2898 [=====================>........] - ETA: 0s
2336/2898 [=======================>......] - ETA: 0s
2464/2898 [========================>.....] - ETA: 0s
2560/2898 [=========================>....] - ETA: 0s
2656/2898 [==========================>...] - ETA: 0s
2784/2898 [===========================>..] - ETA: 0s
2848/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 3s 1ms/step
Accuracies per class for LSTM
[0.31 0.71 0.95 0.55 0.43 0.64 0.81  nan 0.89 0.   0.   0.58 0.2  0.
 0.33 0.3  0.83 0.   0.   0.93 0.84 0.   0.74]
[3.1458274420365666, 3.0584558935996538, 2.9436354713314485, 2.819527155977195, 2.695362286414231, 2.585408123272642, 2.4922953287088343, 2.414802093474496, 2.349837989574143, 2.2953722357105866, 2.2449223816548822, 2.1892452922706345, 2.1343466975696215, 2.0754917149591927, 2.0225956725603766, 1.9671976706050276, 1.916730470015517, 1.86831361583233, 1.8221375647917415, 1.7758620196394665, 1.7299563183536937, 1.6849169844272749, 1.6376765827799953, 1.591310972573193, 1.54701277955978, 1.5010094887106413, 1.457246091560716, 1.4164369840114532, 1.3775214392981507, 1.3415447993403957, 1.3055697362524314, 1.2727263085657872, 1.2385406478108838, 1.2070032913751492, 1.169357754281353, 1.133186629436261, 1.100544780219752, 1.0693536383153017, 1.0383566588142374, 1.0091576009029677, 0.9804949086681901, 0.9518677585976825, 0.9249531048996952, 0.8959697227971443, 0.867906879337007, 0.8408399182031601, 0.8138672232571862, 0.7891531529590673, 0.7654646455170488, 0.7433596360332002]
[0.090439275, 0.115809254, 0.13436693, 0.14423303, 0.14775664, 0.18017383, 0.21376556, 0.21447028, 0.21400046, 0.21400046, 0.21540992, 0.22034296, 0.23725629, 0.2504111, 0.2701433, 0.3056143, 0.34672305, 0.38242894, 0.4040404, 0.42635658, 0.44749823, 0.46323702, 0.48249942, 0.5120977, 0.54921305, 0.57481796, 0.5978389, 0.612638, 0.6234437, 0.6368334, 0.65022314, 0.6638478, 0.6807611, 0.6929763, 0.70519143, 0.71529245, 0.7221048, 0.7347897, 0.7455955, 0.7514682, 0.7596899, 0.7679117, 0.7763683, 0.7794221, 0.78693914, 0.7942213, 0.8015034, 0.807611, 0.81160444, 0.8195913]
[3.0907898426055906, 2.989591878120888, 2.8574189927096656, 2.720692659655647, 2.6094639625907505, 2.5211060447871967, 2.460907270315108, 2.419635521078334, 2.3815176321307256, 2.356293963938252, 2.3266822931352356, 2.2893308858916233, 2.2603273763343203, 2.234601351240991, 2.2056289666135545, 2.18688381736827, 2.1607777604474707, 2.1414504841459747, 2.1184226196136833, 2.1094690368768756, 2.1047091634060857, 2.1017254527186005, 2.101007527700612, 2.0903383525884207, 2.087610604617517, 2.0842579105650314, 2.078915338784876, 2.0730826594460177, 2.067679786626162, 2.062615368008054, 2.0452403827452326, 2.0446690047850633, 2.0187909899183283, 1.9887976772348646, 1.9551010648409526, 1.9497458975919535, 1.9366009221950047, 1.9230980252995737, 1.9160334555475924, 1.9110117744392072, 1.9056191062703378, 1.8934517470883652, 1.8788467637250121, 1.881343242260212, 1.869836855886128, 1.8666973536283198, 1.8611686293927716, 1.8608393261690095, 1.8589630269668471, 1.8466405076040349]
[0.09671361744403839, 0.11455398797988892, 0.13708920776844025, 0.1436619758605957, 0.14460094273090363, 0.19624413549900055, 0.18028168380260468, 0.1812206506729126, 0.18309858441352844, 0.18028168380260468, 0.18403755128383636, 0.19154930114746094, 0.20093896985054016, 0.2112676054239273, 0.22253520786762238, 0.24319249391555786, 0.2666666805744171, 0.2920187711715698, 0.3032863736152649, 0.30234742164611816, 0.3032863736152649, 0.31361502408981323, 0.3173708915710449, 0.32676056027412415, 0.34366196393966675, 0.3549295663833618, 0.36431923508644104, 0.3755868673324585, 0.38591548800468445, 0.391549289226532, 0.39436620473861694, 0.4000000059604645, 0.4131455421447754, 0.43661972880363464, 0.4469483494758606, 0.44600939750671387, 0.4507042169570923, 0.4563380181789398, 0.4516431987285614, 0.44507041573524475, 0.44600939750671387, 0.4516431987285614, 0.4563380181789398, 0.4600938856601715, 0.4638497531414032, 0.45727699995040894, 0.45727699995040894, 0.454460084438324, 0.4553990662097931, 0.454460084438324]
recall 0.6679
precision 0.6665
f1 0.6354
mcc 0.5838
RMSE: 5.340
classification report:
              precision    recall  f1-score   support

           1       0.42      0.31      0.35        72
           2       0.25      0.71      0.37        68
           3       0.72      0.95      0.82       252
           4       0.79      0.55      0.65       132
           5       0.87      0.43      0.58       191
           6       0.77      0.64      0.70       150
           7       0.81      0.81      0.81       240
           8       0.00      0.00      0.00         0
           9       0.66      0.89      0.76       100
          10       0.00      0.00      0.00        61
          11       0.00      0.00      0.00        87
          12       0.83      0.58      0.68       595
          13       0.55      0.20      0.30        59
          14       0.00      0.00      0.00         6
          15       0.29      0.33      0.31        72
          16       0.56      0.30      0.39        30
          17       0.45      0.83      0.59       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.42      0.93      0.58       222
          22       0.68      0.84      0.75       168
          23       0.00      0.00      0.00        65
          24       0.56      0.74      0.64        82

   micro avg       0.61      0.61      0.61      2898
   macro avg       0.42      0.44      0.40      2898
weighted avg       0.61      0.61      0.58      2898

>#8: 61.077
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 39s - loss: 3.1543 - acc: 0.0470 - val_loss: 3.0933 - val_acc: 0.1268
Epoch 2/50
 - 41s - loss: 3.0689 - acc: 0.1247 - val_loss: 3.0170 - val_acc: 0.1418
Epoch 3/50
 - 41s - loss: 2.9808 - acc: 0.1358 - val_loss: 2.9259 - val_acc: 0.1324
Epoch 4/50
 - 41s - loss: 2.8700 - acc: 0.1736 - val_loss: 2.8194 - val_acc: 0.1897
Epoch 5/50
 - 42s - loss: 2.7425 - acc: 0.2077 - val_loss: 2.7062 - val_acc: 0.1484
Epoch 6/50
 - 41s - loss: 2.6140 - acc: 0.2300 - val_loss: 2.5994 - val_acc: 0.1512
Epoch 7/50
 - 41s - loss: 2.4970 - acc: 0.2532 - val_loss: 2.5102 - val_acc: 0.1624
Epoch 8/50
 - 40s - loss: 2.3954 - acc: 0.2690 - val_loss: 2.4350 - val_acc: 0.1700
Epoch 9/50
 - 31s - loss: 2.3079 - acc: 0.2821 - val_loss: 2.3719 - val_acc: 0.1775
Epoch 10/50
 - 14s - loss: 2.2309 - acc: 0.3002 - val_loss: 2.3184 - val_acc: 0.1944
Epoch 11/50
 - 12s - loss: 2.1605 - acc: 0.3305 - val_loss: 2.2676 - val_acc: 0.2272
Epoch 12/50
 - 12s - loss: 2.0955 - acc: 0.3568 - val_loss: 2.2250 - val_acc: 0.2657
Epoch 13/50
 - 12s - loss: 2.0346 - acc: 0.3834 - val_loss: 2.1848 - val_acc: 0.2930
Epoch 14/50
 - 12s - loss: 1.9759 - acc: 0.4080 - val_loss: 2.1482 - val_acc: 0.3080
Epoch 15/50
 - 12s - loss: 1.9195 - acc: 0.4240 - val_loss: 2.1171 - val_acc: 0.3408
Epoch 16/50
 - 12s - loss: 1.8644 - acc: 0.4381 - val_loss: 2.0884 - val_acc: 0.3681
Epoch 17/50
 - 15s - loss: 1.8121 - acc: 0.4571 - val_loss: 2.0677 - val_acc: 0.3793
Epoch 18/50
 - 18s - loss: 1.7617 - acc: 0.4700 - val_loss: 2.0438 - val_acc: 0.4094
Epoch 19/50
 - 15s - loss: 1.7082 - acc: 0.4947 - val_loss: 2.0199 - val_acc: 0.4225
Epoch 20/50
 - 13s - loss: 1.6572 - acc: 0.5140 - val_loss: 2.0010 - val_acc: 0.4338
Epoch 21/50
 - 13s - loss: 1.6094 - acc: 0.5304 - val_loss: 1.9828 - val_acc: 0.4460
Epoch 22/50
 - 13s - loss: 1.5635 - acc: 0.5431 - val_loss: 1.9637 - val_acc: 0.4488
Epoch 23/50
 - 14s - loss: 1.5186 - acc: 0.5537 - val_loss: 1.9471 - val_acc: 0.4526
Epoch 24/50
 - 13s - loss: 1.4753 - acc: 0.5633 - val_loss: 1.9778 - val_acc: 0.4291
Epoch 25/50
 - 12s - loss: 1.4328 - acc: 0.5739 - val_loss: 1.9719 - val_acc: 0.4263
Epoch 26/50
 - 12s - loss: 1.3902 - acc: 0.5856 - val_loss: 1.9286 - val_acc: 0.4347
Epoch 27/50
 - 12s - loss: 1.3497 - acc: 0.5992 - val_loss: 1.8965 - val_acc: 0.4460
Epoch 28/50
 - 12s - loss: 1.3106 - acc: 0.6122 - val_loss: 1.8772 - val_acc: 0.4592
Epoch 29/50
 - 12s - loss: 1.2733 - acc: 0.6274 - val_loss: 1.8612 - val_acc: 0.4667
Epoch 30/50
 - 12s - loss: 1.2379 - acc: 0.6458 - val_loss: 1.8447 - val_acc: 0.4695
Epoch 31/50
 - 12s - loss: 1.2039 - acc: 0.6596 - val_loss: 1.8304 - val_acc: 0.4676
Epoch 32/50
 - 12s - loss: 1.1711 - acc: 0.6730 - val_loss: 1.8183 - val_acc: 0.4714
Epoch 33/50
 - 12s - loss: 1.1395 - acc: 0.6852 - val_loss: 1.8093 - val_acc: 0.4704
Epoch 34/50
 - 12s - loss: 1.1090 - acc: 0.6958 - val_loss: 1.8009 - val_acc: 0.4826
Epoch 35/50
 - 12s - loss: 1.0789 - acc: 0.7054 - val_loss: 1.8007 - val_acc: 0.4808
Epoch 36/50
 - 12s - loss: 1.0499 - acc: 0.7169 - val_loss: 1.7935 - val_acc: 0.5033
Epoch 37/50
 - 12s - loss: 1.0226 - acc: 0.7228 - val_loss: 1.8064 - val_acc: 0.4836
Epoch 38/50
 - 12s - loss: 0.9948 - acc: 0.7287 - val_loss: 1.7880 - val_acc: 0.5164
Epoch 39/50
 - 12s - loss: 0.9705 - acc: 0.7367 - val_loss: 1.8041 - val_acc: 0.4939
Epoch 40/50
 - 12s - loss: 0.9444 - acc: 0.7425 - val_loss: 1.8042 - val_acc: 0.5033
Epoch 41/50
 - 12s - loss: 0.9205 - acc: 0.7508 - val_loss: 1.7937 - val_acc: 0.5174
Epoch 42/50
 - 12s - loss: 0.8988 - acc: 0.7529 - val_loss: 1.8147 - val_acc: 0.5061
Epoch 43/50
 - 12s - loss: 0.8759 - acc: 0.7576 - val_loss: 1.8078 - val_acc: 0.5174
Epoch 00043: early stopping
LSTM training time: 803.4305119514465s
Model: "sequential_19"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_9 (LSTM)                (None, 8)                 2944      
_________________________________________________________________
dense_37 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_38 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.154349256183498, 3.068897903875647, 2.9807948601310286, 2.87002258298541, 2.7425090144827022, 2.6140473004319396, 2.497013147319178, 2.3954272550233853, 2.307880114187286, 2.230861104484004, 2.160518055542383, 2.095453709146913, 2.034574033902065, 1.9759354675378995, 1.9194754665940195, 1.8644436356157155, 1.8120610215503172, 1.7616870327681367, 1.708214461145363, 1.6571760563495324, 1.6093739031737249, 1.5634903248582392, 1.5186119560123414, 1.4752904465424221, 1.4327850217463014, 1.3901577534349643, 1.3496534435127945, 1.3106043053983887, 1.2732973514881274, 1.2379373876959443, 1.2039424823936173, 1.1711456096449564, 1.139538978351403, 1.1089943208435709, 1.0789395160229238, 1.049921017732591, 1.022583247253193, 0.9948053622514619, 0.9704658322654085, 0.9444013992427296, 0.9204540205780318, 0.8988346181720642, 0.8758968835163206]
[0.046981443, 0.12473573, 0.13577637, 0.17359643, 0.20765798, 0.22997417, 0.25322998, 0.26896876, 0.28212357, 0.30021143, 0.33051446, 0.35682404, 0.38336858, 0.40803382, 0.4240075, 0.43810195, 0.45712945, 0.47004932, 0.4947146, 0.513977, 0.5304205, 0.5431055, 0.5536763, 0.56330746, 0.5738783, 0.5856237, 0.5992483, 0.6121682, 0.6274372, 0.64575994, 0.65961945, 0.67300916, 0.68522435, 0.6957952, 0.70542634, 0.7169368, 0.7228095, 0.72868216, 0.736669, 0.7425417, 0.7507635, 0.7528776, 0.75757575]
[3.093276499134834, 3.0170400682189653, 2.9258695400936503, 2.819382617842983, 2.706172816406393, 2.599402321336415, 2.5101621112913035, 2.434974922260768, 2.371921004487875, 2.318441840292702, 2.2675610797505983, 2.22499780834001, 2.1848172355705584, 2.148184109853467, 2.117120426025749, 2.088415085653744, 2.0676516176949087, 2.0438133730015284, 2.0199486040733228, 2.0009973136472032, 1.9828295342799083, 1.963680931995732, 1.9471022393222146, 1.9777561339973844, 1.971948134731239, 1.92863055775423, 1.8965192466834342, 1.8772232679134244, 1.8611920698147983, 1.8447307124384131, 1.830351596035308, 1.8183183841302362, 1.8093344644761422, 1.800851072000226, 1.8006955182048636, 1.7934916172788737, 1.8064095349378988, 1.787977020897216, 1.804067367846977, 1.8041706020283588, 1.793678695439173, 1.814722728589331, 1.8078448662735487]
[0.1267605572938919, 0.14178404211997986, 0.13239437341690063, 0.1896713674068451, 0.14835681021213531, 0.15117371082305908, 0.16244131326675415, 0.16995304822921753, 0.1774647831916809, 0.1943662017583847, 0.227230042219162, 0.265727698802948, 0.29295775294303894, 0.3079812228679657, 0.3408450782299042, 0.36807510256767273, 0.3793427348136902, 0.4093896746635437, 0.4225352108478546, 0.4338028132915497, 0.44600939750671387, 0.44882628321647644, 0.45258215069770813, 0.42910799384117126, 0.4262910783290863, 0.4347417950630188, 0.44600939750671387, 0.4591549336910248, 0.46666666865348816, 0.46948355436325073, 0.4676056206226349, 0.4713614881038666, 0.47042253613471985, 0.48262912034988403, 0.4807511866092682, 0.5032863616943359, 0.48356807231903076, 0.5164319276809692, 0.4938967227935791, 0.5032863616943359, 0.517370879650116, 0.5061032772064209, 0.517370879650116]

  32/2898 [..............................] - ETA: 1:07
 320/2898 [==>...........................] - ETA: 6s  
 704/2898 [======>.......................] - ETA: 2s
1088/2898 [==========>...................] - ETA: 1s
1472/2898 [==============>...............] - ETA: 0s
1856/2898 [==================>...........] - ETA: 0s
2240/2898 [======================>.......] - ETA: 0s
2592/2898 [=========================>....] - ETA: 0s
2898/2898 [==============================] - 1s 407us/step
Accuracies per class for LSTM
[0.   0.72 0.24 0.02 0.76 0.52 0.84 0.98 0.   0.   0.2  0.   0.   0.35
 0.33 0.63 0.03 0.   0.   0.8  0.   0.74]
[3.154349256183498, 3.068897903875647, 2.9807948601310286, 2.87002258298541, 2.7425090144827022, 2.6140473004319396, 2.497013147319178, 2.3954272550233853, 2.307880114187286, 2.230861104484004, 2.160518055542383, 2.095453709146913, 2.034574033902065, 1.9759354675378995, 1.9194754665940195, 1.8644436356157155, 1.8120610215503172, 1.7616870327681367, 1.708214461145363, 1.6571760563495324, 1.6093739031737249, 1.5634903248582392, 1.5186119560123414, 1.4752904465424221, 1.4327850217463014, 1.3901577534349643, 1.3496534435127945, 1.3106043053983887, 1.2732973514881274, 1.2379373876959443, 1.2039424823936173, 1.1711456096449564, 1.139538978351403, 1.1089943208435709, 1.0789395160229238, 1.049921017732591, 1.022583247253193, 0.9948053622514619, 0.9704658322654085, 0.9444013992427296, 0.9204540205780318, 0.8988346181720642, 0.8758968835163206]
[0.046981443, 0.12473573, 0.13577637, 0.17359643, 0.20765798, 0.22997417, 0.25322998, 0.26896876, 0.28212357, 0.30021143, 0.33051446, 0.35682404, 0.38336858, 0.40803382, 0.4240075, 0.43810195, 0.45712945, 0.47004932, 0.4947146, 0.513977, 0.5304205, 0.5431055, 0.5536763, 0.56330746, 0.5738783, 0.5856237, 0.5992483, 0.6121682, 0.6274372, 0.64575994, 0.65961945, 0.67300916, 0.68522435, 0.6957952, 0.70542634, 0.7169368, 0.7228095, 0.72868216, 0.736669, 0.7425417, 0.7507635, 0.7528776, 0.75757575]
[3.093276499134834, 3.0170400682189653, 2.9258695400936503, 2.819382617842983, 2.706172816406393, 2.599402321336415, 2.5101621112913035, 2.434974922260768, 2.371921004487875, 2.318441840292702, 2.2675610797505983, 2.22499780834001, 2.1848172355705584, 2.148184109853467, 2.117120426025749, 2.088415085653744, 2.0676516176949087, 2.0438133730015284, 2.0199486040733228, 2.0009973136472032, 1.9828295342799083, 1.963680931995732, 1.9471022393222146, 1.9777561339973844, 1.971948134731239, 1.92863055775423, 1.8965192466834342, 1.8772232679134244, 1.8611920698147983, 1.8447307124384131, 1.830351596035308, 1.8183183841302362, 1.8093344644761422, 1.800851072000226, 1.8006955182048636, 1.7934916172788737, 1.8064095349378988, 1.787977020897216, 1.804067367846977, 1.8041706020283588, 1.793678695439173, 1.814722728589331, 1.8078448662735487]
[0.1267605572938919, 0.14178404211997986, 0.13239437341690063, 0.1896713674068451, 0.14835681021213531, 0.15117371082305908, 0.16244131326675415, 0.16995304822921753, 0.1774647831916809, 0.1943662017583847, 0.227230042219162, 0.265727698802948, 0.29295775294303894, 0.3079812228679657, 0.3408450782299042, 0.36807510256767273, 0.3793427348136902, 0.4093896746635437, 0.4225352108478546, 0.4338028132915497, 0.44600939750671387, 0.44882628321647644, 0.45258215069770813, 0.42910799384117126, 0.4262910783290863, 0.4347417950630188, 0.44600939750671387, 0.4591549336910248, 0.46666666865348816, 0.46948355436325073, 0.4676056206226349, 0.4713614881038666, 0.47042253613471985, 0.48262912034988403, 0.4807511866092682, 0.5032863616943359, 0.48356807231903076, 0.5164319276809692, 0.4938967227935791, 0.5032863616943359, 0.517370879650116, 0.5061032772064209, 0.517370879650116]
recall 0.4354
precision 0.3906
f1 0.3617
mcc 0.3408
RMSE: 6.577
classification report:
              precision    recall  f1-score   support

           1       0.00      0.00      0.00        72
           2       0.27      0.72      0.39        68
           3       0.40      0.24      0.30       252
           4       0.02      0.02      0.02       132
           5       0.27      0.76      0.40       191
           6       0.23      0.52      0.32       150
           7       0.56      0.84      0.67       240
           9       0.31      0.98      0.47       100
          10       0.00      0.00      0.00        61
          11       0.00      0.00      0.00        87
          12       0.56      0.20      0.30       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.64      0.35      0.45        72
          16       0.24      0.33      0.28        30
          17       0.33      0.63      0.43       156
          18       0.07      0.03      0.04        36
          19       0.00      0.00      0.00        54
          21       0.00      0.00      0.00       222
          22       0.70      0.80      0.75       168
          23       0.00      0.00      0.00        65
          24       0.53      0.74      0.62        82

   micro avg       0.37      0.37      0.37      2898
   macro avg       0.23      0.33      0.25      2898
weighted avg       0.34      0.37      0.31      2898

>#9: 37.474
Train on 4257 samples, validate on 1065 samples
Epoch 1/50
 - 13s - loss: 3.1675 - acc: 0.0296 - val_loss: 3.1265 - val_acc: 0.0479
Epoch 2/50
 - 12s - loss: 3.1042 - acc: 0.0660 - val_loss: 3.0421 - val_acc: 0.1192
Epoch 3/50
 - 12s - loss: 3.0042 - acc: 0.1712 - val_loss: 2.9213 - val_acc: 0.1831
Epoch 4/50
 - 12s - loss: 2.8675 - acc: 0.2213 - val_loss: 2.7872 - val_acc: 0.2160
Epoch 5/50
 - 12s - loss: 2.7227 - acc: 0.2149 - val_loss: 2.6528 - val_acc: 0.2122
Epoch 6/50
 - 12s - loss: 2.5908 - acc: 0.2236 - val_loss: 2.5379 - val_acc: 0.2244
Epoch 7/50
 - 12s - loss: 2.4804 - acc: 0.2349 - val_loss: 2.4453 - val_acc: 0.2282
Epoch 8/50
 - 12s - loss: 2.3859 - acc: 0.2452 - val_loss: 2.3644 - val_acc: 0.2319
Epoch 9/50
 - 12s - loss: 2.3020 - acc: 0.2586 - val_loss: 2.2928 - val_acc: 0.2423
Epoch 10/50
 - 12s - loss: 2.2269 - acc: 0.2746 - val_loss: 2.2277 - val_acc: 0.2516
Epoch 11/50
 - 12s - loss: 2.1503 - acc: 0.2878 - val_loss: 2.1651 - val_acc: 0.2648
Epoch 12/50
 - 12s - loss: 2.0795 - acc: 0.3317 - val_loss: 2.1100 - val_acc: 0.2873
Epoch 13/50
 - 12s - loss: 2.0141 - acc: 0.3780 - val_loss: 2.0596 - val_acc: 0.3014
Epoch 14/50
 - 12s - loss: 1.9528 - acc: 0.3951 - val_loss: 2.0137 - val_acc: 0.3052
Epoch 15/50
 - 12s - loss: 1.8948 - acc: 0.4029 - val_loss: 1.9791 - val_acc: 0.3005
Epoch 16/50
 - 12s - loss: 1.8422 - acc: 0.4217 - val_loss: 1.9462 - val_acc: 0.3005
Epoch 17/50
 - 12s - loss: 1.7950 - acc: 0.4405 - val_loss: 1.9236 - val_acc: 0.3127
Epoch 18/50
 - 12s - loss: 1.7531 - acc: 0.4599 - val_loss: 1.9121 - val_acc: 0.3775
Epoch 19/50
 - 12s - loss: 1.7123 - acc: 0.4686 - val_loss: 1.8945 - val_acc: 0.3878
Epoch 20/50
 - 12s - loss: 1.6780 - acc: 0.4708 - val_loss: 1.8864 - val_acc: 0.4000
Epoch 21/50
 - 12s - loss: 1.6452 - acc: 0.4717 - val_loss: 1.8661 - val_acc: 0.4056
Epoch 22/50
 - 12s - loss: 1.6123 - acc: 0.4745 - val_loss: 1.8482 - val_acc: 0.4103
Epoch 23/50
 - 12s - loss: 1.5824 - acc: 0.4804 - val_loss: 1.8078 - val_acc: 0.4291
Epoch 24/50
 - 12s - loss: 1.5542 - acc: 0.4877 - val_loss: 1.8002 - val_acc: 0.4404
Epoch 25/50
 - 12s - loss: 1.5247 - acc: 0.4907 - val_loss: 1.7826 - val_acc: 0.4507
Epoch 26/50
 - 12s - loss: 1.4980 - acc: 0.4996 - val_loss: 1.7536 - val_acc: 0.4592
Epoch 27/50
 - 12s - loss: 1.4710 - acc: 0.5081 - val_loss: 1.7366 - val_acc: 0.4620
Epoch 28/50
 - 12s - loss: 1.4458 - acc: 0.5196 - val_loss: 1.7197 - val_acc: 0.4685
Epoch 29/50
 - 12s - loss: 1.4227 - acc: 0.5295 - val_loss: 1.7147 - val_acc: 0.4667
Epoch 30/50
 - 12s - loss: 1.3987 - acc: 0.5335 - val_loss: 1.7280 - val_acc: 0.4648
Epoch 31/50
 - 12s - loss: 1.3737 - acc: 0.5389 - val_loss: 1.6925 - val_acc: 0.4761
Epoch 32/50
 - 12s - loss: 1.3500 - acc: 0.5534 - val_loss: 1.7048 - val_acc: 0.4723
Epoch 33/50
 - 12s - loss: 1.3267 - acc: 0.5624 - val_loss: 1.6884 - val_acc: 0.4751
Epoch 34/50
 - 12s - loss: 1.3045 - acc: 0.5715 - val_loss: 1.6402 - val_acc: 0.4742
Epoch 35/50
 - 12s - loss: 1.2810 - acc: 0.5753 - val_loss: 1.6762 - val_acc: 0.4704
Epoch 36/50
 - 12s - loss: 1.2583 - acc: 0.5859 - val_loss: 1.6700 - val_acc: 0.4685
Epoch 37/50
 - 12s - loss: 1.2356 - acc: 0.5943 - val_loss: 1.6719 - val_acc: 0.4638
Epoch 38/50
 - 12s - loss: 1.2133 - acc: 0.6002 - val_loss: 1.6914 - val_acc: 0.4601
Epoch 39/50
 - 12s - loss: 1.1921 - acc: 0.6079 - val_loss: 1.6878 - val_acc: 0.4601
Epoch 00039: early stopping
LSTM training time: 483.74126529693604s
Model: "sequential_20"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_10 (LSTM)               (None, 8)                 2944      
_________________________________________________________________
dense_39 (Dense)             (None, 8)                 72        
_________________________________________________________________
dense_40 (Dense)             (None, 24)                216       
=================================================================
Total params: 3,232
Trainable params: 3,232
Non-trainable params: 0
_________________________________________________________________
None
LSTM history:
[3.1674528277085536, 3.104150057231033, 3.00418701153953, 2.8674516611209016, 2.7227080372932906, 2.590846130706235, 2.4803930516250823, 2.385863900492666, 2.301955223307521, 2.2269497378655996, 2.150281006219607, 2.0794671215808305, 2.0140546824356913, 1.9528030119845081, 1.8947642923717576, 1.842243290854811, 1.7950072695398545, 1.7530731781564577, 1.7123210747420634, 1.6780423190018536, 1.6451888589307908, 1.6123328271930364, 1.5823862219965734, 1.5542207749931078, 1.5247243332812448, 1.4979761492514347, 1.470988399214607, 1.4458063640363736, 1.4227067266015736, 1.3987040295969413, 1.3737305461562292, 1.3499864240137378, 1.3266749093306858, 1.3045210051710179, 1.280993512518562, 1.258311550886608, 1.2355857438147277, 1.2132713815710594, 1.1921317854032905]
[0.029598309, 0.066008925, 0.17124736, 0.22128259, 0.2149401, 0.22363167, 0.23490721, 0.24524313, 0.25863284, 0.27460653, 0.28776133, 0.33168897, 0.3779657, 0.39511392, 0.40286586, 0.42165846, 0.44045103, 0.45994833, 0.46863988, 0.47075406, 0.4716937, 0.47451258, 0.48038524, 0.48766738, 0.49072117, 0.49964765, 0.5081043, 0.51961476, 0.5294809, 0.53347427, 0.5388771, 0.5534414, 0.56236786, 0.57152927, 0.57528776, 0.5858586, 0.59431523, 0.6001879, 0.60793984]
[3.1265152109620717, 3.042068040650775, 2.9212752451919055, 2.7872108022931594, 2.652761235035641, 2.5378543636608573, 2.4453316278860604, 2.364359930871238, 2.292832319277553, 2.2276833160382483, 2.165146556370695, 2.110035654076948, 2.059597883985636, 2.013715774016761, 1.979082296711738, 1.9462411854748436, 1.9235509676552713, 1.9120711732918108, 1.8944522620366773, 1.8864249168987006, 1.866066712280954, 1.8482166511911742, 1.807833511728636, 1.800186927777501, 1.7825671897807591, 1.7535802284876505, 1.7365761708765521, 1.7196788136388215, 1.7147265412997752, 1.7280000297116562, 1.6924758952548247, 1.7047865553081316, 1.6883922518698822, 1.6401772945699558, 1.6762298986945354, 1.670010795167914, 1.671919910001083, 1.691395450421902, 1.6877770281733482]
[0.047887325286865234, 0.11924882978200912, 0.18309858441352844, 0.21596243977546692, 0.21220657229423523, 0.22441314160823822, 0.2281690090894699, 0.2319248765707016, 0.24225352704524994, 0.25164318084716797, 0.26478874683380127, 0.2873239517211914, 0.30140843987464905, 0.30516430735588074, 0.3004694879055023, 0.3004694879055023, 0.3126760423183441, 0.37746480107307434, 0.3877934217453003, 0.4000000059604645, 0.405633807182312, 0.41032862663269043, 0.42910799384117126, 0.44037559628486633, 0.4507042169570923, 0.4591549336910248, 0.46197181940078735, 0.468544602394104, 0.46666666865348816, 0.4647887349128723, 0.4760563373565674, 0.4723004698753357, 0.47511738538742065, 0.47417840361595154, 0.47042253613471985, 0.468544602394104, 0.4638497531414032, 0.4600938856601715, 0.4600938856601715]

  32/2898 [..............................] - ETA: 1:13
 384/2898 [==>...........................] - ETA: 5s  
 736/2898 [======>.......................] - ETA: 2s
1088/2898 [==========>...................] - ETA: 1s
1440/2898 [=============>................] - ETA: 1s
1792/2898 [=================>............] - ETA: 0s
2176/2898 [=====================>........] - ETA: 0s
2528/2898 [=========================>....] - ETA: 0s
2880/2898 [============================>.] - ETA: 0s
2898/2898 [==============================] - 1s 426us/step
Accuracies per class for LSTM
[0.   0.75 0.98 0.   0.28 0.12 0.84 0.96 0.18 0.   0.4  0.   0.   0.
 0.   0.78 0.   0.   0.75 0.69 0.   0.51]
[3.1674528277085536, 3.104150057231033, 3.00418701153953, 2.8674516611209016, 2.7227080372932906, 2.590846130706235, 2.4803930516250823, 2.385863900492666, 2.301955223307521, 2.2269497378655996, 2.150281006219607, 2.0794671215808305, 2.0140546824356913, 1.9528030119845081, 1.8947642923717576, 1.842243290854811, 1.7950072695398545, 1.7530731781564577, 1.7123210747420634, 1.6780423190018536, 1.6451888589307908, 1.6123328271930364, 1.5823862219965734, 1.5542207749931078, 1.5247243332812448, 1.4979761492514347, 1.470988399214607, 1.4458063640363736, 1.4227067266015736, 1.3987040295969413, 1.3737305461562292, 1.3499864240137378, 1.3266749093306858, 1.3045210051710179, 1.280993512518562, 1.258311550886608, 1.2355857438147277, 1.2132713815710594, 1.1921317854032905]
[0.029598309, 0.066008925, 0.17124736, 0.22128259, 0.2149401, 0.22363167, 0.23490721, 0.24524313, 0.25863284, 0.27460653, 0.28776133, 0.33168897, 0.3779657, 0.39511392, 0.40286586, 0.42165846, 0.44045103, 0.45994833, 0.46863988, 0.47075406, 0.4716937, 0.47451258, 0.48038524, 0.48766738, 0.49072117, 0.49964765, 0.5081043, 0.51961476, 0.5294809, 0.53347427, 0.5388771, 0.5534414, 0.56236786, 0.57152927, 0.57528776, 0.5858586, 0.59431523, 0.6001879, 0.60793984]
[3.1265152109620717, 3.042068040650775, 2.9212752451919055, 2.7872108022931594, 2.652761235035641, 2.5378543636608573, 2.4453316278860604, 2.364359930871238, 2.292832319277553, 2.2276833160382483, 2.165146556370695, 2.110035654076948, 2.059597883985636, 2.013715774016761, 1.979082296711738, 1.9462411854748436, 1.9235509676552713, 1.9120711732918108, 1.8944522620366773, 1.8864249168987006, 1.866066712280954, 1.8482166511911742, 1.807833511728636, 1.800186927777501, 1.7825671897807591, 1.7535802284876505, 1.7365761708765521, 1.7196788136388215, 1.7147265412997752, 1.7280000297116562, 1.6924758952548247, 1.7047865553081316, 1.6883922518698822, 1.6401772945699558, 1.6762298986945354, 1.670010795167914, 1.671919910001083, 1.691395450421902, 1.6877770281733482]
[0.047887325286865234, 0.11924882978200912, 0.18309858441352844, 0.21596243977546692, 0.21220657229423523, 0.22441314160823822, 0.2281690090894699, 0.2319248765707016, 0.24225352704524994, 0.25164318084716797, 0.26478874683380127, 0.2873239517211914, 0.30140843987464905, 0.30516430735588074, 0.3004694879055023, 0.3004694879055023, 0.3126760423183441, 0.37746480107307434, 0.3877934217453003, 0.4000000059604645, 0.405633807182312, 0.41032862663269043, 0.42910799384117126, 0.44037559628486633, 0.4507042169570923, 0.4591549336910248, 0.46197181940078735, 0.468544602394104, 0.46666666865348816, 0.4647887349128723, 0.4760563373565674, 0.4723004698753357, 0.47511738538742065, 0.47417840361595154, 0.47042253613471985, 0.468544602394104, 0.4638497531414032, 0.4600938856601715, 0.4600938856601715]
recall 0.5476
precision 0.4462
f1 0.4669
mcc 0.4291
RMSE: 5.788
classification report:
              precision    recall  f1-score   support

           1       0.00      0.00      0.00        72
           2       0.57      0.75      0.65        68
           3       0.52      0.98      0.68       252
           4       0.00      0.00      0.00       132
           5       0.29      0.28      0.28       191
           6       0.15      0.12      0.13       150
           7       0.41      0.84      0.55       240
           9       0.41      0.96      0.58       100
          10       0.26      0.18      0.21        61
          11       0.00      0.00      0.00        87
          12       0.63      0.40      0.49       595
          13       0.00      0.00      0.00        59
          14       0.00      0.00      0.00         6
          15       0.00      0.00      0.00        72
          16       0.00      0.00      0.00        30
          17       0.42      0.78      0.55       156
          18       0.00      0.00      0.00        36
          19       0.00      0.00      0.00        54
          21       0.62      0.75      0.68       222
          22       0.52      0.69      0.59       168
          23       0.00      0.00      0.00        65
          24       0.50      0.51      0.51        82

   micro avg       0.47      0.47      0.47      2898
   macro avg       0.24      0.33      0.27      2898
weighted avg       0.38      0.47      0.40      2898

>#10: 47.032
Output from summarize_results:
Accuracy: 0.580% (+/-0.136)
Loss: 1.672% (+/-0.524)
Recall: 0.613% (+/-0.123)
Precision: 0.619% (+/-0.132)
F1: 0.581% (+/-0.137)
MCC: 0.553% (+/-0.143)
RMSE: 4.796% (+/-1.104)
#################### end ###################
Summary of results:
   accuracy               algorithm       f1       loss       mcc  precision    recall      rmse  trainingtime
0  0.656300     Logistic Regression  0.63110   1.416500  0.644800   0.714400  0.670200  3.987000    111.698796
0  0.335400             Naive Bayes  0.39640  22.954300  0.321300   0.557500  0.433300  6.482200      0.648967
0  0.432700           Decision Tree  0.40300  19.593400  0.406800   0.489200  0.432700  6.791400      3.011481
0  0.527300  Support Vector Machine  0.50230   1.150700  0.519800   0.479300  0.613900  5.728600   1012.834293
0  0.349200     K-Nearest Neighbors  0.31540  20.859600  0.284600   0.471600  0.429900  7.694400      1.266676
0  0.461000           Random Forest  0.50900   2.283900  0.445100   0.500000  0.611700  6.673500     26.203489
0  0.485900                 Bagging  0.42890  10.417100  0.468200   0.540800  0.486900  6.569900     17.127686
0  0.540400              Extra Tree  0.49540   8.793900  0.525600   0.592300  0.551800  5.112100      0.478823
0  0.563100       Gradient Boosting  0.51810   2.464100  0.554600   0.651800  0.575100  4.817800    782.543489
0  0.563500   Multilayer Perceptron  0.51790   2.449400  0.554900   0.650400  0.575400  4.837600    807.160546
0       NaN      CNN with attention      NaN        NaN       NaN        NaN       NaN       NaN           NaN
0  0.694444                     CNN  0.68915   1.176051  0.675580   0.726480  0.710090  3.761464     73.970846
0  0.579607                    LSTM  0.58086   1.672143  0.552935   0.619095  0.613075  4.796085    334.053042
